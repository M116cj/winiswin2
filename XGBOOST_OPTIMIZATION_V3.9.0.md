# ğŸš€ XGBoost MLç³»ç»Ÿä¼˜åŒ–å®Œæˆ (v3.9.0)

**æ—¥æœŸ**: 2025-10-27  
**ç‰ˆæœ¬**: v3.9.0  
**çŠ¶æ€**: âœ… å·²å®Œæˆ

---

## ğŸ“‹ ä¼˜åŒ–æ¦‚è¿°

åŸºäºæ‚¨æå‡ºçš„å»ºè®®ï¼Œç³»ç»Ÿå·²å®ç°ä»¥ä¸‹6ä¸ªä¸»è¦ä¼˜åŒ–æ–¹å‘ï¼š

| # | ä¼˜åŒ–é¡¹ | çŠ¶æ€ | æ–‡ä»¶ |
|---|--------|------|------|
| 1 | æ ‡ç­¾æ³„æ¼æ£€æµ‹ | âœ… | `label_leakage_validator.py` |
| 2 | æ ·æœ¬ä¸å¹³è¡¡å¤„ç† | âœ… | `imbalance_handler.py` |
| 3 | æ¨¡å‹æ¼‚ç§»ç›‘æ§ | âœ… | `drift_detector.py` |
| 4 | ç›®æ ‡å˜é‡ä¼˜åŒ– | âœ… | `target_optimizer.py` |
| 5 | ä¸ç¡®å®šæ€§é‡åŒ– | âœ… | `uncertainty_quantifier.py` |
| 6 | ç‰¹å¾é‡è¦æ€§ç›‘æ§ | âœ… | `feature_importance_monitor.py` |

---

## ğŸ” 1. æ ‡ç­¾æ³„æ¼æ£€æµ‹

### **é—®é¢˜**
è‹¥ç‰¹å¾åŒ…å«æœªæ¥ä¿¡æ¯ï¼ˆå¦‚åŠ¨æ€è°ƒæ•´çš„TP/SLï¼‰ï¼Œä¼šå¯¼è‡´è®­ç»ƒæ—¶æ€§èƒ½è™šé«˜ï¼Œä½†å®ç›˜è¡¨ç°å·®ã€‚

### **è§£å†³æ–¹æ¡ˆ**
åˆ›å»º `LabelLeakageValidator` ç±»ï¼Œå®ç°4å±‚éªŒè¯ï¼š

#### **æ£€æŸ¥1ï¼šç›®æ ‡å˜é‡ç›¸å…³æ€§æ£€æµ‹**
```python
# æ£€æµ‹ç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§ï¼ˆ>0.9è¡¨ç¤ºå¯èƒ½æ³„æ¼ï¼‰
correlations = df[distance_features].corrwith(df['is_winner'])
```

#### **æ£€æŸ¥2ï¼šæ—¶é—´å¯¹é½éªŒè¯**
```python
# éªŒè¯æ­¢æŸ/æ­¢ç›ˆè·ç¦»æ˜¯å¦åŸºäºå¼€ä»“æ—¶åˆ»è®¡ç®—
expected_stop_dist = abs((stop_loss - entry_price) / entry_price)
actual_stop_dist = row['stop_distance_pct']

# å…è®¸0.1%æµ®ç‚¹è¯¯å·®
if abs(expected - actual) > 0.001:
    logger.warning("æ—¶é—´å¯¹é½ä¸åŒ¹é…ï¼")
```

#### **æ£€æŸ¥3ï¼šç‰¹å¾åˆç†æ€§æ£€æŸ¥**
- `stop_distance_pct` åº”åœ¨ 0-20% èŒƒå›´
- `tp_distance_pct` åº”åœ¨ 0-50% èŒƒå›´

#### **æ£€æŸ¥4ï¼šæœªæ¥ä¿¡æ¯æ£€æµ‹**
ç¦æ­¢ä½¿ç”¨çš„ç‰¹å¾ï¼ˆä»…å¹³ä»“åå¯çŸ¥ï¼‰ï¼š
- `actual_hold_duration`
- `final_pnl_pct`
- `exit_price`
- `close_time`

### **é›†æˆæ–¹å¼**
```python
# model_trainer.py è®­ç»ƒå‰è‡ªåŠ¨éªŒè¯
leakage_report = self.leakage_validator.validate_training_data(df)
if leakage_report['has_leakage']:
    logger.warning(f"æ£€æµ‹åˆ°æ½œåœ¨æ ‡ç­¾æ³„æ¼ï¼š{leakage_report['leakage_features']}")
```

---

## ğŸ“Š 2. æ ·æœ¬ä¸å¹³è¡¡å¤„ç†

### **é—®é¢˜**
- å‡†ç¡®ç‡81.56%å¯èƒ½å—å¸‚åœºå•è¾¹è¡Œæƒ…å½±å“
- LONG/SHORTæ ·æœ¬ä¸å¹³è¡¡å¯¼è‡´åå·®
- å¾®å°ç›ˆåˆ©ï¼ˆå¦‚0.01%ï¼‰è¢«äº¤æ˜“æˆæœ¬åå™¬

### **è§£å†³æ–¹æ¡ˆ**
åˆ›å»º `ImbalanceHandler` ç±»ï¼Œæä¾›ï¼š

#### **æ··æ·†çŸ©é˜µè¯¦ç»†æŠ¥å‘Š**
```python
confusion_matrix_detailed = {
    'overall_metrics': {
        'accuracy': 0.8156,
        'precision': 0.7845,
        'recall': 0.8523,
        'f1_score': 0.8170
    },
    'direction_metrics': {
        'long': {'accuracy': 0.82, 'f1': 0.81, 'samples': 450},
        'short': {'accuracy': 0.81, 'f1': 0.82, 'samples': 420}
    }
}
```

#### **æˆæœ¬æ„ŸçŸ¥å­¦ä¹ **
```python
# è®¡ç®—XGBoostçš„scale_pos_weightå‚æ•°
scale_pos_weight = num_negative / num_positive

# è®­ç»ƒæ—¶åº”ç”¨
params['scale_pos_weight'] = scale_pos_weight  # ä¾‹å¦‚: 1.2
```

#### **æ ·æœ¬æƒé‡è®¡ç®—**
```python
# æ–¹æ³•1: balancedï¼ˆsklearnæ ‡å‡†ï¼‰
weights[cls] = total_samples / (n_classes * class_count)

# æ–¹æ³•2: sqrtï¼ˆæ¸©å’Œï¼‰
weights[cls] = sqrt(total_samples / class_count)

# æ–¹æ³•3: logï¼ˆæœ€æ¸©å’Œï¼‰
weights[cls] = log(total_samples / class_count + 1)
```

### **é›†æˆæ–¹å¼**
```python
# è‡ªåŠ¨æ£€æµ‹å¹¶å¤„ç†ä¸å¹³è¡¡
if balance_report['needs_balancing']:
    sample_weights = self.imbalance_handler.calculate_sample_weight(y_train)
    scale_pos_weight = self.imbalance_handler.get_scale_pos_weight(y_train)
    
    model.fit(X_train, y_train, sample_weight=sample_weights)
```

---

## ğŸ”„ 3. æ¨¡å‹æ¼‚ç§»ç›‘æ§

### **é—®é¢˜**
- å¢é‡å­¦ä¹ é•¿æœŸç´¯ç§¯å¯¼è‡´æ—§æ•°æ®å½±å“è¿‡å¤§
- æ— æ³•é€‚åº”å¸‚åœºregime shiftï¼ˆå¦‚æ³¢åŠ¨ç‡çªå˜ï¼‰
- ç‰¹å¾åˆ†å¸ƒæ¼‚ç§»æœªè¢«æ£€æµ‹

### **è§£å†³æ–¹æ¡ˆ**
åˆ›å»º `DriftDetector` ç±»ï¼Œå®ç°ï¼š

#### **æ»‘åŠ¨çª—å£è®­ç»ƒ**
```python
# åªä¿ç•™æœ€è¿‘Nç¬”æ•°æ®è®­ç»ƒ
df_windowed = self.drift_detector.apply_sliding_window(df, window_size=1000)

# ä¼˜ç‚¹ï¼š
# - å‡å°‘æ—§æ•°æ®å½±å“
# - é€‚åº”å¸‚åœºå˜åŒ–
# - é˜²æ­¢æ¨¡å‹è¿‡æ—¶
```

#### **åŠ¨æ€æ ·æœ¬æƒé‡**
```python
# æ–°æ ·æœ¬æƒé‡ > æ—§æ ·æœ¬ï¼ˆæŒ‡æ•°è¡°å‡ï¼‰
weights = decay_factor ** ages  # decay_factor = 0.95

# æœ€æ–°æ ·æœ¬æƒé‡: 1.00
# 100ç¬”å‰æ ·æœ¬æƒé‡: 0.59
# 500ç¬”å‰æ ·æœ¬æƒé‡: 0.00
```

#### **ç‰¹å¾åˆ†å¸ƒæ¼‚ç§»æ£€æµ‹ï¼ˆKSæ£€éªŒï¼‰**
```python
# Kolmogorov-Smirnovæ£€éªŒ
ks_stat, p_value = kstest(current_values, baseline_distribution)

if p_value < 0.05:  # 5%æ˜¾è‘—æ€§æ°´å¹³
    logger.warning(f"ç‰¹å¾{feature}å‘ç”Ÿæ¼‚ç§»ï¼")
    
    # å»ºè®®
    if drift_ratio > 0.3:  # >30%ç‰¹å¾æ¼‚ç§»
        recommendation = 'full_retrain'  # å®Œæ•´é‡è®­ç»ƒ
    else:
        recommendation = 'incremental_retrain'  # å¢é‡é‡è®­ç»ƒ
```

#### **è‡ªåŠ¨é‡è®­ç»ƒè§¦å‘**
```python
should_retrain, reason = self.drift_detector.should_retrain(
    current_samples=1000,
    last_training_samples=950,
    last_training_time=datetime(...),
    new_sample_threshold=50,       # ç´¯ç§¯50ç¬”æ–°æ•°æ®
    time_threshold_hours=24,       # æˆ–24å°æ—¶+10ç¬”
    min_new_samples_for_time=10
)

# è§¦å‘æ¡ä»¶ï¼ˆä»»ä¸€æ»¡è¶³ï¼‰ï¼š
# 1. ç´¯ç§¯â‰¥50ç¬”æ–°äº¤æ˜“
# 2. è·ä¸Šæ¬¡â‰¥24å°æ—¶ ä¸” æœ‰â‰¥10ç¬”æ–°æ•°æ®
# 3. æ£€æµ‹åˆ°ä¸¥é‡ç‰¹å¾æ¼‚ç§»
```

### **é›†æˆæ–¹å¼**
```python
# è®­ç»ƒå‰åº”ç”¨æ»‘åŠ¨çª—å£
df = self.drift_detector.apply_sliding_window(df, window_size=1000)

# æ£€æµ‹ç‰¹å¾æ¼‚ç§»
drift_report = self.drift_detector.detect_feature_drift(X, X.columns.tolist())

# è®¡ç®—åŠ¨æ€æƒé‡
time_weights = self.drift_detector.calculate_sample_weights(df, decay_factor=0.95)
sample_weights = class_weights * time_weights  # ç»„åˆç±»åˆ«æƒé‡å’Œæ—¶é—´æƒé‡
```

---

## ğŸ¯ 4. ç›®æ ‡å˜é‡ä¼˜åŒ–

### **é—®é¢˜**
- äºŒåˆ†ç±»ç›®æ ‡ï¼ˆis_winnerï¼‰æ— æ³•åŒºåˆ†ç›ˆåˆ©å¤§å°
- æ— æ³•ç›´æ¥ä¼˜åŒ–æœŸæœ›æ”¶ç›Š
- æœªè€ƒè™‘å¸‚åœºæ³¢åŠ¨ç‡

### **è§£å†³æ–¹æ¡ˆ**
åˆ›å»º `TargetOptimizer` ç±»ï¼Œæ”¯æŒ3ç§ç›®æ ‡ç±»å‹ï¼š

#### **ç±»å‹1: äºŒåˆ†ç±»ï¼ˆbinaryï¼‰- é»˜è®¤æ¨¡å¼**
```python
target = df['is_winner']  # 1=ç›ˆåˆ©, 0=äºæŸ

params = {
    'objective': 'binary:logistic',
    'eval_metric': 'auc'
}
```

#### **ç±»å‹2: ç›ˆäºç™¾åˆ†æ¯”ï¼ˆpnl_pctï¼‰- å›å½’æ¨¡å¼**
```python
target = df['pnl_pct']  # ç›´æ¥é¢„æµ‹æ”¶ç›Šï¼š-5.2%, +3.8%, ...

params = {
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse'
}

# è¯„ä¼°æŒ‡æ ‡ï¼š
# - MAEï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰
# - RMSEï¼ˆå‡æ–¹æ ¹è¯¯å·®ï¼‰
# - RÂ²åˆ†æ•°
# - æ–¹å‘å‡†ç¡®ç‡ï¼ˆé¢„æµ‹ç¬¦å·æ˜¯å¦æ­£ç¡®ï¼‰
```

#### **ç±»å‹3: é£é™©è°ƒæ•´æ”¶ç›Šï¼ˆrisk_adjustedï¼‰- æ¨èæ¨¡å¼**
```python
# è€ƒè™‘å¸‚åœºæ³¢åŠ¨ç‡
target = df['pnl_pct'] / df['atr_entry']

# ä¼˜ç‚¹ï¼š
# - é¿å…é«˜æ³¢åŠ¨æœŸçš„è™šå‡æ”¶ç›Š
# - æ›´ç¨³å®šçš„è¯„ä¼°æŒ‡æ ‡
# - ç±»ä¼¼Sharpe Ratioçš„æ€æƒ³

params = {
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse'
}
```

### **ä½¿ç”¨æ–¹å¼**
```python
# åˆå§‹åŒ–ä¼˜åŒ–å™¨
target_optimizer = TargetOptimizer(target_type='risk_adjusted')

# å‡†å¤‡ç›®æ ‡å˜é‡
y, target_meta = target_optimizer.prepare_target(df)

# è°ƒæ•´æ¨¡å‹å‚æ•°
params = target_optimizer.get_model_params(base_params)

# è®­ç»ƒ
model.fit(X, y)

# è¯„ä¼°
metrics = target_optimizer.evaluate_prediction(y_true, y_pred)
```

---

## ğŸ² 5. ä¸ç¡®å®šæ€§é‡åŒ–

### **é—®é¢˜**
- å•ä¸€æ¨¡å‹é¢„æµ‹ç¼ºä¹ä¸ç¡®å®šæ€§ä¼°è®¡
- æ— æ³•è¯†åˆ«ä½ä¿¡å¿ƒé¢„æµ‹
- æ— é¢„æµ‹åŒºé—´

### **è§£å†³æ–¹æ¡ˆ**
åˆ›å»º `UncertaintyQuantifier` ç±»ï¼Œä½¿ç”¨Bootstrapæ–¹æ³•ï¼š

#### **Bootstrapé›†æˆè®­ç»ƒ**
```python
# è®­ç»ƒNä¸ªæ¨¡å‹ï¼ˆBootstrapé‡‡æ ·ï¼‰
for i in range(n_bootstrap=50):
    # æœ‰æ”¾å›é‡‡æ ·
    indices = np.random.choice(n_samples, size=int(n_samples*0.8), replace=True)
    X_boot, y_boot = X[indices], y[indices]
    
    # è®­ç»ƒæ¨¡å‹
    model_i.fit(X_boot, y_boot)
    bootstrap_models.append(model_i)
```

#### **é¢„æµ‹åŒºé—´ç”Ÿæˆ**
```python
# æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
predictions = [model.predict_proba(X)[:, 1] for model in bootstrap_models]

# ç»Ÿè®¡é‡
mean_pred = np.mean(predictions, axis=0)        # å‡å€¼é¢„æµ‹
std_pred = np.std(predictions, axis=0)          # æ ‡å‡†å·®
lower_bound = np.percentile(predictions, 2.5)   # 95%ç½®ä¿¡åŒºé—´ä¸‹ç•Œ
upper_bound = np.percentile(predictions, 97.5)  # 95%ç½®ä¿¡åŒºé—´ä¸Šç•Œ

# ä¸ç¡®å®šæ€§åˆ†æ•°
uncertainty_score = std_pred / (abs(mean_pred) + 1e-6)
```

#### **é«˜ä¿¡å¿ƒè¿‡æ»¤**
```python
# è¿‡æ»¤ä½ä¸ç¡®å®šæ€§ï¼ˆé«˜ä¿¡å¿ƒï¼‰é¢„æµ‹
high_confidence_mask = uncertainty_scores < 0.2  # ä¸ç¡®å®šæ€§<20%

# æ•ˆæœï¼š
# - æ€»é¢„æµ‹: 100ä¸ªä¿¡å·
# - é«˜ä¿¡å¿ƒ: 35ä¸ªä¿¡å·ï¼ˆä¸ç¡®å®šæ€§<20%ï¼‰
# - æ‰§è¡Œç‡: 35% âœ… ï¼ˆåªäº¤æ˜“æœ€æœ‰æŠŠæ¡çš„ä¿¡å·ï¼‰
```

### **ä½¿ç”¨æ–¹å¼**
```python
# è®­ç»ƒBootstrapé›†æˆ
uncertainty_quantifier.fit_bootstrap_models(X, y, base_model)

# é¢„æµ‹ï¼ˆå¸¦åŒºé—´ï¼‰
predictions = uncertainty_quantifier.predict_with_uncertainty(X_test)

# è¿‡æ»¤é«˜ä¿¡å¿ƒé¢„æµ‹
high_conf_mask = uncertainty_quantifier.filter_high_confidence_predictions(
    predictions,
    uncertainty_threshold=0.2
)

# åªæ‰§è¡Œé«˜ä¿¡å¿ƒä¿¡å·
high_conf_signals = signals[high_conf_mask]
```

---

## ğŸ“ˆ 6. ç‰¹å¾é‡è¦æ€§ç›‘æ§

### **é—®é¢˜**
- ç‰¹å¾é‡è¦æ€§å¯èƒ½éšæ—¶é—´å˜åŒ–
- confidence_scoreé‡è¦æ€§çªé™æœªè¢«å¯Ÿè§‰
- æ— è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹å®¡æŸ¥è§¦å‘

### **è§£å†³æ–¹æ¡ˆ**
åˆ›å»º `FeatureImportanceMonitor` ç±»ï¼Œå®ç°ï¼š

#### **é‡è¦æ€§å†å²è®°å½•**
```python
# æ¯æ¬¡è®­ç»ƒåè®°å½•
monitor.record_importance(
    feature_importance={'confidence_score': 0.25, 'leverage': 0.15, ...},
    model_metrics={'accuracy': 0.82, 'f1': 0.81}
)

# ä¿ç•™æœ€è¿‘100æ¬¡è®°å½•
```

#### **çªå˜æ£€æµ‹**
```python
shift_report = monitor.detect_importance_shift(
    current_importance=current_imp,
    shift_threshold=0.3  # å˜åŒ–>30%è®¤ä¸ºçªå˜
)

# ç¤ºä¾‹è¾“å‡ºï¼š
{
    'has_shift': True,
    'shifted_features': [
        {
            'feature': 'confidence_score',
            'previous_importance': 0.25,
            'current_importance': 0.12,
            'relative_change': 0.52  # ä¸‹é™52%ï¼
        },
        {
            'feature': 'atr_entry',
            'previous_importance': 0.08,
            'current_importance': 0.18,
            'relative_change': 1.25  # ä¸Šå‡125%ï¼
        }
    ]
}
```

#### **è¶‹åŠ¿åˆ†æ**
```python
trend = monitor.analyze_importance_trend(
    feature_name='confidence_score',
    window_size=10  # æœ€è¿‘10æ¬¡è®­ç»ƒ
)

# è¾“å‡ºï¼š
{
    'feature': 'confidence_score',
    'trend': 'decreasing',  # 'increasing' / 'decreasing' / 'stable'
    'recent_values': [0.25, 0.24, 0.22, 0.20, 0.18, ...],
    'mean': 0.218,
    'std': 0.024
}
```

#### **è‡ªåŠ¨æ¨è**
```python
recommendations = monitor.recommend_feature_engineering(shift_report)

# ç¤ºä¾‹è¾“å‡ºï¼š
[
    "âš ï¸ confidence_score é‡è¦æ€§ä¸‹é™52%ï¼Œè€ƒè™‘ç§»é™¤æˆ–æ›¿æ¢ä¸ºæ›´æœ‰æ•ˆçš„ç‰¹å¾",
    "âœ… atr_entry é‡è¦æ€§ä¸Šå‡125%ï¼Œè€ƒè™‘åŸºäºæ­¤ç‰¹å¾åˆ›å»ºè¡ç”Ÿç‰¹å¾"
]
```

### **ä½¿ç”¨æ–¹å¼**
```python
# è®­ç»ƒåè®°å½•
feature_importance = model.get_feature_importance()
monitor.record_importance(feature_importance, model_metrics)

# æ£€æµ‹çªå˜
shift_report = monitor.detect_importance_shift(feature_importance)

# è·å–æ¨è
if shift_report['has_shift']:
    recommendations = monitor.recommend_feature_engineering(shift_report)
    for rec in recommendations:
        logger.info(rec)
```

---

## ğŸ”„ é›†æˆåˆ°è®­ç»ƒæµç¨‹

### **å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆv3.9.0ï¼‰**

```python
def train(self, params=None, use_gpu=True, incremental=False):
    # 1. åŠ è½½æ•°æ®
    df = self.data_processor.load_training_data()
    
    # ğŸ” 2. æ ‡ç­¾æ³„æ¼éªŒè¯
    leakage_report = self.leakage_validator.validate_training_data(df)
    
    # ğŸ“Š 3. åº”ç”¨æ»‘åŠ¨çª—å£ï¼ˆé˜²æ­¢æ—§æ•°æ®è¿‡åº¦å½±å“ï¼‰
    df = self.drift_detector.apply_sliding_window(df, window_size=1000)
    
    # å‡†å¤‡ç‰¹å¾
    X, y = self.data_processor.prepare_features(df)
    
    # ğŸ“Š 4. ç±»åˆ«å¹³è¡¡åˆ†æ
    balance_report = self.imbalance_handler.analyze_class_balance(y, X)
    
    # ğŸ” 5. ç‰¹å¾åˆ†å¸ƒæ¼‚ç§»æ£€æµ‹
    drift_report = self.drift_detector.detect_feature_drift(X, X.columns.tolist())
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = split_data(X, y)
    
    # ğŸ›¡ï¸ 6. è®¡ç®—æ ·æœ¬æƒé‡ï¼ˆå¤„ç†ä¸å¹³è¡¡ï¼‰
    if balance_report['needs_balancing']:
        sample_weights = self.imbalance_handler.calculate_sample_weight(y_train)
        time_weights = self.drift_detector.calculate_sample_weights(df)
        sample_weights = sample_weights * time_weights  # ç»„åˆæƒé‡
        
        scale_pos_weight = self.imbalance_handler.get_scale_pos_weight(y_train)
        params['scale_pos_weight'] = scale_pos_weight
    
    # 7. è®­ç»ƒæ¨¡å‹
    model.fit(X_train, y_train, sample_weight=sample_weights)
    
    # 8. é¢„æµ‹
    y_pred = model.predict(X_test)
    
    # ğŸ“Š 9. è¯¦ç»†è¯„ä¼°ï¼ˆåŒ…å«æ··æ·†çŸ©é˜µå’Œåˆ†æ–¹å‘è¯„ä¼°ï¼‰
    confusion_report = self.imbalance_handler.generate_confusion_matrix_report(
        y_test, y_pred, X_test
    )
    
    # ğŸ“ˆ 10. è®°å½•ç‰¹å¾é‡è¦æ€§
    feature_importance = model.get_feature_importance()
    self.importance_monitor.record_importance(feature_importance, metrics)
    shift_report = self.importance_monitor.detect_importance_shift(feature_importance)
    
    # ä¿å­˜æ¨¡å‹å’ŒæŠ¥å‘Š
    metrics['optimization_reports'] = {
        'label_leakage': leakage_report,
        'class_balance': balance_report,
        'feature_drift': drift_report,
        'feature_importance_shift': shift_report
    }
    
    return model, metrics
```

---

## ğŸ“Š è®­ç»ƒè¾“å‡ºç¤ºä¾‹

### **æ§åˆ¶å°è¾“å‡º**
```
ğŸ” æ ‡ç­¾æ³„æ¼éªŒè¯å®Œæˆï¼šæ— æ³„æ¼æ£€æµ‹
ğŸ“Š åº”ç”¨æ»‘åŠ¨çª—å£ï¼šä¿ç•™æœ€è¿‘1000ç¬”æ•°æ®ï¼Œä¸¢å¼ƒ250ç¬”æ—§æ•°æ®
ğŸ“Š ç±»åˆ«å¹³è¡¡åˆ†æï¼šä¸å¹³è¡¡æ¯”ç‡ 1.45, åˆ†å¸ƒ {0: 580, 1: 420}
ğŸ“Š æ£€æµ‹åˆ°ç±»åˆ«ä¸å¹³è¡¡ï¼Œè®¡ç®—åŠ¨æ€æ ·æœ¬æƒé‡...
ğŸ“Š å¯ç”¨æˆæœ¬æ„ŸçŸ¥å­¦ä¹ ï¼šscale_pos_weight = 1.38
ğŸ” ç‰¹å¾åˆ†å¸ƒæ¼‚ç§»æ£€æµ‹ï¼šæœªæ£€æµ‹åˆ°æ¼‚ç§»

=== è®­ç»ƒå¼€å§‹ ===
è®­ç»ƒé›†å¤§å°: (800, 29), æµ‹è¯•é›†å¤§å°: (200, 29)
... XGBoostè®­ç»ƒ ...

==================================================
ğŸ¯ æ··æ·†çŸ©é˜µæŠ¥å‘Š
==================================================
              é¢„æµ‹è´Ÿç±»    é¢„æµ‹æ­£ç±»
å®é™…è´Ÿç±»       115         10
å®é™…æ­£ç±»        12         63
==================================================
å‡†ç¡®ç‡ (Accuracy):  0.8900
ç²¾ç¡®ç‡ (Precision): 0.8630
å¬å›ç‡ (Recall):    0.8400
F1åˆ†æ•° (F1-Score):  0.8514
==================================================

ğŸ“Š åˆ†æ–¹å‘è¯„ä¼°ï¼š
  LONG:  æ ·æœ¬æ•° 105, å‡†ç¡®ç‡89.52%, F1=0.8755
  SHORT: æ ·æœ¬æ•°  95, å‡†ç¡®ç‡88.42%, F1=0.8268

ğŸ“Š Top 10 ç‰¹å¾ï¼š
  1. confidence_score: 0.2254
  2. leverage: 0.1453
  3. atr_entry: 0.1124
  4. rsi_entry: 0.0896
  5. macd_histogram_entry: 0.0745
  ...

âš ï¸ æ£€æµ‹åˆ°ç‰¹å¾é‡è¦æ€§çªå˜ï¼2ä¸ªç‰¹å¾
  - confidence_score: 0.2500 â†’ 0.2254 (å˜åŒ– 9.8%)
  - atr_entry: 0.0850 â†’ 0.1124 (å˜åŒ– 32.2%)

æ¨èï¼š
âœ… atr_entry é‡è¦æ€§ä¸Šå‡32%ï¼Œè€ƒè™‘åŸºäºæ­¤ç‰¹å¾åˆ›å»ºè¡ç”Ÿç‰¹å¾
```

---

## ğŸ“ æ–°å¢æ–‡ä»¶ç»“æ„

```
src/ml/
â”œâ”€â”€ label_leakage_validator.py     # æ ‡ç­¾æ³„æ¼æ£€æµ‹
â”œâ”€â”€ imbalance_handler.py            # æ ·æœ¬ä¸å¹³è¡¡å¤„ç†
â”œâ”€â”€ drift_detector.py               # æ¨¡å‹æ¼‚ç§»ç›‘æ§
â”œâ”€â”€ target_optimizer.py             # ç›®æ ‡å˜é‡ä¼˜åŒ–
â”œâ”€â”€ uncertainty_quantifier.py       # ä¸ç¡®å®šæ€§é‡åŒ–
â”œâ”€â”€ feature_importance_monitor.py   # ç‰¹å¾é‡è¦æ€§ç›‘æ§
â””â”€â”€ model_trainer.py                # è®­ç»ƒå™¨ï¼ˆå·²æ›´æ–°é›†æˆï¼‰

data/models/
â”œâ”€â”€ baseline_stats.json             # åŸºå‡†ç‰¹å¾ç»Ÿè®¡ï¼ˆæ¼‚ç§»æ£€æµ‹ï¼‰
â””â”€â”€ feature_importance_history.json # ç‰¹å¾é‡è¦æ€§å†å²
```

---

## ğŸ¯ å®é™…ä½¿ç”¨å»ºè®®

### **1. æ ‡ç­¾æ³„æ¼æ£€æµ‹**
- âœ… **å¿…é¡»å¯ç”¨**ï¼šæ¯æ¬¡è®­ç»ƒå‰è‡ªåŠ¨éªŒè¯
- âš ï¸ å¦‚æœæ£€æµ‹åˆ°æ³„æ¼ï¼Œç«‹å³å®¡æŸ¥æ•°æ®æº

### **2. æ ·æœ¬ä¸å¹³è¡¡å¤„ç†**
- âœ… **æ¨èå¯ç”¨**ï¼šæˆæœ¬æ„ŸçŸ¥å­¦ä¹ ï¼ˆscale_pos_weightï¼‰
- ğŸ“Š å®šæœŸæŸ¥çœ‹æ··æ·†çŸ©é˜µï¼Œæ£€æŸ¥LONG/SHORTå¹³è¡¡

### **3. æ¨¡å‹æ¼‚ç§»ç›‘æ§**
- âœ… **å¼ºçƒˆæ¨è**ï¼šæ»‘åŠ¨çª—å£è®­ç»ƒï¼ˆwindow_size=1000ï¼‰
- ğŸ”„ æ¯å‘¨æ£€æŸ¥æ¼‚ç§»æŠ¥å‘Šï¼Œä¸¥é‡æ¼‚ç§»æ—¶å®Œæ•´é‡è®­ç»ƒ

### **4. ç›®æ ‡å˜é‡ä¼˜åŒ–**
- ğŸ¯ **åˆæœŸ**ï¼šä½¿ç”¨äºŒåˆ†ç±»ï¼ˆbinaryï¼‰ç¨³å®šç³»ç»Ÿ
- ğŸš€ **è¿›é˜¶**ï¼šåˆ‡æ¢åˆ°é£é™©è°ƒæ•´æ”¶ç›Šï¼ˆrisk_adjustedï¼‰è·å¾—æ›´ç¨³å®šè¡¨ç°

### **5. ä¸ç¡®å®šæ€§é‡åŒ–**
- âš¡ **å¯é€‰**ï¼šBootstrapè®­ç»ƒè¾ƒæ…¢ï¼ˆ50ä¸ªæ¨¡å‹ï¼‰
- ğŸ’¡ **é€‚ç”¨åœºæ™¯**ï¼šèµ„é‡‘è¾ƒå¤§æ—¶ï¼Œåªäº¤æ˜“é«˜ä¿¡å¿ƒä¿¡å·

### **6. ç‰¹å¾é‡è¦æ€§ç›‘æ§**
- ğŸ“ˆ **æ¨èå¯ç”¨**ï¼šæ¯æ¬¡è®­ç»ƒåè‡ªåŠ¨è®°å½•
- ğŸ”” é‡è¦æ€§çªå˜æ—¶ï¼Œè€ƒè™‘ç‰¹å¾å·¥ç¨‹æ”¹è¿›

---

## âš™ï¸ é…ç½®å‚æ•°

### **æ»‘åŠ¨çª—å£å¤§å°**
```python
window_size = 1000  # ä¿ç•™æœ€è¿‘1000ç¬”æ•°æ®
# å»ºè®®ï¼š500-2000ç¬”
```

### **æ¼‚ç§»æ£€æµ‹é˜ˆå€¼**
```python
drift_threshold = 0.05  # KSæ£€éªŒpå€¼<0.05è®¤ä¸ºæ¼‚ç§»
# å»ºè®®ï¼š0.01ï¼ˆä¸¥æ ¼ï¼‰ - 0.10ï¼ˆå®½æ¾ï¼‰
```

### **ä¸ç¡®å®šæ€§é˜ˆå€¼**
```python
uncertainty_threshold = 0.2  # ä¸ç¡®å®šæ€§<20%è®¤ä¸ºé«˜ä¿¡å¿ƒ
# å»ºè®®ï¼š0.15ï¼ˆä¸¥æ ¼ï¼‰ - 0.30ï¼ˆå®½æ¾ï¼‰
```

### **ç‰¹å¾é‡è¦æ€§å˜åŒ–é˜ˆå€¼**
```python
shift_threshold = 0.3  # å˜åŒ–>30%è®¤ä¸ºçªå˜
# å»ºè®®ï¼š0.2ï¼ˆæ•æ„Ÿï¼‰ - 0.5ï¼ˆä¸æ•æ„Ÿï¼‰
```

---

## ğŸ‰ ä¼˜åŒ–æ•ˆæœé¢„æœŸ

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æ”¹è¿› |
|------|--------|--------|------|
| **æ ‡ç­¾æ³„æ¼é£é™©** | æœªçŸ¥ | è‡ªåŠ¨æ£€æµ‹ | âœ… æ¶ˆé™¤éšæ‚£ |
| **ç±»åˆ«ä¸å¹³è¡¡** | æœªå¤„ç† | æˆæœ¬æ„ŸçŸ¥å­¦ä¹  | âœ… æå‡å°‘æ•°ç±»å‡†ç¡®ç‡ |
| **æ¨¡å‹æ—¶æ•ˆæ€§** | é•¿æœŸç´¯ç§¯ | æ»‘åŠ¨çª—å£+æ¼‚ç§»æ£€æµ‹ | âœ… é€‚åº”å¸‚åœºå˜åŒ– |
| **é¢„æµ‹è´¨é‡** | å•ä¸€é¢„æµ‹ | é¢„æµ‹åŒºé—´+ä¸ç¡®å®šæ€§ | âœ… è¯†åˆ«ä½ä¿¡å¿ƒäº¤æ˜“ |
| **ç‰¹å¾å·¥ç¨‹** | äººå·¥å®¡æŸ¥ | è‡ªåŠ¨ç›‘æ§+æ¨è | âœ… åŠæ—¶å‘ç°é—®é¢˜ |
| **LONG/SHORTå¹³è¡¡** | æœªåˆ†æ | åˆ†æ–¹å‘è¯„ä¼° | âœ… æ¶ˆé™¤æ–¹å‘åå·® |

---

## ğŸ“ åç»­å»ºè®®

### **A/Bæµ‹è¯•é›†æˆæ¨¡å‹**
```python
# å•æ¨¡å‹ vs é›†æˆæ¨¡å‹ï¼ˆXGB+LGB+CatBoostï¼‰
# å¯¹æ¯”å®é™…PnLè€Œéå‡†ç¡®ç‡
```

### **åœ¨çº¿å­¦ä¹ æ›¿ä»£æ–¹æ¡ˆ**
```python
# è€ƒè™‘ River æˆ– Vowpal Wabbit
# çœŸæ­£çš„online learningï¼Œæ¯”å¢é‡è®­ç»ƒæ›´é€‚åº”æµæ•°æ®
```

### **ç›®æ ‡å˜é‡å‡çº§**
```python
# ä»äºŒåˆ†ç±» â†’ é¢„æµ‹æœŸæœ›æ”¶ç›Šï¼ˆè¿ç»­å€¼ï¼‰
target_optimizer = TargetOptimizer(target_type='risk_adjusted')
```

---

**ä¼˜åŒ–å®Œæˆæ—¥æœŸ**: 2025-10-27  
**ç‰ˆæœ¬**: v3.9.0  
**çŠ¶æ€**: âœ… å·²é›†æˆåˆ°è®­ç»ƒå™¨ï¼Œå¯ç«‹å³ä½¿ç”¨

**ğŸ¯ æ ¸å¿ƒä»·å€¼**: é€šè¿‡6ä¸ªä¼˜åŒ–æ¨¡å—ï¼Œç³»ç»Ÿç°åœ¨èƒ½å¤Ÿï¼š
1. è‡ªåŠ¨æ£€æµ‹æ•°æ®è´¨é‡é—®é¢˜ï¼ˆæ ‡ç­¾æ³„æ¼ï¼‰
2. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆæˆæœ¬æ„ŸçŸ¥å­¦ä¹ ï¼‰
3. é€‚åº”å¸‚åœºå˜åŒ–ï¼ˆæ»‘åŠ¨çª—å£+æ¼‚ç§»æ£€æµ‹ï¼‰
4. é‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§ï¼ˆBootstrapï¼‰
5. ç›‘æ§ç‰¹å¾å¥åº·åº¦ï¼ˆé‡è¦æ€§è¿½è¸ªï¼‰
6. æä¾›æ›´ç»†ç²’åº¦çš„è¯„ä¼°ï¼ˆåˆ†æ–¹å‘ã€é¢„æµ‹åŒºé—´ï¼‰
