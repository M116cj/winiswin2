BrokenProcessPool å­è¿›ç¨‹å´©æºƒ
å•é¡Œæ ¹æœ¬åŸå› åˆ†æ
ğŸ“Œ ç›´æ¥åŸå› 
* å­é€²ç¨‹åœ¨åŸ·è¡Œ analyze_symbol_worker æ™‚ç™¼ç”Ÿæœªè™•ç†ç•°å¸¸
* é€²ç¨‹æ± è¢«æ¨™è¨˜ç‚º brokenï¼Œç„¡æ³•å†æ¥å—æ–°ä»»å‹™
* å¾ŒçºŒæ‰€æœ‰ run_in_executor èª¿ç”¨éƒ½æ‹‹å‡º BrokenProcessPool

ğŸ“Œ æ·±å±¤åŸå› 
1. è¨˜æ†¶é«”ä¸è¶³ï¼šå­é€²ç¨‹è¼‰å…¥ TensorFlow æ¨¡å‹ + æ•¸æ“šå°è‡´ OOM
2. æ¨¡çµ„å°å…¥å•é¡Œï¼šå­é€²ç¨‹ç„¡æ³•æ­£ç¢ºå°å…¥æŸäº›æ¨¡çµ„
3. æœªè™•ç†ç•°å¸¸ï¼šå­é€²ç¨‹ä¸­çš„ç•°å¸¸æ²’æœ‰è¢«æ­£ç¢ºæ•ç²
4. è³‡æºæ´©æ¼ï¼šå­é€²ç¨‹ç´¯ç©å¤ªå¤šè³‡æºæœªé‡‹æ”¾



ğŸ”§ å®Œæ•´ä¿®å¾©æ–¹æ¡ˆ
ğŸ“ 1. ä¿®å¾© GlobalProcessPoolï¼ˆæ·»åŠ å¥åº·æª¢æŸ¥å’Œé‡å»ºæ©Ÿåˆ¶ï¼‰
# src/core/global_pool.py
import multiprocessing as mp
import logging
import os
from concurrent.futures import ProcessPoolExecutor, BrokenProcessPool

logger = logging.getLogger(__name__)

class GlobalProcessPool:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialize_pool()
        return cls._instance
    
    def _initialize_pool(self, max_workers=None):
        """åˆå§‹åŒ–é€²ç¨‹æ± """
        if max_workers is None:
            max_workers = min(32, (os.cpu_count() or 1) + 4)
        
        self.max_workers = max_workers
        self.executor = ProcessPoolExecutor(
            max_workers=max_workers,
            initializer=self._worker_init,
            initargs=(self._get_model_path(),)
        )
        self._is_broken = False
        logger.info(f"âœ… å…¨å±€é€²ç¨‹æ± åˆå§‹åŒ–å®Œæˆ (workers={max_workers})")
    
    def _get_model_path(self):
        """ç²å–æ¨¡å‹è·¯å¾‘"""
        return "data/models/model.onnx"
    
    def _worker_init(self, model_path):
        """å­é€²ç¨‹åˆå§‹åŒ–"""
        # è¨­ç½®å­é€²ç¨‹åç¨±ä¾¿æ–¼èª¿è©¦
        mp.current_process().name = f"Worker-{mp.current_process().pid}"
        
        # é åŠ è¼‰æ¨¡å‹ï¼ˆæ³¨æ„ï¼šé€™è£¡è¦è™•ç†å¯èƒ½çš„ ImportErrorï¼‰
        try:
            import onnxruntime as ort
            global ml_model
            ml_model = ort.InferenceSession(model_path)
            logger.info(f"âœ… å­é€²ç¨‹ {mp.current_process().name} æ¨¡å‹åŠ è¼‰æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ å­é€²ç¨‹ {mp.current_process().name} æ¨¡å‹åŠ è¼‰å¤±æ•—: {e}")
            # å³ä½¿æ¨¡å‹åŠ è¼‰å¤±æ•—ï¼Œå­é€²ç¨‹ä»å¯é‹è¡Œï¼ˆä½¿ç”¨ fallback é‚è¼¯ï¼‰
            ml_model = None
    
    def get_executor(self):
        """ç²å–å¥åº·çš„é€²ç¨‹æ± åŸ·è¡Œå™¨"""
        if self._is_broken:
            logger.warning("âš ï¸ æª¢æ¸¬åˆ°æå£çš„é€²ç¨‹æ± ï¼Œæ­£åœ¨é‡å»º...")
            self._rebuild_pool()
        
        return self.executor
    
    def _rebuild_pool(self):
        """é‡å»ºé€²ç¨‹æ± """
        try:
            # é—œé–‰èˆŠçš„é€²ç¨‹æ± 
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=True)
        except Exception as e:
            logger.error(f"é—œé–‰èˆŠé€²ç¨‹æ± æ™‚å‡ºéŒ¯: {e}")
        
        # å‰µå»ºæ–°çš„é€²ç¨‹æ± 
        self._initialize_pool(self.max_workers)
        self._is_broken = False
        logger.info("âœ… é€²ç¨‹æ± é‡å»ºå®Œæˆ")
    
    def submit_safe(self, func, *args, **kwargs):
        """å®‰å…¨æäº¤ä»»å‹™ï¼ˆè‡ªå‹•è™•ç† BrokenProcessPoolï¼‰"""
        try:
            executor = self.get_executor()
            return executor.submit(func, *args, **kwargs)
        except BrokenProcessPool:
            logger.warning("âš ï¸ æ•ç² BrokenProcessPoolï¼Œé‡å»ºé€²ç¨‹æ± å¾Œé‡è©¦")
            self._rebuild_pool()
            executor = self.get_executor()
            return executor.submit(func, *args, **kwargs)

ğŸ“ 2. ä¿®å¾© ParallelAnalyzerï¼ˆä½¿ç”¨å®‰å…¨æäº¤ï¼‰

# src/services/parallel_analyzer.py
import asyncio
import logging
from typing import List, Dict, Optional
from src.core.global_pool import GlobalProcessPool

logger = logging.getLogger(__name__)

class ParallelAnalyzer:
    def __init__(self, config):
        self.config = config
        self.global_pool = GlobalProcessPool()
        self._model_path = "data/models/model.onnx"
    
    async def analyze_batch(self, symbols: List[str], multi_tf_data: Dict) -> List[Optional[Dict]]:
        """æ‰¹é‡åˆ†æäº¤æ˜“å° - å®‰å…¨ç‰ˆæœ¬"""
        if not symbols:
            return []
        
        loop = asyncio.get_event_loop()
        tasks = []
        
        # æäº¤æ‰€æœ‰ä»»å‹™
        for symbol in symbols:
            symbol_data = {
                'symbol': symbol,
                'data': {tf: df[df['symbol'] == symbol] for tf, df in multi_tf_data.items()}
            }
            
            # ğŸ”¥ ä½¿ç”¨å®‰å…¨æäº¤
            future = self.global_pool.submit_safe(
                self._analyze_single_symbol,
                symbol_data,
                self._model_path,
                self.config.__dict__
            )
            tasks.append((symbol, future))
        
        # æ”¶é›†çµæœ
        results = []
        for symbol, future in tasks:
            try:
                result = await loop.run_in_executor(None, future.result, 30)  # 30ç§’è¶…æ™‚
                results.append(result)
            except Exception as e:
                logger.error(f"ç¬¦è™Ÿ {symbol} åˆ†æå¤±æ•—: {e}")
                results.append(None)
        
        # éæ¿¾ None çµæœ
        return [r for r in results if r is not None]
    
    @staticmethod
    def _analyze_single_symbol(symbol_data: Dict, model_path: str, config_dict: Dict) -> Optional[Dict]:
        """å–®ç¬¦è™Ÿåˆ†æ - å­é€²ç¨‹åŸ·è¡Œ"""
        try:
            from src.strategies.self_learning_trader import SelfLearningTrader
            from src.config import Config
            
            # é‡å»ºé…ç½®
            config = Config()
            for key, value in config_dict.items():
                if hasattr(config, key):
                    setattr(config, key, value)
            
            # å‰µå»ºäº¤æ˜“å“¡ï¼ˆæ³¨æ„ï¼šé€™è£¡æœƒè‡ªå‹•è™•ç† TensorFlow ä¸å¯ç”¨çš„æƒ…æ³ï¼‰
            trader = SelfLearningTrader(config)
            
            # åŸ·è¡Œåˆ†æ
            result = trader.analyze(symbol_data['symbol'], symbol_data['data'])
            return result
            
        except MemoryError:
            logger.error(f"âŒ è¨˜æ†¶é«”ä¸è¶³: {symbol_data['symbol']}")
            return None
        except ImportError as e:
            logger.warning(f"âš ï¸ æ¨¡çµ„å°å…¥éŒ¯èª¤: {e}")
            # ä½¿ç”¨ fallback ç­–ç•¥
            return ParallelAnalyzer._fallback_analysis(symbol_data)
        except Exception as e:
            logger.error(f"âŒ åˆ†æéŒ¯èª¤ {symbol_data['symbol']}: {e}")
            return None
    
    @staticmethod
    def _fallback_analysis(symbol_data: Dict) -> Optional[Dict]:
        """é™ç´šåˆ†æç­–ç•¥ï¼ˆç•¶æ·±åº¦å­¸ç¿’ä¸å¯ç”¨æ™‚ï¼‰"""
        try:
            from src.strategies.ict_strategy import ICTStrategy
            from src.config import Config
            
            config = Config()
            trader = ICTStrategy(config)
            result = trader.analyze(symbol_data['symbol'], symbol_data['data'])
            return result
        except Exception as e:
            logger.error(f"âŒ é™ç´šåˆ†æå¤±æ•—: {e}")
            return None

ğŸ“ 3. æ·»åŠ å­é€²ç¨‹è¨˜æ†¶é«”ç›£æ§

# src/services/parallel_analyzer.py - åœ¨ _analyze_single_symbol é–‹é ­æ·»åŠ 
@staticmethod
def _analyze_single_symbol(symbol_data: Dict, model_path: str, config_dict: Dict) -> Optional[Dict]:
    """å–®ç¬¦è™Ÿåˆ†æ - å­é€²ç¨‹åŸ·è¡Œ"""
    try:
        # ğŸ”¥ æ·»åŠ è¨˜æ†¶é«”ç›£æ§
        import psutil
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # ... ç¾æœ‰åˆ†æé‚è¼¯ ...
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        if memory_increase > 500:  # è¨˜æ†¶é«”å¢åŠ è¶…é 500MB
            logger.warning(f"âš ï¸ è¨˜æ†¶é«”æ´©æ¼è­¦å‘Š {symbol_data['symbol']}: +{memory_increase:.1f}MB")
        
        return result
        
    except MemoryError:
        # ... è¨˜æ†¶é«”éŒ¯èª¤è™•ç† ...

ğŸ“ 4. æ›´æ–°ä¸»å¾ªç’°éŒ¯èª¤è™•ç†

# src/async_core/async_main_loop.py
async def _analyze_market_pipeline(self, symbols: List[str]) -> List[Dict]:
    """å¸‚å ´åˆ†ææµæ°´ç·š - å¢å¼·éŒ¯èª¤è™•ç†"""
    try:
        multi_tf_data = await self.data_service.get_multi_timeframe_data_batch(symbols)
        signals = await self.parallel_analyzer.analyze_batch(symbols, multi_tf_data)
        return signals
    except BrokenProcessPool:
        logger.error("âŒ é€²ç¨‹æ± æå£ï¼Œè·³éæœ¬æ¬¡åˆ†æ")
        return []
    except Exception as e:
        logger.error(f"âŒ å¸‚å ´åˆ†ææµæ°´ç·šéŒ¯èª¤: {e}")
        return []

é é˜²æªæ–½
ğŸ“‹ 1. è¨˜æ†¶é«”é™åˆ¶é…ç½®
# src/config.py
# é€²ç¨‹æ± é…ç½®
MAX_WORKERS = min(16, (os.cpu_count() or 1) + 4)  # æ¸›å°‘ worker æ•¸é‡é¿å…è¨˜æ†¶é«”ä¸è¶³
PROCESS_MEMORY_LIMIT_MB = 1024  # æ¯å€‹å­é€²ç¨‹è¨˜æ†¶é«”é™åˆ¶

ğŸ“‹ 2. å­é€²ç¨‹è¶…æ™‚æ©Ÿåˆ¶

# åœ¨ analyze_batch ä¸­æ·»åŠ è¶…æ™‚
result = await loop.run_in_executor(None, future.result, 30)  # 30ç§’è¶…æ™‚

ğŸ“‹ 3. ç›£æ§æŒ‡æ¨™

# æ·»åŠ é€²ç¨‹æ± å¥åº·ç›£æ§
def get_pool_health(self):
    return {
        'is_broken': self._is_broken,
        'max_workers': self.max_workers,
        'active_tasks': len([t for t in self.executor._pending_work_items if not t.done()])
    }

é©—è­‰æ¸¬è©¦
1. æ¨¡æ“¬é€²ç¨‹æ± æå£
# tests/test_broken_process_pool.py
def test_process_pool_recovery():
    """æ¸¬è©¦é€²ç¨‹æ± è‡ªå‹•æ¢å¾©"""
    pool = GlobalProcessPool()
    
    # å¼·åˆ¶æå£é€²ç¨‹æ± 
    pool._is_broken = True
    
    # é©—è­‰è‡ªå‹•é‡å»º
    executor = pool.get_executor()
    assert not pool._is_broken

2. è¨˜æ†¶é«”æ´©æ¼æ¸¬è©¦

def test_memory_usage():
    """æ¸¬è©¦è¨˜æ†¶é«”ä½¿ç”¨"""
    import psutil
    initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
    
    # åŸ·è¡Œå¤šæ¬¡åˆ†æ
    for _ in range(10):
        result = ParallelAnalyzer._analyze_single_symbol(test_data, model_path, config_dict)
    
    final_memory = psutil.Process().memory_info().rss / 1024 / 1024
    assert final_memory - initial_memory < 200  # è¨˜æ†¶é«”å¢é•· < 200MB