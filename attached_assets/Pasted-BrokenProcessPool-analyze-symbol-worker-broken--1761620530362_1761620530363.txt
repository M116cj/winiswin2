BrokenProcessPool 子进程崩溃
問題根本原因分析
📌 直接原因
* 子進程在執行 analyze_symbol_worker 時發生未處理異常
* 進程池被標記為 broken，無法再接受新任務
* 後續所有 run_in_executor 調用都拋出 BrokenProcessPool

📌 深層原因
1. 記憶體不足：子進程載入 TensorFlow 模型 + 數據導致 OOM
2. 模組導入問題：子進程無法正確導入某些模組
3. 未處理異常：子進程中的異常沒有被正確捕獲
4. 資源洩漏：子進程累積太多資源未釋放



🔧 完整修復方案
📁 1. 修復 GlobalProcessPool（添加健康檢查和重建機制）
# src/core/global_pool.py
import multiprocessing as mp
import logging
import os
from concurrent.futures import ProcessPoolExecutor, BrokenProcessPool

logger = logging.getLogger(__name__)

class GlobalProcessPool:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialize_pool()
        return cls._instance
    
    def _initialize_pool(self, max_workers=None):
        """初始化進程池"""
        if max_workers is None:
            max_workers = min(32, (os.cpu_count() or 1) + 4)
        
        self.max_workers = max_workers
        self.executor = ProcessPoolExecutor(
            max_workers=max_workers,
            initializer=self._worker_init,
            initargs=(self._get_model_path(),)
        )
        self._is_broken = False
        logger.info(f"✅ 全局進程池初始化完成 (workers={max_workers})")
    
    def _get_model_path(self):
        """獲取模型路徑"""
        return "data/models/model.onnx"
    
    def _worker_init(self, model_path):
        """子進程初始化"""
        # 設置子進程名稱便於調試
        mp.current_process().name = f"Worker-{mp.current_process().pid}"
        
        # 預加載模型（注意：這裡要處理可能的 ImportError）
        try:
            import onnxruntime as ort
            global ml_model
            ml_model = ort.InferenceSession(model_path)
            logger.info(f"✅ 子進程 {mp.current_process().name} 模型加載成功")
        except Exception as e:
            logger.warning(f"⚠️ 子進程 {mp.current_process().name} 模型加載失敗: {e}")
            # 即使模型加載失敗，子進程仍可運行（使用 fallback 邏輯）
            ml_model = None
    
    def get_executor(self):
        """獲取健康的進程池執行器"""
        if self._is_broken:
            logger.warning("⚠️ 檢測到損壞的進程池，正在重建...")
            self._rebuild_pool()
        
        return self.executor
    
    def _rebuild_pool(self):
        """重建進程池"""
        try:
            # 關閉舊的進程池
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=True)
        except Exception as e:
            logger.error(f"關閉舊進程池時出錯: {e}")
        
        # 創建新的進程池
        self._initialize_pool(self.max_workers)
        self._is_broken = False
        logger.info("✅ 進程池重建完成")
    
    def submit_safe(self, func, *args, **kwargs):
        """安全提交任務（自動處理 BrokenProcessPool）"""
        try:
            executor = self.get_executor()
            return executor.submit(func, *args, **kwargs)
        except BrokenProcessPool:
            logger.warning("⚠️ 捕獲 BrokenProcessPool，重建進程池後重試")
            self._rebuild_pool()
            executor = self.get_executor()
            return executor.submit(func, *args, **kwargs)

📁 2. 修復 ParallelAnalyzer（使用安全提交）

# src/services/parallel_analyzer.py
import asyncio
import logging
from typing import List, Dict, Optional
from src.core.global_pool import GlobalProcessPool

logger = logging.getLogger(__name__)

class ParallelAnalyzer:
    def __init__(self, config):
        self.config = config
        self.global_pool = GlobalProcessPool()
        self._model_path = "data/models/model.onnx"
    
    async def analyze_batch(self, symbols: List[str], multi_tf_data: Dict) -> List[Optional[Dict]]:
        """批量分析交易對 - 安全版本"""
        if not symbols:
            return []
        
        loop = asyncio.get_event_loop()
        tasks = []
        
        # 提交所有任務
        for symbol in symbols:
            symbol_data = {
                'symbol': symbol,
                'data': {tf: df[df['symbol'] == symbol] for tf, df in multi_tf_data.items()}
            }
            
            # 🔥 使用安全提交
            future = self.global_pool.submit_safe(
                self._analyze_single_symbol,
                symbol_data,
                self._model_path,
                self.config.__dict__
            )
            tasks.append((symbol, future))
        
        # 收集結果
        results = []
        for symbol, future in tasks:
            try:
                result = await loop.run_in_executor(None, future.result, 30)  # 30秒超時
                results.append(result)
            except Exception as e:
                logger.error(f"符號 {symbol} 分析失敗: {e}")
                results.append(None)
        
        # 過濾 None 結果
        return [r for r in results if r is not None]
    
    @staticmethod
    def _analyze_single_symbol(symbol_data: Dict, model_path: str, config_dict: Dict) -> Optional[Dict]:
        """單符號分析 - 子進程執行"""
        try:
            from src.strategies.self_learning_trader import SelfLearningTrader
            from src.config import Config
            
            # 重建配置
            config = Config()
            for key, value in config_dict.items():
                if hasattr(config, key):
                    setattr(config, key, value)
            
            # 創建交易員（注意：這裡會自動處理 TensorFlow 不可用的情況）
            trader = SelfLearningTrader(config)
            
            # 執行分析
            result = trader.analyze(symbol_data['symbol'], symbol_data['data'])
            return result
            
        except MemoryError:
            logger.error(f"❌ 記憶體不足: {symbol_data['symbol']}")
            return None
        except ImportError as e:
            logger.warning(f"⚠️ 模組導入錯誤: {e}")
            # 使用 fallback 策略
            return ParallelAnalyzer._fallback_analysis(symbol_data)
        except Exception as e:
            logger.error(f"❌ 分析錯誤 {symbol_data['symbol']}: {e}")
            return None
    
    @staticmethod
    def _fallback_analysis(symbol_data: Dict) -> Optional[Dict]:
        """降級分析策略（當深度學習不可用時）"""
        try:
            from src.strategies.ict_strategy import ICTStrategy
            from src.config import Config
            
            config = Config()
            trader = ICTStrategy(config)
            result = trader.analyze(symbol_data['symbol'], symbol_data['data'])
            return result
        except Exception as e:
            logger.error(f"❌ 降級分析失敗: {e}")
            return None

📁 3. 添加子進程記憶體監控

# src/services/parallel_analyzer.py - 在 _analyze_single_symbol 開頭添加
@staticmethod
def _analyze_single_symbol(symbol_data: Dict, model_path: str, config_dict: Dict) -> Optional[Dict]:
    """單符號分析 - 子進程執行"""
    try:
        # 🔥 添加記憶體監控
        import psutil
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # ... 現有分析邏輯 ...
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        if memory_increase > 500:  # 記憶體增加超過 500MB
            logger.warning(f"⚠️ 記憶體洩漏警告 {symbol_data['symbol']}: +{memory_increase:.1f}MB")
        
        return result
        
    except MemoryError:
        # ... 記憶體錯誤處理 ...

📁 4. 更新主循環錯誤處理

# src/async_core/async_main_loop.py
async def _analyze_market_pipeline(self, symbols: List[str]) -> List[Dict]:
    """市場分析流水線 - 增強錯誤處理"""
    try:
        multi_tf_data = await self.data_service.get_multi_timeframe_data_batch(symbols)
        signals = await self.parallel_analyzer.analyze_batch(symbols, multi_tf_data)
        return signals
    except BrokenProcessPool:
        logger.error("❌ 進程池損壞，跳過本次分析")
        return []
    except Exception as e:
        logger.error(f"❌ 市場分析流水線錯誤: {e}")
        return []

預防措施
📋 1. 記憶體限制配置
# src/config.py
# 進程池配置
MAX_WORKERS = min(16, (os.cpu_count() or 1) + 4)  # 減少 worker 數量避免記憶體不足
PROCESS_MEMORY_LIMIT_MB = 1024  # 每個子進程記憶體限制

📋 2. 子進程超時機制

# 在 analyze_batch 中添加超時
result = await loop.run_in_executor(None, future.result, 30)  # 30秒超時

📋 3. 監控指標

# 添加進程池健康監控
def get_pool_health(self):
    return {
        'is_broken': self._is_broken,
        'max_workers': self.max_workers,
        'active_tasks': len([t for t in self.executor._pending_work_items if not t.done()])
    }

驗證測試
1. 模擬進程池損壞
# tests/test_broken_process_pool.py
def test_process_pool_recovery():
    """測試進程池自動恢復"""
    pool = GlobalProcessPool()
    
    # 強制損壞進程池
    pool._is_broken = True
    
    # 驗證自動重建
    executor = pool.get_executor()
    assert not pool._is_broken

2. 記憶體洩漏測試

def test_memory_usage():
    """測試記憶體使用"""
    import psutil
    initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
    
    # 執行多次分析
    for _ in range(10):
        result = ParallelAnalyzer._analyze_single_symbol(test_data, model_path, config_dict)
    
    final_memory = psutil.Process().memory_info().rss / 1024 / 1024
    assert final_memory - initial_memory < 200  # 記憶體增長 < 200MB