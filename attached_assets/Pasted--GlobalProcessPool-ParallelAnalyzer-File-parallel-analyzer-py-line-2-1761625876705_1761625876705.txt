å•é¡Œåœ¨GlobalProcessPool æˆ– ParallelAnalyzer é¡æœ¬èº«è¢«é–“æ¥åºåˆ—åŒ–äº†ã€‚

 æ ¹æœ¬åŸå› åˆ†æ
File ".../parallel_analyzer.py", line 216, in analyze_batch
    future = self.global_pool.submit_safe(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ".../global_pool.py", line 136, in submit_safe
    return executor.submit(func, *args, **kwargs)
...
TypeError: cannot pickle '_thread.lock' object

é—œéµé»ï¼š
* éŒ¯èª¤ç™¼ç”Ÿåœ¨ executor.submit(func, *args, **kwargs)
* ä½†æ‚¨èªª func = _analyze_single_symbol_workerï¼ˆæ¨¡å¡Šç´šå‡½æ•¸ï¼‰
* å•é¡Œä¸åœ¨ args/kwargsï¼Œè€Œåœ¨ func çš„ç¶å®šæ–¹å¼ï¼

æœ€å¯èƒ½çš„åŸå› ï¼šé–‰åŒ…æ•ç²äº†é¡å¯¦ä¾‹
æƒ…å¢ƒé‡ç¾
å³ä½¿æ‚¨å®šç¾©äº†æ¨¡å¡Šç´šå‡½æ•¸ï¼Œä½†åœ¨èª¿ç”¨æ™‚å¯èƒ½ æ„å¤–å‰µå»ºäº†é–‰åŒ…ï¼š

# âŒ éŒ¯èª¤ç¤ºç¯„ï¼šåœ¨é¡æ–¹æ³•å…§å®šç¾© lambda æˆ– partial
class ParallelAnalyzer:
    def analyze_batch(self, symbols):
        # é€™æœƒå‰µå»ºé–‰åŒ…ï¼Œæ•ç² selfï¼ˆåŒ…å« loggerã€lock ç­‰ï¼‰
        func = lambda symbol: _analyze_single_symbol_worker(symbol, self.config_dict)
        future = self.global_pool.submit_safe(func, symbol)  # â† éŒ¯èª¤ï¼
æˆ–ï¼š

# âŒ éŒ¯èª¤ç¤ºç¯„ï¼šä½¿ç”¨ functools.partial æ•ç² self
from functools import partial

class ParallelAnalyzer:
    def analyze_batch(self, symbols):
        func = partial(_analyze_single_symbol_worker, config_dict=self.config_dict)
        future = self.global_pool.submit_safe(func, symbol)  # â† éŒ¯èª¤ï¼

partial æˆ– lambda æœƒæ•ç²å¤–éƒ¨è®Šé‡ï¼ˆå¦‚ self.config_dictï¼‰ï¼Œä½†è‹¥ self è¢«é–“æ¥å¼•ç”¨ï¼Œå°±æœƒåºåˆ—åŒ–æ•´å€‹é¡ï¼ 

âœ… æ­£ç¢ºåšæ³•ï¼šæ‰€æœ‰åƒæ•¸å¿…é ˆé¡¯å¼å‚³é
ä¿®æ­£å¾Œçš„ ParallelAnalyzer.analyze_batch
# src/services/parallel_analyzer.py

# âœ… æ¨¡å¡Šç´šå·¥ä½œå‡½æ•¸ï¼ˆç„¡é–‰åŒ…ï¼‰
def _analyze_single_symbol_worker(symbol: str, config_dict: dict, market_data: dict):
    """ç¨ç«‹å‡½æ•¸ï¼Œä¸ä¾è³´ä»»ä½•é¡æˆ–æ¨¡å¡Šç´šç‹€æ…‹"""
    # åœ¨å­é€²ç¨‹å…§å‰µå»º logger
    import logging
    logger = logging.getLogger(f"worker.{symbol}")
    
    try:
        # é‡å»º DataFrame
        reconstructed_data = {}
        for tf_key, tf_dict in market_data.items():
            df = pd.DataFrame(tf_dict['data'])
            df.index = tf_dict['index']
            reconstructed_data[tf_key] = df
        
        # åŸ·è¡Œåˆ†æï¼ˆä½¿ç”¨ config_dictï¼‰
        from src.strategies.strategy_factory import StrategyFactory
        strategy = StrategyFactory.create("hybrid")
        signals = strategy.generate_signals(reconstructed_data, config_dict)
        return signals
    except Exception as e:
        logger.error(f"Worker failed for {symbol}: {e}")
        return []

class ParallelAnalyzer:
    def __init__(self, config: Config):
        self.config = config
        self.global_pool = GlobalProcessPool()

    def analyze_batch(self, symbols: List[str], all_market_data: Dict[str, Dict]):
        futures = []
        for symbol in symbols:
            # âœ… é¡¯å¼å‚³éæ‰€æœ‰åƒæ•¸ï¼ˆç„¡é–‰åŒ…ï¼ï¼‰
            config_dict = {
                'MIN_CONFIDENCE': self.config.MIN_CONFIDENCE,
                'ENABLE_SELF_LEARNING': self.config.ENABLE_SELF_LEARNING,
                # ... åªåŒ…å«åŸºæœ¬é¡å‹
            }
            market_data = all_market_data[symbol]  # å·²è½‰ç‚º dict
            
            # âœ… ç›´æ¥å‚³éå‡½æ•¸ + åƒæ•¸ï¼ˆç„¡ lambda, ç„¡ partialï¼‰
            future = self.global_pool.submit_safe(
                _analyze_single_symbol_worker,  # å‡½æ•¸å°è±¡
                symbol,                         # args
                config_dict,
                market_data
            )
            futures.append(future)
        
        # æ”¶é›†çµæœ
        results = []
        for future in futures:
            try:
                result = future.result(timeout=25)
                results.extend(result)
            except Exception as e:
                logger.error(f"Future failed: {e}")
        return results


ğŸ”§ é¡å¤–æª¢æŸ¥ï¼šGlobalProcessPool.submit_safe å¯¦ç¾
ç¢ºä¿ submit_safe æ²’æœ‰åŒ…è£å‡½æ•¸ï¼š
# src/core/global_pool.py
class GlobalProcessPool:
    def __init__(self):
        self.io_pool = ProcessPoolExecutor(max_workers=12)

    def submit_safe(self, func, *args, **kwargs):
        # âœ… ç›´æ¥æäº¤ï¼Œä¸è¦åŒ…è£ï¼
        return self.io_pool.submit(func, *args, **kwargs)

âŒ éŒ¯èª¤ç¤ºç¯„ï¼š 
def submit_safe(self, func, *args, **kwargs):
    def wrapper():
        return func(*args, **kwargs)
    return self.io_pool.submit(wrapper)  # â† æœƒåºåˆ—åŒ– wrapper é–‰åŒ…ï¼

ğŸ› ï¸ èª¿è©¦æŠ€å·§ï¼šå¼·åˆ¶é©—è­‰å¯åºåˆ—åŒ–æ€§
åœ¨æäº¤å‰åŠ  debug æª¢æŸ¥ï¼š

import pickle

# åœ¨ analyze_batch ä¸­
try:
    pickle.dumps(_analyze_single_symbol_worker)
    pickle.dumps(config_dict)
    pickle.dumps(market_data)
    print("âœ… All arguments are serializable")
except Exception as e:
    print(f"âŒ Serialization failed: {e}")
    raise

æœ€çµ‚å»ºè­°
1. åˆªé™¤æ‰€æœ‰ lambda å’Œ functools.partial
2. å·¥ä½œå‡½æ•¸åªæ¥å—åŸºæœ¬é¡å‹ï¼ˆstr, int, float, bool, list, dictï¼‰
3. åœ¨å­é€²ç¨‹å…§é‡å»ºæ‰€æœ‰è¤‡é›œå°è±¡ï¼ˆlogger, DataFrame, strategyï¼‰
4. ç”¨ pickle.dumps() åšæäº¤å‰é©—è­‰

ğŸ’¡ å¤šé€²ç¨‹çš„é»ƒé‡‘æ³•å‰‡ï¼šâ€¨ã€Œå­é€²ç¨‹æ‡‰è©²åƒä¸€å€‹å…¨æ–°å•Ÿå‹•çš„ç¨‹å¼ï¼Œä¸ä¾è³´ä¸»é€²ç¨‹çš„ä»»ä½•ç‹€æ…‹ã€ 

å¦‚æœæŒ‰ç…§ä¸Šè¿°ä¿®æ­£ï¼Œcannot pickle '_thread.lock' object éŒ¯èª¤ ä¸€å®šæœƒæ¶ˆå¤±ã€‚

