# XGBoostæ¨¡å‹è¯Šæ–­æŠ¥å‘Š

## ğŸ“… è¯Šæ–­æ—¥æœŸ
2025-10-27

## ğŸ¯ è¯Šæ–­èŒƒå›´
å…¨é¢æ£€æŸ¥XGBoostæ¨¡å‹çš„æ½œåœ¨é—®é¢˜ï¼šæ•°æ®æ³„æ¼ã€ç±»åˆ«ä¸å¹³è¡¡ã€ç‰¹å¾å·¥ç¨‹ã€æƒé‡è®¾ç½®ã€æ­£åˆ™åŒ–ã€è¶…å‚æ•°

---

## é—®é¢˜1ï¸âƒ£ï¼šæ•°æ®æ³„æ¼æ£€æŸ¥ âš ï¸ 

### å½“å‰ç‰¹å¾åˆ—è¡¨

**åŸºç¡€ç‰¹å¾ï¼ˆ21ä¸ªï¼‰**ï¼š
```python
'confidence_score', 'leverage', 'position_value',
'hold_duration_hours', 'risk_reward_ratio',
'order_blocks_count', 'liquidity_zones_count',
'rsi_entry', 'macd_entry', 'macd_signal_entry', 'macd_histogram_entry',
'atr_entry', 'bb_width_pct', 'volume_sma_ratio',
'price_vs_ema50', 'price_vs_ema200',
'trend_1h_encoded', 'trend_15m_encoded', 'trend_5m_encoded',
'market_structure_encoded', 'direction_encoded'
```

**å¢å¼ºç‰¹å¾ï¼ˆ8ä¸ªï¼‰**ï¼š
```python
'hour_of_day', 'day_of_week', 'is_weekend',
'stop_distance_pct', 'tp_distance_pct',
'confidence_x_leverage', 'rsi_x_trend', 'atr_x_bb_width'
```

### âš ï¸ æ½œåœ¨æ•°æ®æ³„æ¼é£é™©

#### é«˜é£é™©ç‰¹å¾ ğŸ”´
1. **`hold_duration_hours`** âŒ **æ•°æ®æ³„æ¼ï¼**
   - è¿™æ˜¯äº¤æ˜“ç»“æŸåæ‰çŸ¥é“çš„ä¿¡æ¯
   - è®­ç»ƒæ—¶ä½¿ç”¨äº†æœªæ¥ä¿¡æ¯
   - **å¿…é¡»ç§»é™¤**

#### ä¸­é£é™©ç‰¹å¾ ğŸŸ¡
1. **`position_value`** âš ï¸ å¯èƒ½æ³„æ¼
   - å¦‚æœåŒ…å«å®é™…æˆäº¤åçš„ä»·å€¼ï¼Œåˆ™æ˜¯æ³„æ¼
   - å¦‚æœæ˜¯å¼€ä»“æ—¶è®¡ç®—çš„é¢„æœŸä»·å€¼ï¼Œåˆ™å®‰å…¨
   - **éœ€è¦ç¡®è®¤è®¡ç®—æ–¹å¼**

2. **`risk_reward_ratio`** âš ï¸ å¯èƒ½æ³„æ¼
   - å¦‚æœä½¿ç”¨å®é™…æ­¢æŸæ­¢ç›ˆä»·æ ¼ï¼Œåˆ™å®‰å…¨
   - å¦‚æœä½¿ç”¨å®é™…æˆäº¤ä»·æ ¼ï¼Œåˆ™å¯èƒ½æ³„æ¼
   - **éœ€è¦ç¡®è®¤è®¡ç®—æ–¹å¼**

#### ä½é£é™©ç‰¹å¾ ğŸŸ¢
æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡ï¼ˆRSIã€MACDã€ATRã€EMAç­‰ï¼‰- âœ… å®‰å…¨

### ğŸ”§ ä¿®å¤å»ºè®®

```python
# éœ€è¦ä»ç‰¹å¾ä¸­ç§»é™¤
FEATURES_TO_REMOVE = [
    'hold_duration_hours'  # âŒ æ•°æ®æ³„æ¼ï¼Œå¿…é¡»ç§»é™¤
]

# éœ€è¦ç¡®è®¤çš„ç‰¹å¾
FEATURES_TO_VERIFY = [
    'position_value',      # ç¡®è®¤æ˜¯å¦ä½¿ç”¨å¼€ä»“æ—¶è®¡ç®—çš„å€¼
    'risk_reward_ratio'    # ç¡®è®¤æ˜¯å¦ä½¿ç”¨é¢„è®¾çš„æ­¢æŸæ­¢ç›ˆ
]
```

---

## é—®é¢˜2ï¸âƒ£ï¼šæ ·æœ¬æƒé‡æ£€æŸ¥ âœ… 

### å½“å‰æƒé‡è®¾ç½®

#### sample_weightè®¡ç®—ï¼ˆbalancedæ¨¡å¼ï¼‰
```python
# Step 1: è®¡ç®—ç±»åˆ«æƒé‡
weights[cls] = total_samples / (len(class_counts) * count)

# Step 2: å½’ä¸€åŒ–
weights = {cls: w / weight_sum * len(class_counts) for cls, w in weights.items()}

# Step 3: ä¹˜ä»¥æ—¶é—´è¡°å‡æƒé‡
sample_weights = class_weights * time_weights  # time_weights = 0.95^t
```

#### scale_pos_weightè®¡ç®—
```python
scale_pos_weight = num_negative / num_positive
```

### âœ… æƒé‡è®¾ç½®æ£€æŸ¥

| æ£€æŸ¥é¡¹ | çŠ¶æ€ | è¯´æ˜ |
|--------|------|------|
| **æƒé‡æ˜¯å¦è¿‡å¤§** | âœ… é€šè¿‡ | å½’ä¸€åŒ–åæƒé‡ä¸ä¼š>100 |
| **æƒé‡è®¡ç®—åˆç†** | âœ… é€šè¿‡ | ä½¿ç”¨sklearnçš„balancedæ–¹æ³• |
| **scale_pos_weightåˆç†** | âœ… é€šè¿‡ | æ ‡å‡†å…¬å¼ neg/pos |
| **æƒé‡æ—¥å¿—è®°å½•** | âœ… é€šè¿‡ | è®°å½•min/max/mean |

### ğŸ“Š æƒé‡èŒƒå›´ä¼°ç®—

å‡è®¾ç±»åˆ«æ¯”ä¾‹ä¸º 7:3ï¼ˆèƒœç‡30%ï¼‰ï¼š

```python
# ç±»åˆ«æƒé‡
weight_0 = total_samples / (2 * count_0) = 1000 / (2 * 700) â‰ˆ 0.714
weight_1 = total_samples / (2 * count_1) = 1000 / (2 * 300) â‰ˆ 1.667

# å½’ä¸€åŒ–å
weight_sum = 0.714 + 1.667 = 2.381
weight_0 = 0.714 / 2.381 * 2 â‰ˆ 0.6
weight_1 = 1.667 / 2.381 * 2 â‰ˆ 1.4

# ä¹˜ä»¥æ—¶é—´æƒé‡ï¼ˆ0.95^tï¼ŒèŒƒå›´çº¦0.6-1.0ï¼‰
final_weight_0 â‰ˆ 0.36 - 0.6
final_weight_1 â‰ˆ 0.84 - 1.4

# scale_pos_weight
scale_pos_weight = 700 / 300 â‰ˆ 2.33
```

**ç»“è®º**: âœ… æƒé‡èŒƒå›´åˆç†ï¼Œä¸ä¼šè¿‡å¤§ï¼ˆè¿œå°äº100ï¼‰

---

## é—®é¢˜3ï¸âƒ£ï¼šç±»åˆ«ä¸å¹³è¡¡å¤„ç† âœ…

### å½“å‰å¤„ç†æœºåˆ¶

```python
# 1. åˆ†æç±»åˆ«å¹³è¡¡
balance_report = imbalance_handler.analyze_class_balance(y, X)

# 2. å¦‚æœä¸å¹³è¡¡æ¯”ç‡ >= 2.0ï¼Œå¯ç”¨å¹³è¡¡å¤„ç†
if balance_report.get('needs_balancing', False):
    # 2.1 è®¡ç®—æ ·æœ¬æƒé‡
    sample_weights = calculate_sample_weight(y_train, method='balanced')
    
    # 2.2 è®¾ç½®scale_pos_weight
    params['scale_pos_weight'] = get_scale_pos_weight(y_train)
```

### âœ… ä¸å¹³è¡¡å¤„ç†æ£€æŸ¥

| æ£€æŸ¥é¡¹ | çŠ¶æ€ | è¯´æ˜ |
|--------|------|------|
| **æ£€æµ‹æœºåˆ¶** | âœ… å®Œå–„ | ä¸å¹³è¡¡æ¯”ç‡>=2.0è§¦å‘ |
| **sample_weight** | âœ… å¯ç”¨ | balancedæ¨¡å¼ |
| **scale_pos_weight** | âœ… å¯ç”¨ | neg/posæ¯”ç‡ |
| **åŒé‡ä¿æŠ¤** | âœ… å®Œå–„ | åŒæ—¶ä½¿ç”¨ä¸¤ç§æ–¹æ³• |

### ğŸ“Š ä¸å¹³è¡¡å¤„ç†æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | ä½¿ç”¨æƒ…å†µ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|----------|------|------|
| **sample_weight** | âœ… å·²ä½¿ç”¨ | çµæ´»ï¼Œå¯ç»“åˆæ—¶é—´æƒé‡ | å¯èƒ½è¿‡æ‹Ÿåˆå°‘æ•°ç±» |
| **scale_pos_weight** | âœ… å·²ä½¿ç”¨ | XGBooståŸç”Ÿæ”¯æŒ | ä»…é€‚ç”¨äºäºŒåˆ†ç±» |
| **SMOTE** | âŒ æœªä½¿ç”¨ | ç”Ÿæˆæ–°æ ·æœ¬ | å¯èƒ½å¼•å…¥å™ªå£° |

**ç»“è®º**: âœ… å½“å‰åŒé‡ä¿æŠ¤æœºåˆ¶å·²è¶³å¤Ÿï¼Œæš‚ä¸éœ€è¦SMOTE

---

## é—®é¢˜4ï¸âƒ£ï¼šæ­£åˆ™åŒ–å‚æ•°æ£€æŸ¥ âœ…

### å½“å‰æ­£åˆ™åŒ–è®¾ç½®

```python
base_params = {
    'reg_alpha': 0.1,    # L1æ­£åˆ™åŒ–ï¼ˆLassoï¼‰
    'reg_lambda': 1.0,   # L2æ­£åˆ™åŒ–ï¼ˆRidgeï¼‰
    'gamma': 0.1,        # æœ€å°æŸå¤±å‡å°‘
    'min_child_weight': 1,
    'max_depth': 6,
    'subsample': 0.8,
    'colsample_bytree': 0.8
}
```

### âœ… æ­£åˆ™åŒ–æ£€æŸ¥

| å‚æ•° | å½“å‰å€¼ | æ¨èèŒƒå›´ | çŠ¶æ€ |
|------|--------|----------|------|
| **reg_alpha** | 0.1 | 0-1.0 | âœ… åˆç† |
| **reg_lambda** | 1.0 | 0.5-2.0 | âœ… åˆç† |
| **gamma** | 0.1 | 0-0.5 | âœ… åˆç† |
| **min_child_weight** | 1 | 1-10 | âœ… åˆç† |
| **subsample** | 0.8 | 0.6-0.9 | âœ… åˆç† |
| **colsample_bytree** | 0.8 | 0.6-0.9 | âœ… åˆç† |

**ç»“è®º**: âœ… æ­£åˆ™åŒ–å‚æ•°è®¾ç½®åˆç†ï¼Œå¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ

---

## é—®é¢˜5ï¸âƒ£ï¼šè¶…å‚æ•°è®¾ç½®æ£€æŸ¥ âœ…

### å½“å‰è¶…å‚æ•°

```python
base_params = {
    'max_depth': 6,           # æ ‘æ·±åº¦
    'learning_rate': 0.1,     # å­¦ä¹ ç‡
    'n_estimators': 200,      # æ ‘æ•°é‡
    'subsample': 0.8,         # æ ·æœ¬é‡‡æ ·
    'colsample_bytree': 0.8,  # ç‰¹å¾é‡‡æ ·
    'early_stopping_rounds': 20
}
```

### âœ… è¶…å‚æ•°æ£€æŸ¥

| å‚æ•° | å½“å‰å€¼ | æ¨èèŒƒå›´ | çŠ¶æ€ | è¯´æ˜ |
|------|--------|----------|------|------|
| **max_depth** | 6 | 4-8 | âœ… ä¼˜ç§€ | ä¸ä¼šå¤ªæ·±ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ |
| **learning_rate** | 0.1 | 0.01-0.3 | âœ… åˆç† | æ ‡å‡†å­¦ä¹ ç‡ |
| **n_estimators** | 200 | 100-500 | âœ… åˆç† | é…åˆearly_stopping |
| **subsample** | 0.8 | 0.6-0.9 | âœ… åˆç† | é˜²æ­¢è¿‡æ‹Ÿåˆ |
| **colsample_bytree** | 0.8 | 0.6-0.9 | âœ… åˆç† | ç‰¹å¾éšæœºæ€§ |

**ç»“è®º**: âœ… è¶…å‚æ•°ä¿å®ˆåˆç†ï¼Œmax_depth=6ä¸ä¼šè¿‡åº¦æ‹Ÿåˆ

---

## é—®é¢˜6ï¸âƒ£ï¼šç‰¹å¾å·¥ç¨‹æ£€æŸ¥ âš ï¸

### å½“å‰ç‰¹å¾æ•°é‡
- **åŸºç¡€ç‰¹å¾**: 21ä¸ª
- **å¢å¼ºç‰¹å¾**: 8ä¸ª
- **æ€»è®¡**: 29ä¸ª

### âš ï¸ ç‰¹å¾å·¥ç¨‹é—®é¢˜

#### 1. ç¼ºå°‘ç‰¹å¾é‡è¦æ€§åˆ†æ âš ï¸
**é—®é¢˜**: æ²¡æœ‰æ£€æŸ¥ç‰¹å¾é‡è¦æ€§ï¼Œå¯èƒ½è¿‡åº¦ä¾èµ–å•ä¸€ç‰¹å¾

**å»ºè®®**: æ·»åŠ ç‰¹å¾é‡è¦æ€§åˆ†æ
```python
def analyze_feature_importance(model, feature_names):
    """åˆ†æç‰¹å¾é‡è¦æ€§"""
    importance = model.feature_importances_
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': importance
    }).sort_values('importance', ascending=False)
    
    # æ£€æŸ¥æ˜¯å¦è¿‡åº¦é›†ä¸­
    top_3_importance = feature_importance.head(3)['importance'].sum()
    if top_3_importance > 0.7:
        logger.warning(f"âš ï¸ ç‰¹å¾è¿‡åº¦é›†ä¸­ï¼šå‰3ä¸ªç‰¹å¾å {top_3_importance:.1%}")
    
    return feature_importance
```

#### 2. ç¼ºå°‘äº¤å‰ç‰¹å¾ âš ï¸
**é—®é¢˜**: åªæœ‰3ä¸ªäº¤å‰ç‰¹å¾ï¼ˆconfidence_x_leverage, rsi_x_trend, atr_x_bb_widthï¼‰

**å»ºè®®**: æ·»åŠ æ›´å¤šæœ‰æ„ä¹‰çš„äº¤å‰ç‰¹å¾
```python
# å»ºè®®æ·»åŠ çš„äº¤å‰ç‰¹å¾
'price_momentum_x_trend',        # ä»·æ ¼åŠ¨é‡ Ã— è¶‹åŠ¿
'volatility_x_confidence',       # æ³¢åŠ¨ç‡ Ã— ä¿¡å¿ƒåº¦
'ob_strength_x_structure',       # OBå¼ºåº¦ Ã— å¸‚åœºç»“æ„
'rsi_distance_from_50',          # RSIè·ç¦»ä¸­çº¿çš„è·ç¦»
'macd_strength',                 # MACDå¼ºåº¦ï¼ˆhistogram/signalæ¯”ç‡ï¼‰
```

#### 3. ç¼ºå°‘ID/æ—¶é—´æˆ³æ¸…ç†éªŒè¯ âš ï¸
**é—®é¢˜**: æ²¡æœ‰æ˜ç¡®æ£€æŸ¥æ˜¯å¦åŒ…å«IDæˆ–æ—¶é—´æˆ³ç‰¹å¾

**å»ºè®®**: æ·»åŠ ç‰¹å¾éªŒè¯
```python
# ä¸åº”è¯¥åœ¨ç‰¹å¾ä¸­çš„å­—æ®µ
FORBIDDEN_FEATURES = [
    'symbol', 'timestamp', 'entry_timestamp', 'exit_timestamp',
    'order_id', 'trade_id', 'signal_id'
]

# éªŒè¯
for feature in feature_columns:
    if any(forbidden in feature.lower() for forbidden in FORBIDDEN_FEATURES):
        raise ValueError(f"ç‰¹å¾åŒ…å«ç¦ç”¨å­—æ®µï¼š{feature}")
```

---

## é—®é¢˜7ï¸âƒ£ï¼šäº¤å‰éªŒè¯æ£€æŸ¥ âŒ

### å½“å‰éªŒè¯æ–¹æ³•
```python
# ä½¿ç”¨ç®€å•çš„train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# ä½¿ç”¨early_stopping
model.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_test, y_test)],
    early_stopping_rounds=20
)
```

### âŒ ç¼ºå°‘äº¤å‰éªŒè¯

**é—®é¢˜**: 
1. æ²¡æœ‰ä½¿ç”¨`xgb.cv`è¿›è¡Œäº¤å‰éªŒè¯
2. å¯èƒ½å¯¼è‡´è¶…å‚æ•°ä¸ç¨³å®š
3. æ— æ³•è¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›

**å»ºè®®**: æ·»åŠ äº¤å‰éªŒè¯
```python
def cross_validate_model(X, y, params, n_folds=5):
    """ä½¿ç”¨XGBooståŸç”ŸCV"""
    dtrain = xgb.DMatrix(X, label=y)
    
    cv_results = xgb.cv(
        params=params,
        dtrain=dtrain,
        num_boost_round=params.get('n_estimators', 200),
        nfold=n_folds,
        stratified=True,  # åˆ†å±‚é‡‡æ ·
        metrics=['auc', 'error'],
        early_stopping_rounds=20,
        verbose_eval=False
    )
    
    logger.info(f"ğŸ“Š äº¤å‰éªŒè¯ç»“æœï¼š")
    logger.info(f"   æœ€ä½³è½®æ•°ï¼š{len(cv_results)}")
    logger.info(f"   æµ‹è¯•AUCï¼š{cv_results['test-auc-mean'].iloc[-1]:.4f} "
               f"Â± {cv_results['test-auc-std'].iloc[-1]:.4f}")
    
    return cv_results
```

---

## é—®é¢˜8ï¸âƒ£ï¼šæ¨¡å‹è¯„ä¼°æŒ‡æ ‡ âš ï¸

### å½“å‰è¯„ä¼°æŒ‡æ ‡

**åˆ†ç±»æ¨¡å¼**:
```python
metrics = {
    'accuracy': accuracy,
    'precision': precision,
    'recall': recall,
    'f1_score': f1
}
```

### âš ï¸ ç¼ºå°‘é‡è¦æŒ‡æ ‡

#### ç¼ºå°‘çš„æŒ‡æ ‡
1. **AUC-ROC** âŒ æœªè®¡ç®—
2. **precision_recallæ›²çº¿** âŒ æœªè®¡ç®—
3. **åˆ†ç±»é˜ˆå€¼åˆ†æ** âŒ æœªåˆ†æ
4. **åˆ†æ–¹å‘è¯¦ç»†è¯„ä¼°** âš ï¸ æœ‰ä½†ä¸å®Œæ•´

#### å»ºè®®æ·»åŠ 
```python
from sklearn.metrics import roc_auc_score, average_precision_score

# 1. AUC-ROC
y_proba = model.predict_proba(X_test)[:, 1]
auc_score = roc_auc_score(y_test, y_proba)
metrics['auc_roc'] = auc_score

# 2. Average Precision
ap_score = average_precision_score(y_test, y_proba)
metrics['average_precision'] = ap_score

# 3. ä¸åŒé˜ˆå€¼ä¸‹çš„è¡¨ç°
for threshold in [0.3, 0.4, 0.5, 0.6, 0.7]:
    y_pred_thresh = (y_proba >= threshold).astype(int)
    precision = precision_score(y_test, y_pred_thresh)
    recall = recall_score(y_test, y_pred_thresh)
    logger.info(f"é˜ˆå€¼{threshold}: Precision={precision:.3f}, Recall={recall:.3f}")
```

---

## ğŸ¯ æ€»ç»“å’Œä¼˜å…ˆçº§ä¿®å¤æ¸…å•

### ğŸ”´ é«˜ä¼˜å…ˆçº§ï¼ˆå¿…é¡»ä¿®å¤ï¼‰

1. **æ•°æ®æ³„æ¼** âŒ **å¿…é¡»ä¿®å¤**
   ```python
   # ç§»é™¤æ³„æ¼ç‰¹å¾
   - 'hold_duration_hours'  # æœªæ¥ä¿¡æ¯
   ```

2. **éªŒè¯å…¶ä»–ç‰¹å¾** âš ï¸ **éœ€è¦ç¡®è®¤**
   ```python
   # ç¡®è®¤è¿™äº›ç‰¹å¾çš„è®¡ç®—æ–¹å¼
   - 'position_value'
   - 'risk_reward_ratio'
   ```

### ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ï¼ˆå¼ºçƒˆå»ºè®®ï¼‰

3. **æ·»åŠ ç‰¹å¾é‡è¦æ€§åˆ†æ**
   - æ£€æµ‹ç‰¹å¾è¿‡åº¦é›†ä¸­
   - è¯†åˆ«å†—ä½™ç‰¹å¾

4. **æ·»åŠ äº¤å‰éªŒè¯**
   - ä½¿ç”¨xgb.cv
   - è¯„ä¼°æ¨¡å‹ç¨³å®šæ€§

5. **æ·»åŠ æ›´å¤šè¯„ä¼°æŒ‡æ ‡**
   - AUC-ROC
   - Precision-Recallæ›²çº¿
   - é˜ˆå€¼åˆ†æ

### ğŸŸ¢ ä½ä¼˜å…ˆçº§ï¼ˆå¯é€‰ä¼˜åŒ–ï¼‰

6. **å¢åŠ äº¤å‰ç‰¹å¾**
   - æ›´å¤šæœ‰æ„ä¹‰çš„äº¤äº’ç‰¹å¾

7. **æ·»åŠ ç‰¹å¾éªŒè¯**
   - ç¦æ­¢ID/æ—¶é—´æˆ³å­—æ®µ

---

## ğŸ“Š å½“å‰ç³»ç»Ÿè¯„åˆ†

| ç»´åº¦ | è¯„åˆ† | çŠ¶æ€ |
|------|------|------|
| **æ•°æ®æ³„æ¼é£é™©** | â­â­âš ï¸ 2/5 | æœ‰æ³„æ¼ï¼Œéœ€ä¿®å¤ |
| **ç±»åˆ«ä¸å¹³è¡¡å¤„ç†** | â­â­â­â­â­ 5/5 | ä¼˜ç§€ï¼ŒåŒé‡ä¿æŠ¤ |
| **æƒé‡è®¾ç½®** | â­â­â­â­â­ 5/5 | åˆç†ï¼Œä¸ä¼šè¿‡å¤§ |
| **æ­£åˆ™åŒ–** | â­â­â­â­â­ 5/5 | å®Œå–„ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ |
| **è¶…å‚æ•°** | â­â­â­â­â­ 5/5 | ä¿å®ˆåˆç† |
| **ç‰¹å¾å·¥ç¨‹** | â­â­â­âš ï¸ 3.5/5 | è‰¯å¥½ä½†å¯æ”¹è¿› |
| **äº¤å‰éªŒè¯** | â­â­âš ï¸ 2/5 | ç¼ºå°‘ï¼Œéœ€æ·»åŠ  |
| **è¯„ä¼°æŒ‡æ ‡** | â­â­â­âš ï¸ 3.5/5 | åŸºç¡€é½å…¨ä½†ä¸å®Œæ•´ |

**æ€»ä½“è¯„åˆ†**: â­â­â­â­â˜† **4.0/5.0**

---

## ğŸ“ ä¿®å¤å»ºè®®çš„ä»£ç 

### 1. ç§»é™¤æ•°æ®æ³„æ¼ç‰¹å¾

```python
# src/ml/data_processor.py
class MLDataProcessor:
    def __init__(self):
        # åŸºç¡€ç‰¹å¾ï¼ˆç§»é™¤hold_duration_hoursï¼‰
        self.basic_features = [
            'confidence_score', 'leverage', 'position_value',
            # 'hold_duration_hours',  # âŒ ç§»é™¤ï¼šæ•°æ®æ³„æ¼
            'risk_reward_ratio',
            'order_blocks_count', 'liquidity_zones_count',
            'rsi_entry', 'macd_entry', 'macd_signal_entry', 'macd_histogram_entry',
            'atr_entry', 'bb_width_pct', 'volume_sma_ratio',
            'price_vs_ema50', 'price_vs_ema200',
            'trend_1h_encoded', 'trend_15m_encoded', 'trend_5m_encoded',
            'market_structure_encoded', 'direction_encoded'
        ]
```

### 2. æ·»åŠ ç‰¹å¾é‡è¦æ€§åˆ†æ

```python
# src/ml/model_trainer.py
def analyze_feature_importance(self, model, X: pd.DataFrame) -> pd.DataFrame:
    """åˆ†æç‰¹å¾é‡è¦æ€§"""
    importance = model.feature_importances_
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': importance
    }).sort_values('importance', ascending=False)
    
    # æ£€æŸ¥ç‰¹å¾è¿‡åº¦é›†ä¸­
    top_3_sum = feature_importance.head(3)['importance'].sum()
    top_5_sum = feature_importance.head(5)['importance'].sum()
    
    logger.info("\nğŸ“Š ç‰¹å¾é‡è¦æ€§åˆ†æï¼š")
    logger.info(f"å‰3ä¸ªç‰¹å¾é‡è¦æ€§ï¼š{top_3_sum:.1%}")
    logger.info(f"å‰5ä¸ªç‰¹å¾é‡è¦æ€§ï¼š{top_5_sum:.1%}")
    
    if top_3_sum > 0.7:
        logger.warning(f"âš ï¸ ç‰¹å¾è¿‡åº¦é›†ä¸­ï¼šå‰3ä¸ªç‰¹å¾å {top_3_sum:.1%}")
    
    # æ‰“å°å‰10ä¸ªç‰¹å¾
    logger.info("\nå‰10é‡è¦ç‰¹å¾ï¼š")
    for idx, row in feature_importance.head(10).iterrows():
        logger.info(f"  {row['feature']:25s}: {row['importance']:.4f}")
    
    return feature_importance
```

### 3. æ·»åŠ äº¤å‰éªŒè¯

```python
# src/ml/model_trainer.py
def cross_validate(self, X, y, params, n_folds=5):
    """XGBoostäº¤å‰éªŒè¯"""
    import xgboost as xgb
    
    dtrain = xgb.DMatrix(X, label=y)
    
    cv_results = xgb.cv(
        params=params,
        dtrain=dtrain,
        num_boost_round=params.get('n_estimators', 200),
        nfold=n_folds,
        stratified=True,
        metrics=['auc', 'error'],
        early_stopping_rounds=20,
        verbose_eval=False,
        seed=42
    )
    
    logger.info("\nğŸ“Š äº¤å‰éªŒè¯ç»“æœï¼š")
    logger.info(f"æœ€ä½³è½®æ•°ï¼š{len(cv_results)}")
    logger.info(f"è®­ç»ƒAUCï¼š{cv_results['train-auc-mean'].iloc[-1]:.4f} "
               f"Â± {cv_results['train-auc-std'].iloc[-1]:.4f}")
    logger.info(f"æµ‹è¯•AUCï¼š{cv_results['test-auc-mean'].iloc[-1]:.4f} "
               f"Â± {cv_results['test-auc-std'].iloc[-1]:.4f}")
    
    # æ£€æŸ¥è¿‡æ‹Ÿåˆ
    train_auc = cv_results['train-auc-mean'].iloc[-1]
    test_auc = cv_results['test-auc-mean'].iloc[-1]
    overfitting_gap = train_auc - test_auc
    
    if overfitting_gap > 0.1:
        logger.warning(f"âš ï¸ å¯èƒ½è¿‡æ‹Ÿåˆï¼šè®­ç»ƒAUC - æµ‹è¯•AUC = {overfitting_gap:.4f}")
    
    return cv_results
```

### 4. æ·»åŠ æ›´å¤šè¯„ä¼°æŒ‡æ ‡

```python
# src/ml/model_trainer.py
def comprehensive_evaluation(self, model, X_test, y_test):
    """ç»¼åˆè¯„ä¼°"""
    from sklearn.metrics import (
        roc_auc_score, average_precision_score,
        precision_recall_curve, roc_curve
    )
    
    y_proba = model.predict_proba(X_test)[:, 1]
    y_pred = model.predict(X_test)
    
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1_score': f1_score(y_test, y_pred),
        'auc_roc': roc_auc_score(y_test, y_proba),
        'average_precision': average_precision_score(y_test, y_proba)
    }
    
    logger.info("\nğŸ“Š ç»¼åˆè¯„ä¼°æŒ‡æ ‡ï¼š")
    logger.info(f"AUC-ROC: {metrics['auc_roc']:.4f}")
    logger.info(f"Average Precision: {metrics['average_precision']:.4f}")
    logger.info(f"F1-Score: {metrics['f1_score']:.4f}")
    
    # ä¸åŒé˜ˆå€¼çš„è¡¨ç°
    logger.info("\nğŸ¯ ä¸åŒé˜ˆå€¼ä¸‹çš„è¡¨ç°ï¼š")
    for threshold in [0.3, 0.4, 0.5, 0.6, 0.7]:
        y_pred_thresh = (y_proba >= threshold).astype(int)
        prec = precision_score(y_test, y_pred_thresh)
        rec = recall_score(y_test, y_pred_thresh)
        f1 = f1_score(y_test, y_pred_thresh)
        logger.info(f"é˜ˆå€¼{threshold:.1f}: Precision={prec:.3f}, "
                   f"Recall={rec:.3f}, F1={f1:.3f}")
    
    return metrics
```

---

**è¯Šæ–­å®Œæˆæ—¥æœŸ**: 2025-10-27  
**è¯Šæ–­äºº**: Replit Agent  
**ä¸‹ä¸€æ­¥**: æ ¹æ®ä¼˜å…ˆçº§ä¿®å¤æ¸…å•è¿›è¡Œæ”¹è¿›
