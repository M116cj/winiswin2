"""
ðŸ’¾ Experience Buffer - Collect trading data for ML model training
Records every signal and its outcome for supervised learning
"""

import logging
import json
import asyncio
from typing import Dict, List, Optional
from datetime import datetime
import asyncpg
import uuid as uuid_module

logger = logging.getLogger(__name__)


class ExperienceBuffer:
    """
    Collects trading experiences (signals, outcomes) for ML training
    
    Flow:
    1. Record signal generated (features + confidence)
    2. Track order execution (actual price, size)
    3. Record trade result (PnL, win/loss)
    4. Use data to train ML model
    """
    
    def __init__(self, max_size: int = 10000):
        """
        Initialize experience buffer
        
        Args:
            max_size: Maximum number of experiences to keep in memory
        """
        self.max_size = max_size
        self.experiences: List[Dict] = []
        self.lock = asyncio.Lock()
    
    async def record_signal(self, signal_id: str, signal_data: Dict) -> None:
        """
        Record a new trading signal (çµ±ä¸€æ ¼å¼ + ç™¾åˆ†æ¯”æ”¶ç›ŠçŽ‡)
        
        Args:
            signal_id: Unique signal identifier
            signal_data: {symbol, confidence, features, position_size, predicted_return_pct, position_sizing, timestamp (milliseconds)}
        """
        try:
            async with self.lock:
                experience = {
                    'signal_id': signal_id,
                    'type': 'signal',
                    'symbol': signal_data.get('symbol', ''),
                    'timestamp': int(signal_data.get('timestamp', int(datetime.now().timestamp() * 1000))),  # âœ“ æ¯«ç§’
                    'features': signal_data.get('features', {}),
                    # âœ… NEW: Percentage return data
                    'predicted_return_pct': signal_data.get('predicted_return_pct', 0.0),
                    'position_sizing': signal_data.get('position_sizing', {}),
                    'order_amount': signal_data.get('order_amount', 0.0),
                    'tp_pct': signal_data.get('tp_pct', 0.0),
                    'sl_pct': signal_data.get('sl_pct', 0.0),
                    'outcome': None,
                    'recorded_at': int(datetime.now().timestamp() * 1000)  # âœ“ æ¯«ç§’
                }
                
                self.experiences.append(experience)
                
                # Keep buffer size bounded
                if len(self.experiences) > self.max_size:
                    self.experiences.pop(0)
                
                logger.debug(f"ðŸ“ Signal recorded: {signal_data['symbol']} @ {signal_data.get('confidence', 0.5):.2f} | Return +{signal_data.get('predicted_return_pct', 0):.2%}")
        
        except Exception as e:
            logger.error(f"âŒ Error recording signal: {e}", exc_info=True)
    
    async def record_trade_outcome(self, signal_id: str, trade_data: Dict) -> None:
        """
        Record the outcome of a trade (çµ±ä¸€æ ¼å¼ + ç™¾åˆ†æ¯”æ”¶ç›ŠçŽ‡çµæžœ)
        
        Args:
            signal_id: Unique signal identifier
            trade_data: {price, quantity, side, pnl, return_pct, status, close_reason}
        """
        try:
            async with self.lock:
                # Find corresponding signal
                for exp in self.experiences:
                    if exp.get('signal_id') == signal_id:
                        pnl = trade_data.get('pnl', 0)
                        exp['outcome'] = {
                            'entry_price': trade_data.get('entry_price', trade_data.get('price', 0)),
                            'exit_price': trade_data.get('exit_price', trade_data.get('price', 0)),
                            'quantity': trade_data.get('quantity', 0),
                            'side': trade_data.get('side', 'BUY'),
                            'pnl': pnl,
                            'pnl_percent': trade_data.get('pnl_percent', 0),
                            'status': trade_data.get('status', 'FILLED'),
                            'close_reason': trade_data.get('close_reason', 'UNKNOWN'),
                            'win': pnl > 0
                        }
                        exp['type'] = 'complete_trade'
                        logger.debug(f"âœ… Trade outcome recorded: PnL ${pnl:.2f}")
                        break
        
        except Exception as e:
            logger.error(f"âŒ Error recording outcome: {e}", exc_info=True)
    
    async def get_training_data(self) -> List[Dict]:
        """
        Get all recorded experiences with outcomes (for training)
        
        Returns:
            List of complete trades with features and labels
        """
        try:
            async with self.lock:
                # Filter only complete trades
                complete = [
                    exp for exp in self.experiences
                    if exp.get('type') == 'complete_trade'
                ]
                
                logger.info(f"ðŸ“Š Training data available: {len(complete)} complete trades")
                return complete
        
        except Exception as e:
            logger.error(f"âŒ Error getting training data: {e}")
            return []
    
    async def save_to_database(self, db_url: str) -> int:
        """
        Save buffer to Postgres for persistent storage (ä¿®å¾©ç‰ˆæœ¬ - èˆ‡å¯¦éš›è¡¨çµæ§‹åŒ¹é…)
        
        âœ… è¡¨çµæ§‹: experience_buffer (id, signal_id, features, outcome, created_at)
        âœ… åªåœ¨å…§å­˜ç·©è¡å€ä¸­å­˜å„²å®Œæ•´äº¤æ˜“ï¼ˆåŒ…å« outcomeï¼‰
        âœ… å°‡å…§å­˜çš„å®Œæ•´ experience å°è±¡åºåˆ—åŒ–ç‚º features JSONB å’Œ outcome JSONB
        
        Args:
            db_url: Database connection URL
        
        Returns:
            Number of experiences saved
        """
        conn = None
        try:
            conn = await asyncpg.connect(db_url)
            
            # æ’å…¥æ‰€æœ‰å®Œæ•´çš„äº¤æ˜“è¨˜éŒ„
            count = 0
            error_count = 0
            async with self.lock:
                for exp in self.experiences:
                    # åªå„²å­˜æœ‰ outcome çš„å®Œæ•´äº¤æ˜“
                    if exp.get('type') == 'complete_trade' and exp.get('outcome') is not None:
                        try:
                            # æ§‹å»º features JSONB - åŒ…å«ä¿¡è™Ÿçš„æ‰€æœ‰ç‰¹å¾µæ•¸æ“š
                            features_data = {
                                'symbol': exp.get('symbol', ''),
                                'timestamp': exp.get('timestamp', 0),
                                'features': exp.get('features', {}),
                                'predicted_return_pct': exp.get('predicted_return_pct', 0.0),
                                'position_sizing': exp.get('position_sizing', {}),
                                'order_amount': exp.get('order_amount', 0.0),
                                'tp_pct': exp.get('tp_pct', 0.0),
                                'sl_pct': exp.get('sl_pct', 0.0),
                                'recorded_at': exp.get('recorded_at', 0),
                                'type': exp.get('type', '')
                            }
                            
                            # è½‰æ› signal_id ç‚º UUIDï¼ˆå¦‚æžœæ˜¯å­—ç¬¦ä¸²ï¼‰
                            signal_id_val = exp.get('signal_id', '')
                            try:
                                # å˜—è©¦è½‰æ›ç‚º UUID
                                if signal_id_val and isinstance(signal_id_val, str):
                                    signal_id_uuid = uuid_module.UUID(signal_id_val)
                                else:
                                    # ç„¡æ•ˆçš„ signal_idï¼Œè·³éŽ
                                    logger.debug(f"âš ï¸ è·³éŽç„¡æ•ˆçš„ signal_id: {signal_id_val}")
                                    error_count += 1
                                    continue
                            except (ValueError, AttributeError) as uuid_err:
                                # ç„¡æ•ˆçš„ signal_idï¼Œè·³éŽæ­¤è¨˜éŒ„
                                logger.debug(f"âš ï¸ è·³éŽç„¡æ•ˆçš„ signal_id æ ¼å¼: {signal_id_val}, åŽŸå› : {uuid_err}")
                                error_count += 1
                                continue
                            
                            # INSERT INTO experience_buffer (signal_id, features, outcome)
                            await conn.execute("""
                                INSERT INTO experience_buffer (signal_id, features, outcome)
                                VALUES ($1, $2::jsonb, $3::jsonb)
                            """,
                                signal_id_uuid,
                                json.dumps(features_data),
                                json.dumps(exp.get('outcome', {}))
                            )
                            count += 1
                            logger.debug(f"âœ… ä¿å­˜æˆåŠŸ: signal_id={signal_id_uuid}")
                        
                        except asyncpg.UniqueViolationError as dup_err:
                            logger.debug(f"âš ï¸ é‡è¤‡è¨˜éŒ„ (signal_id å·²å­˜åœ¨): {dup_err}")
                            error_count += 1
                        
                        except Exception as e:
                            logger.debug(f"âš ï¸ ä¿å­˜ experience å¤±æ•—: {type(e).__name__}: {e}")
                            error_count += 1
            
            logger.critical(f"ðŸ’¾ æˆåŠŸä¿å­˜ {count} ç­† experience åˆ° PostgreSQL (å¤±æ•— {error_count} ç­†)")
            return count
        
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ°æ•¸æ“šåº«å¤±æ•—: {e}", exc_info=True)
            return 0
        
        finally:
            if conn:
                try:
                    await conn.close()
                except:
                    pass
    
    async def read_from_database(self, db_url: str, limit: int = 100) -> List[Dict]:
        """
        å¾ž PostgreSQL è®€å–å·²ä¿å­˜çš„ experience è¨˜éŒ„
        
        Args:
            db_url: Database connection URL
            limit: æœ€å¤šè®€å–è¨˜éŒ„æ•¸
        
        Returns:
            List of experiences from database
        """
        conn = None
        try:
            conn = await asyncpg.connect(db_url)
            
            # SELECT å¾ž experience_buffer è®€å–æœ€æ–°çš„è¨˜éŒ„
            rows = await conn.fetch("""
                SELECT id, signal_id, features, outcome, created_at
                FROM experience_buffer
                ORDER BY created_at DESC
                LIMIT $1
            """, limit)
            
            result = []
            for row in rows:
                try:
                    experience = {
                        'id': row['id'],
                        'signal_id': str(row['signal_id']) if row['signal_id'] else None,
                        'features': row['features'] if isinstance(row['features'], dict) else json.loads(row['features'] or '{}'),
                        'outcome': row['outcome'] if isinstance(row['outcome'], dict) else json.loads(row['outcome'] or '{}'),
                        'created_at': row['created_at']
                    }
                    result.append(experience)
                except Exception as e:
                    logger.debug(f"âš ï¸ è§£æžè¨˜éŒ„å¤±æ•—: {e}")
            
            logger.info(f"ðŸ“– å¾ž PostgreSQL è®€å– {len(result)} ç­† experience")
            return result
        
        except Exception as e:
            logger.error(f"âŒ å¾žæ•¸æ“šåº«è®€å–å¤±æ•—: {e}", exc_info=True)
            return []
        
        finally:
            if conn:
                try:
                    await conn.close()
                except:
                    pass
    
    async def get_database_stats(self, db_url: str) -> Dict:
        """
        ç²å– PostgreSQL experience_buffer è¡¨çš„çµ±è¨ˆä¿¡æ¯
        
        Args:
            db_url: Database connection URL
        
        Returns:
            Dictionary with statistics
        """
        conn = None
        try:
            conn = await asyncpg.connect(db_url)
            
            stats = await conn.fetchrow("""
                SELECT
                    COUNT(*) as total_records,
                    COUNT(CASE WHEN outcome IS NOT NULL THEN 1 END) as records_with_outcome,
                    COUNT(CASE WHEN features IS NOT NULL THEN 1 END) as records_with_features,
                    MAX(created_at) as latest_record,
                    MIN(created_at) as oldest_record
                FROM experience_buffer
            """)
            
            return dict(stats) if stats else {}
        
        except Exception as e:
            logger.error(f"âŒ ç²å–çµ±è¨ˆä¿¡æ¯å¤±æ•—: {e}", exc_info=True)
            return {}
        
        finally:
            if conn:
                try:
                    await conn.close()
                except:
                    pass
    
    async def clear(self) -> None:
        """Clear all experiences from buffer"""
        async with self.lock:
            self.experiences.clear()
            logger.info("ðŸ§¹ Experience buffer cleared")


# Global instance
_buffer: Optional[ExperienceBuffer] = None


def get_experience_buffer() -> ExperienceBuffer:
    """Get or create global experience buffer"""
    global _buffer
    if _buffer is None:
        _buffer = ExperienceBuffer()
        logger.info("âœ… Experience buffer initialized")
    return _buffer
