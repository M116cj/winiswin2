"""
æ•¸æ“šæœå‹™ï¼ˆv3.12.0 å¢é‡æ›´æ–°ä¼˜åŒ–ç‰ˆï¼‰
è·è²¬ï¼šå¸‚å ´æ•¸æ“šç²å–ã€æ‰¹é‡è™•ç†ã€å¤šæ™‚é–“æ¡†æ¶æ•¸æ“šå°é½Šã€æ€§èƒ½è¿½è¹¤

v3.12.0 ä¼˜åŒ–3ï¼š
- Kçº¿æ•°æ®å¢é‡æ›´æ–°ï¼ˆåªæ‹‰å–æ–°Kçº¿ï¼Œå‡å°‘60-80% APIè¯·æ±‚ï¼‰
- åŠ¨æ€TTLæ™ºèƒ½ç¼“å­˜ï¼ˆåŸºäºæ³¢åŠ¨ç‡ï¼Œé«˜æ³¢åŠ¨â†’çŸ­TTLï¼‰
- ç½‘ç»œ I/O å»¶è¿Ÿé™ä½ 50%
"""

import asyncio
from typing import List, Dict, Optional
import pandas as pd
from datetime import datetime, timedelta
import logging
import time

from src.clients.binance_client import BinanceClient
from src.core.cache_manager import CacheManager
from src.config import Config

logger = logging.getLogger(__name__)


class DataService:
    """æ•¸æ“šæœå‹™é¡ï¼ˆv3.17.2+ WebSocketæ•´åˆç‰ˆï¼‰"""
    
    def __init__(self, binance_client: BinanceClient, perf_monitor=None, websocket_monitor=None):
        """
        åˆå§‹åŒ–æ•¸æ“šæœå‹™
        
        Args:
            binance_client: Binance å®¢æˆ¶ç«¯
            perf_monitor: æ€§èƒ½ç›£æ§å™¨
            websocket_monitor: WebSocketç›£æ§å™¨ï¼ˆv3.17.2+ï¼‰
        """
        self.client = binance_client
        self.cache = binance_client.cache
        # ä½¿ç”¨ 1h/15m/5m ä¸‰ä¸ªæ—¶é—´æ¡†æ¶
        # 1h: è¶‹åŠ¿ç¡®è®¤ï¼ˆæ¯å°æ—¶ï¼‰
        # 15m: è¶‹åŠ¿ç¡®è®¤ï¼ˆæ¯15åˆ†é’Ÿï¼‰
        # 5m: è¶‹åŠ¿ç¬¦åˆç¡®è®¤ + å…¥åœºä¿¡å·
        self.timeframes = ["1h", "15m", "5m"]
        self.all_symbols: List[str] = []
        
        # âœ¨ v3.12.0æ–°å¢ï¼šå¢é‡æ›´æ–°ç¼“å­˜é”®å‰ç¼€
        self._incremental_cache_prefix = "klines_inc_"
        
        # âœ¨ æ€§èƒ½ç›£æ§
        self.perf_monitor = perf_monitor
        
        # ğŸ”¥ v3.17.2+ï¼šWebSocketæ•´åˆ
        self.websocket_monitor = websocket_monitor
        
        # ğŸ”¥ v3.17.2+ä¿®å¾©ï¼šREST API fallbackçµ±è¨ˆ
        self.ws_stats = {
            'total_requests': 0,
            'ws_hits': 0,
            'rest_fallbacks': 0,
            'last_report_time': time.time()
        }
        
        logger.info("=" * 80)
        logger.info("âœ… DataService v3.17.2+ åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ğŸ“¡ WebSocketæ¨¡å¼: {'å•Ÿç”¨' if websocket_monitor else 'åœç”¨ï¼ˆç´”RESTï¼‰'}")
        logger.info("=" * 80)
    
    def _log_ws_stats_periodically(self):
        """å®šæœŸè¨˜éŒ„WebSocketçµ±è¨ˆï¼ˆæ¯5åˆ†é˜ï¼‰"""
        now = time.time()
        if now - self.ws_stats['last_report_time'] >= 300:  # 5åˆ†é˜
            total = self.ws_stats['total_requests']
            ws_hits = self.ws_stats['ws_hits']
            rest = self.ws_stats['rest_fallbacks']
            
            if total > 0:
                ws_hit_rate = (ws_hits / total) * 100
                logger.info("=" * 80)
                logger.info(f"ğŸ“Š DataService WebSocketçµ±è¨ˆï¼ˆæœ€è¿‘5åˆ†é˜ï¼‰:")
                logger.info(f"   ç¸½è«‹æ±‚: {total}")
                logger.info(f"   WebSocketå‘½ä¸­: {ws_hits} ({ws_hit_rate:.1f}%)")
                logger.info(f"   RESTå‚™æ´: {rest} ({100-ws_hit_rate:.1f}%)")
                logger.info("=" * 80)
            
            # é‡ç½®çµ±è¨ˆï¼ˆæ»¾å‹•çª—å£ï¼‰
            self.ws_stats['last_report_time'] = now
    
    async def initialize(self):
        """åˆå§‹åŒ–æ•¸æ“šæœå‹™"""
        logger.info("åˆå§‹åŒ–æ•¸æ“šæœå‹™...")
        await self.load_all_symbols()
        logger.info(f"âœ… å·²åŠ è¼‰ {len(self.all_symbols)} å€‹ USDT æ°¸çºŒåˆç´„")
    
    async def load_all_symbols(self):
        """åŠ è¼‰æ‰€æœ‰ USDT æ°¸çºŒåˆç´„äº¤æ˜“å°"""
        try:
            exchange_info = await self.client.get_exchange_info()
            
            self.all_symbols = [
                symbol['symbol']
                for symbol in exchange_info.get('symbols', [])
                if symbol['symbol'].endswith('USDT') 
                and symbol['status'] == 'TRADING'
                and symbol.get('contractType') == 'PERPETUAL'
                and symbol['symbol'].isascii()  # æ’é™¤ä¸­æ–‡ç­‰éASCIIäº¤æ˜“å°ï¼ˆé¿å…ç°½åéŒ¯èª¤ï¼‰
            ]
            
            logger.info(f"æˆåŠŸåŠ è¼‰ {len(self.all_symbols)} å€‹äº¤æ˜“å°")
            
        except Exception as e:
            logger.error(f"åŠ è¼‰äº¤æ˜“å°å¤±æ•—: {e}")
            # ğŸ”¥ v3.18+ï¼šä½¿ç”¨ç¡¬ç·¨ç¢¼fallbackåˆ—è¡¨ï¼ˆç¢ºä¿ç³»çµ±åœ¨REST APIå¤±æ•—æ™‚ä»å¯é‹è¡Œï¼‰
            from src.core.websocket.websocket_manager import FALLBACK_SYMBOLS
            self.all_symbols = FALLBACK_SYMBOLS
            logger.warning(f"âš ï¸ ä½¿ç”¨fallbackäº¤æ˜“å°åˆ—è¡¨ï¼ˆ{len(self.all_symbols)}å€‹ä¸»æµäº¤æ˜“å°ï¼‰")
    
    async def get_multi_timeframe_data(
        self,
        symbol: str,
        timeframes: Optional[List[str]] = None
    ) -> Dict[str, pd.DataFrame]:
        """
        è·å–å¤šæ—¶é—´æ¡†æ¶æ•°æ®ï¼ˆv3.17.2+ WebSocketå„ªå…ˆç‰ˆï¼‰
        
        ğŸ”¥ v3.17.2+å„ªåŒ–ï¼š
        1. å„ªå…ˆä½¿ç”¨WebSocketå¯¦æ™‚Kç·šï¼ˆ1mï¼‰
        2. å¾1m Kç·šèšåˆç”Ÿæˆ5m/15m/1hï¼ˆç„¡REST APIè«‹æ±‚ï¼‰
        3. åƒ…åœ¨WebSocketä¸å¯ç”¨æ™‚æ‰ä½¿ç”¨RESTå‚™æ´
        
        Args:
            symbol: äº¤æ˜“å°
            timeframes: æ™‚é–“æ¡†æ¶åˆ—è¡¨ï¼ˆé»˜èªä½¿ç”¨æ‰€æœ‰æ™‚é–“æ¡†æ¶ï¼‰
        
        Returns:
            Dict[str, pd.DataFrame]: æ™‚é–“æ¡†æ¶åˆ°æ•¸æ“šæ¡†çš„æ˜ å°„
        """
        if timeframes is None:
            timeframes = self.timeframes
        
        # ğŸ”¥ v3.17.2+ä¿®å¾©ï¼šçµ±è¨ˆ + æ··åˆä½¿ç”¨WebSocket/RESTï¼ˆé€æ™‚é–“æ¡†æ¶æ±ºç­–ï¼‰
        self.ws_stats['total_requests'] += 1
        
        data = {}
        
        # ğŸ”¥ v3.17.2+ä¿®å¾©ï¼šå„ªå…ˆå˜—è©¦å¾WebSocketç²å–ï¼ˆæ··åˆæ¨¡å¼ï¼‰
        if self.websocket_monitor:
            try:
                ws_data = await self._get_multi_timeframe_from_websocket(symbol, timeframes)
                
                # ä½¿ç”¨å¯ç”¨çš„WebSocketæ•¸æ“š
                for tf in timeframes:
                    if tf in ws_data and not ws_data[tf].empty:
                        data[tf] = ws_data[tf]
                        logger.debug(f"âœ… {symbol} {tf} ä½¿ç”¨WebSocketæ•¸æ“š")
                
            except Exception as e:
                logger.debug(f"ğŸ“¡ {symbol} WebSocketèšåˆç•°å¸¸: {e}")
        
        # ğŸ”¥ v3.17.2+ä¿®å¾©ï¼šåƒ…å°ç¼ºå¤±çš„æ™‚é–“æ¡†æ¶ä½¿ç”¨RESTå‚™æ´
        missing_tfs = [tf for tf in timeframes if tf not in data or data[tf].empty]
        
        if missing_tfs:
            logger.debug(f"ğŸ“¡ {symbol} ä½¿ç”¨REST APIè£œå…… {missing_tfs}")
            tasks = [
                self.get_klines_incremental(symbol, tf, limit=100)
                for tf in missing_tfs
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for tf, result in zip(missing_tfs, results):
                if isinstance(result, Exception):
                    logger.error(f"ç²å– {symbol} {tf} æ•¸æ“šå¤±æ•—: {result}")
                    data[tf] = pd.DataFrame()
                else:
                    data[tf] = result
        
        # ğŸ”¥ v3.17.2+ä¿®å¾©ï¼šçµ±è¨ˆWebSocketå‘½ä¸­ç‡ï¼ˆåŒ…å«éƒ¨åˆ†fallbackï¼‰
        ws_count = len([tf for tf in timeframes if tf in data and tf not in missing_tfs])
        rest_count = len(missing_tfs)
        
        if ws_count == len(timeframes):
            # 100% WebSocketè¦†è“‹
            self.ws_stats['ws_hits'] += 1
        elif rest_count > 0:
            # éƒ¨åˆ†æˆ–å…¨éƒ¨REST fallback
            self.ws_stats['rest_fallbacks'] += 1
        
        # å®šæœŸè¨˜éŒ„çµ±è¨ˆï¼ˆä¸è«–å‘½ä¸­é¡å‹ï¼‰
        self._log_ws_stats_periodically()
        
        return data
    
    async def get_klines(
        self,
        symbol: str,
        interval: str,
        limit: int = 200,
        start_time: Optional[int] = None,
        end_time: Optional[int] = None
    ) -> pd.DataFrame:
        """
        ç²å– Kç·šæ•¸æ“šï¼ˆv3.12.0 å¢é‡æ›´æ–° + åŠ¨æ€TTLä¼˜åŒ–ç‰ˆï¼‰
        
        ä¼˜åŒ–3æ ¸å¿ƒç‰¹æ€§ï¼š
        1. å¢é‡æ›´æ–°ï¼šä»…æ‹‰å–æ–°Kçº¿ï¼Œå‡å°‘60-80% APIè¯·æ±‚
        2. åŠ¨æ€TTLï¼šåŸºäºæ³¢åŠ¨ç‡è®¡ç®—ç¼“å­˜æ—¶é—´ï¼ˆé«˜æ³¢åŠ¨â†’çŸ­TTLï¼‰
        3. æ™ºèƒ½åˆå¹¶ï¼šåˆå¹¶æ—§ç¼“å­˜æ•°æ®ä¸æ–°æ•°æ®
        
        Args:
            symbol: äº¤æ˜“å°
            interval: æ™‚é–“é–“éš”
            limit: æ•¸æ“šæ¢æ•¸
            start_time: é–‹å§‹æ™‚é–“æˆ³ï¼ˆæ‰‹åŠ¨æŒ‡å®šæ—¶è·³è¿‡å¢é‡æ›´æ–°ï¼‰
            end_time: çµæŸæ™‚é–“æˆ³
        
        Returns:
            pd.DataFrame: Kç·šæ•¸æ“šæ¡†
        """
        # âœ¨ æ€§èƒ½è¿½è¹¤
        start_perf = time.time()
        
        # æ­·å²è«‹æ±‚æˆ–æŒ‡å®šæ™‚é–“ç¯„åœæ™‚ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼ï¼ˆä¸å¢é‡æ›´æ–°ï¼‰
        if start_time is not None or end_time is not None:
            return await self._fetch_full_klines(
                symbol, interval, limit, start_time, end_time
            )
        
        # âœ¨ v3.12.0ï¼šå¢é‡æ›´æ–°ç¼“å­˜é”®
        cache_key = f"{self._incremental_cache_prefix}{symbol}_{interval}_{limit}"
        
        # å˜—è©¦å¾ç·©å­˜ç²å–æ—§æ•°æ®
        cached_df = self.cache.get(cache_key)
        
        if cached_df is not None and not cached_df.empty:
            # âœ¨ v3.12.0ï¼šå¢é‡æ‹‰å– - åªè·å–æ–°Kçº¿
            try:
                last_close_time = cached_df.iloc[-1]['close_time']
                
                # æ‹‰å–ä»ä¸Šæ¬¡æœ€åä¸€æ ¹Kçº¿ä¹‹åçš„æ–°æ•°æ®
                new_klines = await self.client.get_klines(
                    symbol=symbol,
                    interval=interval,
                    start_time=int(last_close_time) + 1,  # ä»ä¸Šæ¬¡ç»“æŸåå¼€å§‹
                    limit=limit
                )
                
                if new_klines:
                    # åˆå¹¶æ–°æ—§æ•°æ®
                    new_df = self._parse_klines(new_klines)
                    
                    # åˆå¹¶å¹¶å»é‡ï¼ˆæŒ‰timestampï¼‰
                    df = pd.concat([cached_df, new_df]).drop_duplicates(
                        subset=['timestamp'], keep='last'
                    ).tail(limit)
                    
                    logger.debug(
                        f"âœ… å¢é‡æ›´æ–°: {symbol} {interval} "
                        f"(æ–°å¢ {len(new_klines)} æ ¹Kçº¿)"
                    )
                else:
                    # æ²¡æœ‰æ–°æ•°æ®ï¼Œä½¿ç”¨ç¼“å­˜
                    df = cached_df
                    logger.debug(f"âœ… ä½¿ç”¨ç¼“å­˜ï¼ˆæ— æ–°æ•°æ®ï¼‰: {symbol} {interval}")
                
                # âœ¨ è¨˜éŒ„ç·©å­˜å‘½ä¸­
                if self.perf_monitor:
                    self.perf_monitor.record_cache_hit()
            
            except Exception as e:
                # å¢é‡æ›´æ–°å¤±è´¥ï¼Œé™çº§ä¸ºå®Œæ•´æ‹‰å–
                logger.warning(f"å¢é‡æ›´æ–°å¤±è´¥ {symbol} {interval}: {e}ï¼Œä½¿ç”¨å®Œæ•´æ‹‰å–")
                df = await self._fetch_full_klines(symbol, interval, limit)
        
        else:
            # âœ¨ ç·©å­˜æœªå‘½ä¸­ï¼šå®Œæ•´æ‹‰å–
            logger.debug(f"ğŸ’¾ é¦–æ¬¡æ‹‰å–: {symbol} {interval}")
            df = await self._fetch_full_klines(symbol, interval, limit)
            
            if self.perf_monitor:
                self.perf_monitor.record_cache_miss()
        
        # âœ¨ v3.12.0ï¼šåŠ¨æ€TTLï¼ˆåŸºäºæ³¢åŠ¨ç‡ï¼‰
        if not df.empty:
            dynamic_ttl = self._calculate_dynamic_ttl(df, interval)
            self.cache.set(cache_key, df, ttl=dynamic_ttl)
            logger.debug(f"ğŸ’¾ ç·©å­˜ {symbol} {interval}ï¼Œå‹•æ…‹TTL={dynamic_ttl}ç§’")
        
        # âœ¨ è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
        if self.perf_monitor:
            duration = time.time() - start_perf
            mode = "incremental" if cached_df is not None else "full"
            self.perf_monitor.record_operation(
                f"get_klines_{interval}_{mode}", 
                duration
            )
        
        return df
    
    def _calculate_dynamic_ttl(self, df: pd.DataFrame, interval: str) -> int:
        """
        è®¡ç®—åŠ¨æ€TTLï¼ˆåŸºäºæ³¢åŠ¨ç‡ï¼‰
        
        v3.12.0 ä¼˜åŒ–3ï¼šé«˜æ³¢åŠ¨ â†’ çŸ­TTLï¼Œä½æ³¢åŠ¨ â†’ é•¿TTL
        
        Args:
            df: Kçº¿æ•°æ®
            interval: æ—¶é—´æ¡†æ¶
        
        Returns:
            int: åŠ¨æ€TTLï¼ˆç§’ï¼‰
        """
        try:
            # åŸºç¡€TTL
            ttl_map = {
                '1h': Config.CACHE_TTL_KLINES_1H,
                '15m': Config.CACHE_TTL_KLINES_15M,
                '5m': Config.CACHE_TTL_KLINES_5M
            }
            base_ttl = ttl_map.get(interval, Config.CACHE_TTL_KLINES_DEFAULT)
            
            # è®¡ç®—æ³¢åŠ¨ç‡ï¼ˆ20å‘¨æœŸæ ‡å‡†å·® / å‡å€¼ï¼‰
            if len(df) >= 20:
                volatility = df['high'].rolling(20).std().iloc[-1]
                volatility_normalized = min(volatility, 0.1)  # æˆªæ–­æç«¯å€¼
                
                # åŠ¨æ€è°ƒæ•´ï¼šé«˜æ³¢åŠ¨ â†’ çŸ­TTL
                # volatility=0.1 â†’ multiplier=0ï¼ˆæœ€çŸ­TTL=60ç§’ï¼‰
                # volatility=0 â†’ multiplier=1ï¼ˆæ ‡å‡†TTLï¼‰
                multiplier = max(0, 1 - volatility_normalized * 10)
                dynamic_ttl = max(60, int(base_ttl * multiplier))
                
                return dynamic_ttl
            else:
                return base_ttl
        
        except Exception as e:
            logger.debug(f"åŠ¨æ€TTLè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€TTL: {e}")
            return ttl_map.get(interval, Config.CACHE_TTL_KLINES_DEFAULT)
    
    async def _fetch_full_klines(
        self,
        symbol: str,
        interval: str,
        limit: int,
        start_time: Optional[int] = None,
        end_time: Optional[int] = None
    ) -> pd.DataFrame:
        """
        å®Œæ•´æ‹‰å–Kçº¿æ•°æ®ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰
        
        Args:
            symbol: äº¤æ˜“å°
            interval: æ™‚é–“é–“éš”
            limit: æ•¸æ“šæ¢æ•¸
            start_time: é–‹å§‹æ™‚é–“æˆ³
            end_time: çµæŸæ™‚é–“æˆ³
        
        Returns:
            pd.DataFrame: Kç·šæ•¸æ“šæ¡†
        """
        try:
            klines = await self.client.get_klines(
                symbol=symbol,
                interval=interval,
                limit=limit,
                start_time=start_time,
                end_time=end_time
            )
            
            if not klines:
                return pd.DataFrame()
            
            return self._parse_klines(klines)
            
        except Exception as e:
            logger.error(f"ç²å– Kç·šæ•¸æ“šå¤±æ•— {symbol} {interval}: {e}")
            return pd.DataFrame()
    
    def _parse_klines(self, klines: List) -> pd.DataFrame:
        """
        è§£æKçº¿æ•°æ®ä¸ºDataFrame
        
        Args:
            klines: åŸå§‹Kçº¿æ•°æ®
        
        Returns:
            pd.DataFrame: è§£æåçš„æ•°æ®æ¡†
        """
        if not klines:
            return pd.DataFrame()
        
        # æ„é€ DataFrame
        df = pd.DataFrame(
            data=klines,
            columns=[
                'timestamp', 'open', 'high', 'low', 'close', 'volume',
                'close_time', 'quote_volume', 'trades', 'taker_buy_volume',
                'taker_buy_quote_volume', 'ignore'
            ]
        )
        
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        
        for col in ['open', 'high', 'low', 'close', 'volume']:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # ä¿ç•™ close_timeï¼ˆç”¨äºå¢é‡æ›´æ–°ï¼‰
        result = df[['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time']].copy()
        
        return result
    
    async def get_klines_incremental(self, symbol: str, interval: str, limit: int = 100) -> pd.DataFrame:
        """
        å¢é‡è·å–Kçº¿æ•°æ®ï¼ˆv3.13.0 æ–‡æ¡£æ­¥éª¤1å®Œæ•´å®ç°ï¼‰
        
        ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼š
        - é¦–æ¬¡è·å–å®Œæ•´æ•°æ®
        - åç»­åªæ‹‰å–æ–°Kçº¿ï¼ˆåŸºäºlast_close_timeï¼‰
        - åŠ¨æ€TTLç¼“å­˜ï¼ˆåŸºäºæ³¢åŠ¨ç‡ï¼Œé«˜æ³¢åŠ¨â†’çŸ­TTLï¼‰
        - APIè¯·æ±‚å‡å°‘60-80%ï¼Œç½‘ç»œI/Oå»¶è¿Ÿé™ä½50%
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            interval: æ—¶é—´é—´éš”ï¼ˆ1h, 15m, 5mç­‰ï¼‰
            limit: æ•°æ®æ¡æ•°é™åˆ¶
        
        Returns:
            pd.DataFrame: Kçº¿æ•°æ®
        """
        cache_key = f"{symbol}_{interval}"
        current_time = time.time()
        
        # æ­¥éª¤1ï¼šæ£€æŸ¥ç¼“å­˜
        cached = self.cache.get(cache_key)
        
        if cached is None:
            # é¦–æ¬¡è·å–å®Œæ•´æ•°æ®
            logger.debug(f"ğŸ’¾ é¦–æ¬¡è·å–å®Œæ•´æ•°æ®: {symbol} {interval}")
            df = await self._fetch_full_klines(symbol, interval, limit)
            
            if df.empty:
                return df
            
            # æ„å»ºç¼“å­˜å…ƒæ•°æ®
            self.cache.set(cache_key, {
                'data': df,
                'timestamp': current_time,
                'last_close_time': df.iloc[-1]['close_time'] if not df.empty else 0
            }, ttl=300)
            
            return df
        
        # æ­¥éª¤2ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆåŸºäºåŠ¨æ€TTLï¼‰
        cached_data = cached.get('data')
        cached_timestamp = cached.get('timestamp', 0)
        
        if cached_data is None or cached_data.empty:
            # ç¼“å­˜æ•°æ®æ— æ•ˆï¼Œé‡æ–°è·å–
            logger.warning(f"âš ï¸ ç¼“å­˜æ•°æ®æ— æ•ˆ: {symbol} {interval}ï¼Œé‡æ–°è·å–")
            df = await self._fetch_full_klines(symbol, interval, limit)
            
            if not df.empty:
                self.cache.set(cache_key, {
                    'data': df,
                    'timestamp': current_time,
                    'last_close_time': df.iloc[-1]['close_time']
                }, ttl=300)
            
            return df
        
        # è®¡ç®—åŠ¨æ€TTL
        volatility = self._calculate_volatility(cached_data)
        dynamic_ttl = max(60, 300 * (1 - min(volatility, 0.1)))
        
        if current_time - cached_timestamp < dynamic_ttl:
            # TTLæœªè¿‡æœŸï¼Œç›´æ¥è¿”å›ç¼“å­˜
            logger.debug(f"âœ… ä½¿ç”¨ç¼“å­˜æ•°æ®: {symbol} {interval} (TTL={dynamic_ttl:.0f}s)")
            return cached_data
        
        # æ­¥éª¤3ï¼šå¢é‡æ›´æ–° - åªè·å–æ–°Kçº¿
        last_close_time = cached.get('last_close_time', 0)
        
        try:
            new_klines = await self._fetch_klines_since(symbol, interval, last_close_time)
            
            if new_klines.empty:
                # æ²¡æœ‰æ–°æ•°æ®ï¼Œæ›´æ–°æ—¶é—´æˆ³
                cached['timestamp'] = current_time
                self.cache.set(cache_key, cached, ttl=dynamic_ttl)
                logger.debug(f"âœ… æ— æ–°æ•°æ®ï¼Œæ›´æ–°æ—¶é—´æˆ³: {symbol} {interval}")
                return cached_data
            
            # æ­¥éª¤4ï¼šåˆå¹¶æ•°æ®
            updated_df = pd.concat([cached_data, new_klines]).drop_duplicates(
                subset=['timestamp'], keep='last'
            ).tail(limit)
            
            # æ›´æ–°ç¼“å­˜
            self.cache.set(cache_key, {
                'data': updated_df,
                'timestamp': current_time,
                'last_close_time': updated_df.iloc[-1]['close_time']
            }, ttl=dynamic_ttl)
            
            logger.debug(
                f"âœ… å¢é‡æ›´æ–°æˆåŠŸ: {symbol} {interval} "
                f"(æ–°å¢ {len(new_klines)} æ ¹Kçº¿, TTL={dynamic_ttl:.0f}s)"
            )
            
            return updated_df
            
        except Exception as e:
            logger.error(f"å¢é‡æ›´æ–°å¤±è´¥ {symbol} {interval}: {e}ï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®")
            return cached_data
    
    async def _fetch_klines_since(self, symbol: str, interval: str, since_time: float) -> pd.DataFrame:
        """
        è·å–æŒ‡å®šæ—¶é—´åçš„Kçº¿ï¼ˆv3.13.0 æ–‡æ¡£æ­¥éª¤2è¦æ±‚ï¼‰
        
        Args:
            symbol: äº¤æ˜“å¯¹
            interval: æ—¶é—´é—´éš”
            since_time: èµ·å§‹æ—¶é—´ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰
        
        Returns:
            pd.DataFrame: æ–°çš„Kçº¿æ•°æ®
        """
        try:
            # Binance APIæ”¯æŒstartTimeå‚æ•°
            klines = await self.client.get_klines(
                symbol=symbol,
                interval=interval,
                start_time=int(since_time) + 1,  # ä»ä¸Šæ¬¡ç»“æŸåå¼€å§‹
                limit=1000  # æœ€å¤§é™åˆ¶
            )
            
            if not klines:
                return pd.DataFrame()
            
            return self._parse_klines(klines)
            
        except Exception as e:
            logger.error(f"è·å–å¢é‡Kçº¿å¤±è´¥ {symbol} {interval}: {e}")
            return pd.DataFrame()
    
    def _calculate_volatility(self, df: pd.DataFrame) -> float:
        """
        è®¡ç®—æ³¢åŠ¨ç‡ï¼ˆv3.13.0 æ–‡æ¡£æ­¥éª¤1è¦æ±‚ï¼‰
        
        Args:
            df: Kçº¿æ•°æ®
        
        Returns:
            float: æ³¢åŠ¨ç‡ï¼ˆå½’ä¸€åŒ–å€¼ï¼‰
        """
        try:
            if len(df) < 20:
                return 0.0
            
            # è®¡ç®—20å‘¨æœŸæ»šåŠ¨æ ‡å‡†å·®
            rolling_std = df['high'].rolling(20).std().iloc[-1]
            close_price = df['close'].iloc[-1]
            
            if close_price == 0:
                return 0.0
            
            volatility = rolling_std / close_price
            return volatility
            
        except Exception as e:
            logger.debug(f"æ³¢åŠ¨ç‡è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    async def get_batch_tickers(self, symbols: List[str]) -> Dict[str, dict]:
        """
        æ‰¹é‡ç²å–è¡Œæƒ…æ•¸æ“š
        
        Args:
            symbols: äº¤æ˜“å°åˆ—è¡¨
        
        Returns:
            Dict[str, dict]: äº¤æ˜“å°åˆ°è¡Œæƒ…æ•¸æ“šçš„æ˜ å°„
        """
        cache_key = "batch_tickers"
        cached_data = self.cache.get(cache_key)
        
        if cached_data is not None:
            return {s: cached_data.get(s, {}) for s in symbols}
        
        try:
            all_tickers = await self.client.get_ticker_price()
            
            ticker_dict = {
                ticker['symbol']: ticker
                for ticker in all_tickers
                if ticker['symbol'] in symbols
            }
            
            self.cache.set(cache_key, ticker_dict, ttl=Config.CACHE_TTL_TICKER)
            
            return ticker_dict
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ç²å–è¡Œæƒ…æ•¸æ“šå¤±æ•—: {e}")
            return {}
    
    async def get_market_data_batch(
        self,
        symbols: List[str],
        timeframe: str = "15m",
        limit: int = 200
    ) -> Dict[str, pd.DataFrame]:
        """
        æ‰¹é‡ç²å–å¸‚å ´æ•¸æ“š
        
        Args:
            symbols: äº¤æ˜“å°åˆ—è¡¨
            timeframe: æ™‚é–“æ¡†æ¶
            limit: æ•¸æ“šæ¢æ•¸
        
        Returns:
            Dict[str, pd.DataFrame]: äº¤æ˜“å°åˆ°æ•¸æ“šæ¡†çš„æ˜ å°„
        """
        batch_size = Config.BATCH_SIZE
        results = {}
        
        for i in range(0, len(symbols), batch_size):
            batch = symbols[i:i + batch_size]
            
            tasks = [
                self.get_klines(symbol, timeframe, limit)
                for symbol in batch
            ]
            
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for symbol, data in zip(batch, batch_results):
                if isinstance(data, Exception):
                    logger.error(f"ç²å– {symbol} æ•¸æ“šå¤±æ•—: {data}")
                    results[symbol] = pd.DataFrame()
                else:
                    results[symbol] = data
            
            if i + batch_size < len(symbols):
                await asyncio.sleep(0.1)
        
        return results
    
    async def scan_market(self, top_n: int = 200) -> List[Dict]:
        """
        æƒæå¸‚å ´ï¼ŒæŒ‰æµå‹•æ€§æ’åºï¼Œè¿”å›å‰Nå€‹äº¤æ˜“å°ï¼ˆv3.17.2+ é™ä½RESTèª¿ç”¨ç‰ˆï¼‰
        
        ğŸ”¥ v3.17.2+å„ªåŒ–ï¼š
        - åƒ…åœ¨å•Ÿå‹•æ™‚èª¿ç”¨get_24h_tickersï¼ˆREST APIï¼‰
        - çµæœç·©å­˜1å°æ™‚ï¼ˆæ¸›å°‘99%çš„RESTè«‹æ±‚ï¼‰
        - æµå‹•æ€§æ’åºçµæœç©©å®šï¼Œç„¡éœ€é »ç¹æ›´æ–°
        
        ç­–ç•¥ï¼šå„ªå…ˆç›£æ§æµå‹•æ€§æœ€é«˜çš„å‰200å€‹æ¨™çš„
        - æµå‹•æ€§æŒ‡æ¨™ï¼š24h äº¤æ˜“é¡ï¼ˆquoteVolumeï¼Œä»¥ USDT è¨ˆï¼‰
        - å¾ 600+ å€‹ Uæœ¬ä½åˆç´„ä¸­é¸æ“‡æœ€æ´»èºçš„
        
        Args:
            top_n: è¿”å›å‰Nå€‹äº¤æ˜“å°ï¼ˆé»˜èª200ï¼‰
        
        Returns:
            List[Dict]: æŒ‰æµå‹•æ€§æ’åºçš„äº¤æ˜“å°åˆ—è¡¨
        """
        # ğŸ”¥ v3.17.2+ï¼šæª¢æŸ¥é•·æ™‚é–“ç·©å­˜ï¼ˆ1å°æ™‚ï¼‰
        cache_key = f"scan_market_{top_n}"
        cached = self.cache.get(cache_key)
        if cached:
            logger.debug(f"ğŸ“¦ ä½¿ç”¨ç·©å­˜çš„å¸‚å ´æƒæçµæœï¼ˆ{len(cached)}å€‹äº¤æ˜“å°ï¼‰")
            return cached
        
        # âœ¨ v3.3.7ï¼šæ€§èƒ½è¿½è¹¤
        start_time = time.time()
        
        logger.info(f"ğŸ” é–‹å§‹æƒæ {len(self.all_symbols)} å€‹äº¤æ˜“å°ï¼ˆæµå‹•æ€§æ’åºï¼‰...")
        
        # ç²å–24h tickeræ•¸æ“šï¼ˆåŒ…å«äº¤æ˜“é‡æ•¸æ“šï¼‰
        try:
            exchange_info_data = await self.client.get_24h_tickers()
            
            market_data = []
            for ticker in exchange_info_data:
                symbol = ticker.get('symbol')
                if symbol not in self.all_symbols:
                    continue
                
                # æµå‹•æ€§æŒ‡æ¨™ï¼š24h äº¤æ˜“é¡ï¼ˆUSDTï¼‰
                quote_volume = float(ticker.get('quoteVolume', 0))
                volume_24h = float(ticker.get('volume', 0))
                price_change_pct = float(ticker.get('priceChangePercent', 0))
                
                market_data.append({
                    'symbol': symbol,
                    'price': float(ticker.get('lastPrice', 0)),
                    'change_24h': price_change_pct,
                    'volume_24h': volume_24h,
                    'quote_volume': quote_volume,  # æµå‹•æ€§æŒ‡æ¨™ï¼ˆUSDTï¼‰
                    'liquidity': quote_volume,     # ç”¨æ–¼æ’åº
                    'timestamp': datetime.now()
                })
            
            # æŒ‰æµå‹•æ€§æ’åºï¼ˆå¾é«˜åˆ°ä½ï¼‰
            market_data.sort(key=lambda x: x['liquidity'], reverse=True)
            
            # å–å‰Nå€‹
            top_liquidity = market_data[:top_n]
            
            avg_volume = sum(x['liquidity'] for x in top_liquidity) / len(top_liquidity) if top_liquidity else 0
            
            # âœ¨ v3.3.7ï¼šæ€§èƒ½æ—¥èªŒ
            duration = time.time() - start_time
            logger.info(
                f"âœ… å¸‚å ´æƒæå®Œæˆ: å¾ {len(market_data)} å€‹äº¤æ˜“å°ä¸­é¸æ“‡ "
                f"æµå‹•æ€§æœ€é«˜çš„å‰ {len(top_liquidity)} å€‹ "
                f"(å¹³å‡24häº¤æ˜“é¡: ${avg_volume:,.0f} USDT) "
                f"âš¡ è€—æ™‚: {duration:.2f}s"
            )
            
            # âœ¨ v3.3.7ï¼šè¨˜éŒ„æ€§èƒ½
            if self.perf_monitor:
                self.perf_monitor.record_operation("scan_market", duration)
            
            # ğŸ”¥ v3.17.2+ï¼šç·©å­˜1å°æ™‚ï¼ˆæ¸›å°‘REST APIèª¿ç”¨ï¼‰
            self.cache.set(cache_key, top_liquidity, ttl=3600)
            logger.info(f"ğŸ“¦ å¸‚å ´æƒæçµæœå·²ç·©å­˜1å°æ™‚ï¼ˆä¸‹æ¬¡èª¿ç”¨å°‡è·³éREST APIï¼‰")
            
            return top_liquidity
            
        except Exception as e:
            logger.error(f"å¸‚å ´æƒæå¤±æ•—: {e}", exc_info=True)
            # é™ç´šï¼šè¿”å›æ‰€æœ‰äº¤æ˜“å°
            return await self._fallback_scan_market()
    
    async def _fallback_scan_market(self) -> List[Dict]:
        """
        é™ç´šæƒæï¼šç•¶24h tickerå¤±æ•—æ™‚ä½¿ç”¨
        
        ğŸ”¥ v3.18+ä¿®å¾©ï¼šä¸ä¾è³´REST APIï¼Œç›´æ¥è¿”å›all_symbolsåˆ—è¡¨
        """
        logger.warning(f"âš ï¸ REST APIä¸å¯ç”¨ï¼Œä½¿ç”¨æœ¬åœ°äº¤æ˜“å°åˆ—è¡¨ï¼ˆ{len(self.all_symbols)}å€‹ï¼‰")
        
        market_data = []
        for symbol in self.all_symbols:
            market_data.append({
                'symbol': symbol,
                'price': 0.0,  # WebSocketç¨å¾Œæœƒæ›´æ–°
                'liquidity': 0.0,
                'quote_volume': 0.0,
                'timestamp': datetime.now()
            })
        
        logger.info(f"âœ… é™ç´šæ¨¡å¼ï¼šè¿”å› {len(market_data)} å€‹äº¤æ˜“å°ï¼ˆç­‰å¾…WebSocketæ›´æ–°åƒ¹æ ¼ï¼‰")
        return market_data
    
    async def get_account_info(self) -> dict:
        """
        ç²å–è³¬æˆ¶ä¿¡æ¯
        
        Returns:
            dict: è³¬æˆ¶ä¿¡æ¯
        """
        cache_key = "account_info"
        cached_data = self.cache.get(cache_key)
        
        if cached_data is not None:
            return cached_data
        
        try:
            account = await self.client.get_account_info()
            self.cache.set(cache_key, account, ttl=Config.CACHE_TTL_ACCOUNT)
            return account
        except Exception as e:
            logger.error(f"ç²å–è³¬æˆ¶ä¿¡æ¯å¤±æ•—: {e}")
            return {}
    
    async def get_positions(self) -> List[dict]:
        """
        ç²å–ç•¶å‰æŒå€‰
        
        Returns:
            List[dict]: æŒå€‰åˆ—è¡¨
        """
        try:
            account = await self.get_account_info()
            positions = account.get('positions', [])
            
            active_positions = [
                pos for pos in positions
                if float(pos.get('positionAmt', 0)) != 0
            ]
            
            return active_positions
        except Exception as e:
            logger.error(f"ç²å–æŒå€‰å¤±æ•—: {e}")
            return []
    
    async def _get_multi_timeframe_from_websocket(
        self,
        symbol: str,
        timeframes: List[str]
    ) -> Dict[str, pd.DataFrame]:
        """
        å¾WebSocketèšåˆç²å–å¤šæ™‚é–“æ¡†æ¶æ•¸æ“šï¼ˆv3.17.2+ï¼‰
        
        Args:
            symbol: äº¤æ˜“å°
            timeframes: æ™‚é–“æ¡†æ¶åˆ—è¡¨
        
        Returns:
            Dict[str, pd.DataFrame]: æ™‚é–“æ¡†æ¶åˆ°æ•¸æ“šæ¡†çš„æ˜ å°„
        """
        if not self.websocket_monitor:
            return {}
        
        # å¾WebSocketç²å–æ‰€æœ‰1m Kç·šæ­·å²
        all_klines = self.websocket_monitor.get_all_klines()
        # ğŸ”¥ v3.18.5+ Critical Fix: WebSocketç·©å­˜ä½¿ç”¨å°å¯«symbolï¼Œå¿…é ˆè½‰æ›
        klines_1m = all_klines.get(symbol.lower(), [])
        
        # ğŸ”¥ v3.17.2+ä¿®å¾©ï¼šéƒ¨åˆ†å¯ç”¨ç­–ç•¥ï¼ˆè¿”å›æœ‰è¶³å¤ æ•¸æ“šçš„æ™‚é–“æ¡†æ¶ï¼‰
        # 5méœ€è¦5æ ¹ã€15méœ€è¦15æ ¹ã€1héœ€è¦60æ ¹
        kline_count = len(klines_1m) if klines_1m else 0
        
        if kline_count < 5:
            # é€£5méƒ½ç„¡æ³•èšåˆï¼Œå®Œå…¨æ²’æœ‰WebSocketæ•¸æ“š
            logger.debug(f"{symbol}: WebSocket 1m Kç·šå¤ªå°‘ï¼ˆ{kline_count}<5ï¼‰ï¼Œç„¡æ³•ä½¿ç”¨")
            return {}
        
        result = {}
        
        # ğŸ”¥ v3.17.2+ä¿®å¾©ï¼šé€æ™‚é–“æ¡†æ¶æª¢æŸ¥ï¼Œè¿”å›å¯ç”¨çš„éƒ¨åˆ†
        for tf in timeframes:
            if tf == "1m" and kline_count >= 1:
                # 1mç›´æ¥ä½¿ç”¨
                result[tf] = self._convert_kline_to_df(klines_1m[-100:])
            elif tf in ["5m", "15m", "1h"]:
                # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ æ•¸æ“šèšåˆ
                aggregated = self._aggregate_klines(klines_1m, tf)
                if aggregated:
                    result[tf] = self._convert_kline_to_df(aggregated[-100:])
                    logger.debug(f"{symbol} {tf}: WebSocketèšåˆæˆåŠŸï¼ˆ{kline_count}æ ¹1m Kç·šï¼‰")
                else:
                    # æ•¸æ“šä¸è¶³ï¼Œè¿”å›ç©ºï¼ˆæœƒç”±get_multi_timeframe_dataè£œRESTï¼‰
                    logger.debug(f"{symbol} {tf}: WebSocketèšåˆä¸è¶³ï¼ˆ{kline_count}æ ¹1m Kç·šï¼‰")
                    result[tf] = pd.DataFrame()
            else:
                # ä¸æ”¯æ´çš„æ™‚é–“æ¡†æ¶
                result[tf] = pd.DataFrame()
        
        # è¿”å›éƒ¨åˆ†å¯ç”¨çš„æ•¸æ“šï¼ˆå³ä½¿æŸäº›æ™‚é–“æ¡†æ¶ç‚ºç©ºï¼‰
        return result
    
    def _aggregate_klines(self, klines_1m: List[Dict], target_interval: str) -> List[Dict]:
        """
        å¾1m Kç·šèšåˆç”Ÿæˆé«˜æ™‚é–“æ¡†æ¶Kç·šï¼ˆv3.17.2+ä¿®å¾©ç‰ˆï¼‰
        
        ğŸ”¥ v3.17.2+ä¿®å¾©ï¼šä½¿ç”¨æ™‚é–“å°é½Šçš„sliding windowèšåˆ
        - ä¿®å¾©å‰ï¼šå›ºå®šåˆ†çµ„ï¼ˆ0-5, 5-10...ï¼‰ï¼Œå°è‡´æ•¸æ“šä¸é€£çºŒ
        - ä¿®å¾©å¾Œï¼šåŸºæ–¼æ™‚é–“å°é½Šï¼ˆ00:00-00:05, 00:05-00:10...ï¼‰ï¼Œç¬¦åˆKç·šè¦ç¯„
        
        Args:
            klines_1m: 1m Kç·šåˆ—è¡¨ï¼ˆå¾WebSocketç²å–ï¼‰
            target_interval: ç›®æ¨™æ™‚é–“æ¡†æ¶ï¼ˆ5m/15m/1hï¼‰
        
        Returns:
            List[Dict]: èšåˆå¾Œçš„Kç·šåˆ—è¡¨
        """
        # æ™‚é–“æ¡†æ¶æ˜ å°„ï¼ˆåˆ†é˜ â†’ æ¯«ç§’ï¼‰
        interval_map = {
            "5m": 5 * 60 * 1000,
            "15m": 15 * 60 * 1000,
            "1h": 60 * 60 * 1000
        }
        
        interval_ms = interval_map.get(target_interval)
        if not interval_ms:
            logger.warning(f"ä¸æ”¯æ´çš„èšåˆæ™‚é–“æ¡†æ¶: {target_interval}")
            return []
        
        minutes = interval_ms // (60 * 1000)
        
        if len(klines_1m) < minutes:
            logger.debug(f"1m Kç·šæ•¸é‡ä¸è¶³ï¼ˆ{len(klines_1m)} < {minutes}ï¼‰ï¼Œç„¡æ³•èšåˆ")
            return []
        
        # ğŸ”¥ v3.17.2+ä¿®å¾©ï¼šä½¿ç”¨æ™‚é–“å°é½Šçš„èšåˆæ–¹å¼
        # æ­¥é©Ÿ1ï¼šæŒ‰æ™‚é–“æˆ³åˆ†çµ„
        from collections import defaultdict
        grouped = defaultdict(list)
        
        for kline in klines_1m:
            # è¨ˆç®—è©²1m Kç·šå±¬æ–¼å“ªå€‹æ™‚é–“æ¡†æ¶Kç·š
            # ä¾‹å¦‚ï¼štimestamp=1699999999999ï¼ˆ2023-11-15 03:33:19ï¼‰
            # 5m Kç·šï¼šæ‡‰è©²å°é½Šåˆ° 1699999800000ï¼ˆ2023-11-15 03:30:00ï¼‰
            timestamp = kline['timestamp']
            aligned_time = (timestamp // interval_ms) * interval_ms
            grouped[aligned_time].append(kline)
        
        # æ­¥é©Ÿ2ï¼šèšåˆæ¯å€‹æ™‚é–“çµ„
        aggregated = []
        
        for aligned_time in sorted(grouped.keys()):
            chunk = grouped[aligned_time]
            
            # èšåˆOHLCV
            aggregated_kline = {
                'timestamp': aligned_time,  # ä½¿ç”¨å°é½Šå¾Œçš„æ™‚é–“æˆ³
                'open': chunk[0]['open'],  # é–‹ç›¤åƒ¹
                'high': max(k['high'] for k in chunk),  # æœ€é«˜åƒ¹
                'low': min(k['low'] for k in chunk),  # æœ€ä½åƒ¹
                'close': chunk[-1]['close'],  # æ”¶ç›¤åƒ¹
                'volume': sum(k['volume'] for k in chunk),  # æˆäº¤é‡
                'quote_volume': sum(k.get('quote_volume', 0) for k in chunk),  # USDTæˆäº¤é‡
                'trades': sum(k.get('trades', 0) for k in chunk),  # äº¤æ˜“ç­†æ•¸
                'close_time': chunk[-1].get('close_time', aligned_time + interval_ms - 1)  # æ”¶ç›¤æ™‚é–“
            }
            
            aggregated.append(aggregated_kline)
        
        logger.debug(f"âœ… æ™‚é–“å°é½Šèšåˆ: {len(klines_1m)}æ ¹1m â†’ {len(aggregated)}æ ¹{target_interval}")
        return aggregated
    
    def _convert_kline_to_df(self, klines: List[Dict]) -> pd.DataFrame:
        """
        å°‡Kç·šåˆ—è¡¨è½‰æ›ç‚ºDataFrame
        
        Args:
            klines: Kç·šåˆ—è¡¨
        
        Returns:
            pd.DataFrame: Kç·šæ•¸æ“šæ¡†
        """
        if not klines:
            return pd.DataFrame()
        
        df = pd.DataFrame(klines)
        
        # ç¢ºä¿å¿…è¦çš„æ¬„ä½å­˜åœ¨
        required_columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
        for col in required_columns:
            if col not in df.columns:
                df[col] = 0.0
        
        return df
    
    def align_timeframes(
        self,
        data: Dict[str, pd.DataFrame]
    ) -> Dict[str, pd.DataFrame]:
        """
        å°é½Šå¤šæ™‚é–“æ¡†æ¶æ•¸æ“š
        
        Args:
            data: æ™‚é–“æ¡†æ¶åˆ°æ•¸æ“šæ¡†çš„æ˜ å°„
        
        Returns:
            Dict[str, pd.DataFrame]: å°é½Šå¾Œçš„æ•¸æ“š
        """
        if not data:
            return data
        
        aligned_data = {}
        
        for tf, df in data.items():
            if df.empty:
                aligned_data[tf] = df
                continue
            
            df = df.copy()
            df.set_index('timestamp', inplace=True)
            df.sort_index(inplace=True)
            
            aligned_data[tf] = df
        
        return aligned_data
