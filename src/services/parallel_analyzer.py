"""
ä¸¦è¡Œåˆ†æå™¨ï¼ˆv3.15.1 ä¸²è¡Œç¨³å®šç‰ˆï¼‰
è·è²¬ï¼šæ‰¹é‡è™•ç†å¤§é‡äº¤æ˜“å°åˆ†æã€è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°ã€æ€§èƒ½è¿½è¹¤

v3.15.1 ä¿®å¤ï¼š
- ç¦ç”¨è¿›ç¨‹æ± ï¼ˆé¿å… BrokenProcessPool é”™è¯¯ï¼‰
- æ”¹ç”¨ä¸²è¡Œå¤„ç†ï¼ˆ100% ç¨³å®šï¼‰
- ä¼˜å…ˆä¿è¯ç³»ç»Ÿç¨³å®šæ€§
"""

import asyncio
from typing import List, Dict, Optional
import logging
import time

from src.strategies.ict_strategy import ICTStrategy
from src.config import Config

logger = logging.getLogger(__name__)


class ParallelAnalyzer:
    """ä¸¦è¡Œåˆ†æå™¨ - å……åˆ†åˆ©ç”¨ 32vCPU è³‡æºï¼ˆv3.12.0 å…¨å±€è¿›ç¨‹æ± ä¼˜åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, max_workers: Optional[int] = None, perf_monitor=None):
        """
        åˆå§‹åŒ–ä¸¦è¡Œåˆ†æå™¨
        
        Args:
            max_workers: æœ€å¤§å·¥ä½œé€²ç¨‹æ•¸ï¼ˆNone è¡¨ç¤ºå¾é…ç½®è®€å–ï¼‰
            perf_monitor: æ€§èƒ½ç›£æ§å™¨
        """
        self.config = Config
        self.max_workers = max_workers
        
        # ğŸ”§ v3.15.1ï¼šç¦ç”¨è¿›ç¨‹æ± ï¼Œæ”¹ç”¨ä¸²è¡Œå¤„ç†ï¼ˆé¿å… BrokenProcessPool é”™è¯¯ï¼‰
        # è¿›ç¨‹æ± åœ¨æŸäº›ç¯å¢ƒä¸‹ä¸ç¨³å®šï¼Œå¯¼è‡´å­è¿›ç¨‹å´©æºƒ
        # ä¸²è¡Œå¤„ç†è™½ç„¶ç¨æ…¢ï¼Œä½† 100% ç¨³å®š
        self.global_pool = None  # ç¦ç”¨è¿›ç¨‹æ± 
        
        # ç­–ç•¥å®ä¾‹ï¼ˆç”¨äºä¸²è¡Œåˆ†æï¼‰
        self.strategy = ICTStrategy()
        
        # âœ¨ æ€§èƒ½ç›£æ§
        self.perf_monitor = perf_monitor
        
        logger.info("ä¸¦è¡Œåˆ†æå™¨åˆå§‹åŒ–: ä½¿ç”¨ä¸²è¡Œå¤„ç†æ¨¡å¼ï¼ˆç¨³å®šä¼˜å…ˆï¼‰")
        logger.info("ğŸ”§ v3.15.1: å·²ç¦ç”¨è¿›ç¨‹æ± ï¼Œé¿å… BrokenProcessPool é”™è¯¯")
    
    def _calculate_optimal_batch_size(self, total_symbols: int) -> int:
        """
        è¨ˆç®—æœ€å„ªæ‰¹æ¬¡å¤§å°ï¼ˆè‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°ï¼‰
        
        Args:
            total_symbols: ç¸½äº¤æ˜“å°æ•¸é‡
        
        Returns:
            int: æœ€å„ªæ‰¹æ¬¡å¤§å°
        """
        # ğŸ”§ v3.15.1: ä¸²è¡Œå¤„ç†æ¨¡å¼ï¼Œè¿”å›å›ºå®šæ‰¹æ¬¡å¤§å°
        # ä½¿ç”¨è¾ƒå¤§æ‰¹æ¬¡ä»¥å‡å°‘æ—¥å¿—è¾“å‡º
        return min(total_symbols, 100)
    
    async def analyze_batch(
        self,
        symbols_data: List[Dict],
        data_manager
    ) -> List[Dict]:
        """
        æ‰¹é‡ä¸¦è¡Œåˆ†æå¤šå€‹äº¤æ˜“å°ï¼ˆv3.12.0 å…¨å±€è¿›ç¨‹æ± ä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            symbols_data: äº¤æ˜“å°åˆ—è¡¨
            data_manager: æ•¸æ“šç®¡ç†å™¨å¯¦ä¾‹ï¼ˆDataService æˆ– SmartDataManagerï¼‰
        
        Returns:
            List[Dict]: ç”Ÿæˆçš„äº¤æ˜“ä¿¡è™Ÿåˆ—è¡¨
        """
        try:
            # âœ¨ æ€§èƒ½è¿½è¹¤
            start_time = time.time()
            
            total_symbols = len(symbols_data)
            logger.info(f"é–‹å§‹æ‰¹é‡åˆ†æ {total_symbols} å€‹äº¤æ˜“å°")
            
            # âœ¨ è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°
            batch_size = self._calculate_optimal_batch_size(total_symbols)
            
            signals = []
            total_batches = (total_symbols + batch_size - 1) // batch_size
            
            logger.info(
                f"âš¡ æ‰¹æ¬¡é…ç½®: {batch_size} å€‹/æ‰¹æ¬¡, å…± {total_batches} æ‰¹æ¬¡ "
                f"(å·¥ä½œé€²ç¨‹: {self.global_pool.max_workers})"
            )
            
            for batch_idx in range(total_batches):
                i = batch_idx * batch_size
                batch = symbols_data[i:i + batch_size]
                
                logger.info(f"è™•ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({len(batch)} å€‹äº¤æ˜“å°)")
                
                # ä¸¦è¡Œç²å–å¤šæ™‚é–“æ¡†æ¶æ•¸æ“š
                tasks = [
                    data_manager.get_multi_timeframe_data(item['symbol'])
                    for item in batch
                ]
                
                multi_tf_data_list = await asyncio.gather(*tasks, return_exceptions=True)
                
                # ğŸ”§ v3.15.1: ä¸²è¡Œå¤„ç†ï¼ˆä¸ä½¿ç”¨è¿›ç¨‹æ± ï¼Œ100% ç¨³å®šï¼‰
                batch_signal_count = 0
                
                for j, multi_tf_data in enumerate(multi_tf_data_list):
                    # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
                    if isinstance(multi_tf_data, Exception):
                        logger.debug(f"è·³é {batch[j]['symbol']}: æ•¸æ“šç²å–ç•°å¸¸ - {multi_tf_data}")
                        continue
                    
                    if multi_tf_data is None or not isinstance(multi_tf_data, dict):
                        logger.debug(f"è·³é {batch[j]['symbol']}: æ•¸æ“šç„¡æ•ˆ")
                        continue
                    
                    symbol = batch[j]['symbol']
                    
                    # ä¸²è¡Œåˆ†æï¼ˆä¸»è¿›ç¨‹ä¸­æ‰§è¡Œï¼Œé¿å…è¿›ç¨‹æ± é—®é¢˜ï¼‰
                    try:
                        signal = self.strategy.analyze(symbol, multi_tf_data)
                        if signal:
                            signals.append(signal)
                            batch_signal_count += 1
                    except Exception as e:
                        logger.debug(f"åˆ†æ {symbol} å¤±è´¥: {e}")
                
                batch_time = time.time() - start_time
                logger.info(
                    f"æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} å®Œæˆ: "
                    f"ç”Ÿæˆ {batch_signal_count} å€‹ä¿¡è™Ÿ, "
                    f"ç´¯è¨ˆ {len(signals)} å€‹ "
                    f"âš¡ æ‰¹æ¬¡è€—æ™‚: {batch_time:.2f}s"
                )
                
                # å†…å­˜ç®¡ç†ï¼šåˆ é™¤æ‰¹é‡ä¿¡å·å¼•ç”¨
                del batch_signals
                
                # ä»…åœ¨æå¤§é‡äº¤æ˜“å¯¹ä¸”é«˜è´Ÿè½½æ—¶æ‰å»¶è¿Ÿ
                if total_symbols > 500 and batch_idx < total_batches - 1:
                    cpu_usage = psutil.cpu_percent(interval=0)
                    if cpu_usage > 80:
                        await asyncio.sleep(0.05)
            
            # âœ¨ æ€§èƒ½çµ±è¨ˆ
            total_duration = time.time() - start_time
            avg_per_symbol = total_duration / max(total_symbols, 1)
            
            logger.info(
                f"âœ… æ‰¹é‡åˆ†æå®Œæˆ: åˆ†æ {total_symbols} å€‹äº¤æ˜“å°, "
                f"ç”Ÿæˆ {len(signals)} å€‹ä¿¡è™Ÿ "
                f"âš¡ ç¸½è€—æ™‚: {total_duration:.2f}s "
                f"(å¹³å‡ {avg_per_symbol*1000:.1f}ms/äº¤æ˜“å°)"
            )
            
            # âœ¨ è¨˜éŒ„æ€§èƒ½
            if self.perf_monitor:
                self.perf_monitor.record_operation("analyze_batch", total_duration)
            
            return signals
            
        except Exception as e:
            logger.error(f"æ‰¹é‡åˆ†æå¤±æ•—: {e}", exc_info=True)
            return []
    
    async def analyze_async(self, symbols: List[str], data_manager) -> List[Dict]:
        """
        å¼‚æ­¥å¹¶è¡Œåˆ†æå¤šä¸ªç¬¦å·ï¼ˆæ–‡æ¡£æ­¥éª¤2è¦æ±‚çš„ç®€åŒ–æ¥å£ï¼‰
        
        ä½¿ç”¨å…¨å±€è¿›ç¨‹æ± å¹¶å‘åˆ†æï¼Œé¿å…é‡å¤åˆ›å»º/é”€æ¯è¿›ç¨‹
        
        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            data_manager: æ•°æ®ç®¡ç†å™¨
        
        Returns:
            List[Dict]: åˆ†æç»“æœåˆ—è¡¨
        """
        loop = asyncio.get_event_loop()
        executor = self.global_pool.get_executor()
        
        # ä¸ºæ¯ä¸ªç¬¦å·åˆ›å»ºä»»åŠ¡
        tasks = []
        for symbol in symbols:
            # è·å–å¤šæ—¶é—´æ¡†æ¶æ•°æ®
            multi_tf_data = await data_manager.get_multi_timeframe_data(symbol)
            
            if multi_tf_data is None:
                continue
            
            # æäº¤åˆ°å…¨å±€è¿›ç¨‹æ± 
            task = loop.run_in_executor(
                executor,
                self._analyze_single_symbol,
                (symbol, multi_tf_data)
            )
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰åˆ†æä»»åŠ¡
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿‡æ»¤å¼‚å¸¸ç»“æœ
        signals = [r for r in results if r is not None and not isinstance(r, Exception)]
        
        return signals
    
    def _analyze_single_symbol(self, args) -> Optional[Dict]:
        """
        å•ä¸€ç¬¦å·åˆ†æ - åœ¨å­è¿›ç¨‹ä¸­æ‰§è¡Œï¼ˆæ–‡æ¡£æ­¥éª¤2è¦æ±‚ï¼‰
        
        è¿™ä¸ªæ–¹æ³•åœ¨å­è¿›ç¨‹ä¸­è¿è¡Œï¼Œå¯ä»¥ä½¿ç”¨é¢„åŠ è½½çš„ ml_model
        
        Args:
            args: (symbol, multi_tf_data) å…ƒç»„
        
        Returns:
            Optional[Dict]: åˆ†æä¿¡å·
        """
        try:
            symbol, multi_tf_data = args
            
            # ä½¿ç”¨é¢„åŠ è½½çš„ç­–ç•¥å¼•æ“ï¼ˆé€šè¿‡analyze_symbol_workerï¼‰
            # è¿™ä¸ªå‡½æ•°å·²ç»åœ¨global_poolä¸­å®šä¹‰ï¼Œå¯ä»¥è®¿é—®_worker_strategy
            return analyze_symbol_worker(args)
            
        except Exception as e:
            logger.error(f"åˆ†æç¬¦å· {args[0] if args else 'unknown'} å¤±è´¥: {e}")
            return None
    
    async def close(self):
        """
        é—œé–‰åŸ·è¡Œå™¨
        
        æ³¨æ„ï¼šv3.12.0 ä¸å†å…³é—­å…¨å±€è¿›ç¨‹æ± ï¼ˆç”±åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼‰
        """
        logger.info("ä¸¦è¡Œåˆ†æå™¨é—œé–‰ï¼ˆå…¨å±€é€²ç¨‹æ± ç»§ç»­è¿è¡Œï¼‰")
