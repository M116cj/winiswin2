"""
ä¸¦è¡Œåˆ†æå™¨ï¼ˆv4.6.0+ asyncioä¸¦ç™¼å„ªåŒ–ç‰ˆæœ¬ï¼‰
è·è²¬ï¼šæ‰¹é‡è™•ç†å¤§é‡äº¤æ˜“å°åˆ†æ

v4.6.0+ é‡å¤§å„ªåŒ–ï¼š
- æ·»åŠ ConcurrentMarketScannerï¼ˆçœŸæ­£çš„asyncioä¸¦ç™¼ï¼‰
- ä½¿ç”¨asyncio.Semaphoreæ§åˆ¶ä¸¦ç™¼æ•¸ï¼ˆé¿å…éè¼‰ï¼‰
- ä¿ç•™ThreadPoolç‰ˆæœ¬ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
- æ€§èƒ½æå‡ï¼š60ç§’ â†’ 10ç§’ï¼ˆ6xåŠ é€Ÿï¼‰

v3.17+ å„ªåŒ–ï¼š
- ä½¿ç”¨å…§å»º ThreadPoolExecutorï¼ˆç„¡å¤–éƒ¨ä¾è³´ï¼‰
- ç§»é™¤æ‰€æœ‰ ProcessPool éºç•™ä»£ç¢¼
- ç¢ºä¿èˆ‡ ICT ç­–ç•¥å…¼å®¹
"""

import asyncio
from typing import List, Dict, Optional, Any
import logging
import time
from concurrent.futures import ThreadPoolExecutor, TimeoutError

from src.core.unified_config_manager import config_manager as config

logger = logging.getLogger(__name__)


class ConcurrentMarketScanner:
    """
    ä¸¦ç™¼å¸‚å ´æƒæå™¨ - v4.6.0å„ªåŒ–
    
    è·è²¬ï¼š
    - ä½¿ç”¨asyncioä¸¦ç™¼åˆ†æå¤šå€‹äº¤æ˜“å°
    - é€šéSemaphoreæ§åˆ¶ä¸¦ç™¼æ•¸ï¼ˆé˜²æ­¢éè¼‰ï¼‰
    - æ€§èƒ½ç›£æ§å’Œçµ±è¨ˆ
    
    æ€§èƒ½é æ¸¬ï¼š
    - 200å€‹äº¤æ˜“å°ï¼Œæ¯å€‹300ms
    - é †åºè™•ç†ï¼š60ç§’
    - ä¸¦ç™¼20å€‹ï¼š10ç§’ï¼ˆ6xæå‡ï¼‰
    """
    
    def __init__(self, concurrency_limit: int = 20):
        """
        åˆå§‹åŒ–ä¸¦ç™¼æƒæå™¨
        
        Args:
            concurrency_limit: æœ€å¤§ä¸¦ç™¼æ•¸ï¼ˆé»˜èª20ï¼‰
        """
        self.semaphore = asyncio.Semaphore(concurrency_limit)
        self.concurrency_limit = concurrency_limit
        self.logger = logging.getLogger(__name__)
        
        self.logger.info(f"âœ… ConcurrentMarketScanneråˆå§‹åŒ–ï¼ˆä¸¦ç™¼é™åˆ¶: {concurrency_limit}ï¼‰")
    
    async def analyze_symbol_concurrent(
        self, 
        symbol: str, 
        multi_tf_data: Dict[str, Any],
        analyzer: Any,
        timeout: int = 30
    ) -> Optional[Dict]:
        """
        ä¸¦ç™¼åˆ†æå–®å€‹äº¤æ˜“å°ï¼ˆå¸¶ä¸¦ç™¼æ§åˆ¶ï¼‰
        
        Args:
            symbol: äº¤æ˜“å°åç¨±
            multi_tf_data: å¤šæ™‚é–“æ¡†æ¶æ•¸æ“š
            analyzer: åˆ†æå™¨å¯¦ä¾‹ï¼ˆéœ€è¦æœ‰analyzeæ–¹æ³•ï¼‰
            timeout: è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
        
        Returns:
            Optional[Dict]: äº¤æ˜“ä¿¡è™Ÿï¼Œå¦‚æœåˆ†æå¤±æ•—è¿”å›None
        """
        async with self.semaphore:
            try:
                # èª¿ç”¨analyzer.analyze()ï¼ˆè¿”å›signal, confidence, win_probï¼‰
                # éœ€è¦åœ¨asyncioç’°å¢ƒä¸­é‹è¡ŒåŒæ­¥æ–¹æ³•
                loop = asyncio.get_event_loop()
                
                # ä½¿ç”¨run_in_executoråŸ·è¡ŒåŒæ­¥åˆ†ææ–¹æ³•ï¼ˆå¸¶è¶…æ™‚ï¼‰
                result = await asyncio.wait_for(
                    loop.run_in_executor(
                        None,
                        analyzer.analyze,
                        symbol,
                        multi_tf_data
                    ),
                    timeout=timeout
                )
                
                # analyzer.analyzeè¿”å›(signal, confidence, win_prob)
                signal, confidence, win_prob = result
                
                return signal  # è¿”å›ä¿¡è™Ÿï¼ˆå¦‚æœæœ‰ï¼‰
                
            except asyncio.TimeoutError:
                self.logger.warning(f"âš ï¸ {symbol} åˆ†æè¶…æ™‚ï¼ˆ{timeout}ç§’ï¼‰")
                return None
            except Exception as e:
                self.logger.debug(f"åˆ†æ {symbol} å¤±æ•—: {e}")
                return None
    
    async def scan_batch_concurrent(
        self,
        symbols: List[str],
        batch_data: Dict[str, Dict[str, Any]],
        analyzer: Any,
        timeout: int = 30
    ) -> List[Dict]:
        """
        ä¸¦ç™¼æƒæä¸€æ‰¹äº¤æ˜“å°
        
        Args:
            symbols: äº¤æ˜“å°åˆ—è¡¨
            batch_data: æ‰¹é‡æ•¸æ“šå­—å…¸ {symbol: multi_tf_data}
            analyzer: åˆ†æå™¨å¯¦ä¾‹
            timeout: å–®å€‹åˆ†æè¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
        
        Returns:
            List[Dict]: æœ‰æ•ˆä¿¡è™Ÿåˆ—è¡¨
        """
        start_time = time.time()
        
        # ğŸ”¥ v4.6.0ä¿®å¤ï¼šä½¿ç”¨create_taskç«‹å³è°ƒåº¦ï¼Œç¡®ä¿çœŸæ­£å¹¶å‘
        tasks = []
        for symbol in symbols:
            multi_tf_data = batch_data.get(symbol, {})
            if not multi_tf_data:
                continue
            
            # ç«‹å³è°ƒåº¦ä»»åŠ¡ï¼ˆä¸æ˜¯åˆ›å»ºåç¨‹å¯¹è±¡ï¼‰
            task = asyncio.create_task(
                self.analyze_symbol_concurrent(
                    symbol, 
                    multi_tf_data, 
                    analyzer,
                    timeout
                )
            )
            tasks.append((symbol, task))
        
        # ä¸¦ç™¼åŸ·è¡Œæ‰€æœ‰ä»»å‹™ï¼ˆæœ€å¤šconcurrency_limitå€‹åŒæ™‚ï¼‰
        results = await asyncio.gather(
            *[task for _, task in tasks],
            return_exceptions=True
        )
        
        # éæ¿¾æœ‰æ•ˆä¿¡è™Ÿ
        signals = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                symbol = tasks[i][0]
                self.logger.error(f"âŒ {symbol} åˆ†æç•°å¸¸: {result}")
            elif result is not None:
                signals.append(result)
        
        # æ€§èƒ½çµ±è¨ˆ
        duration = time.time() - start_time
        avg_per_symbol = (duration / len(symbols) * 1000) if symbols else 0
        
        self.logger.info(
            f"âœ… ä¸¦ç™¼æƒæå®Œæˆ: {len(symbols)}å€‹äº¤æ˜“å° | "
            f"è€—æ™‚{duration:.2f}ç§’ | "
            f"å¹³å‡{avg_per_symbol:.1f}ms/äº¤æ˜“å° | "
            f"{len(signals)}å€‹ä¿¡è™Ÿ"
        )
        
        return signals


# ğŸ”¥ v3.16.2 ä¿®å¾©ï¼šå·¥ä½œå‡½æ•¸ï¼ˆç·šç¨‹æ± ç‰ˆæœ¬ï¼Œç„¡åºåˆ—åŒ–å•é¡Œï¼‰
def _analyze_single_symbol_worker(symbol: str, market_data: dict, config_dict: dict) -> Optional[Dict]:
    """
    å–®å€‹äº¤æ˜“å°åˆ†æï¼ˆç·šç¨‹æ± å·¥ä½œå‡½æ•¸ï¼‰
    
    v3.16.2 ThreadPool ç‰ˆæœ¬ï¼š
    - ä½¿ç”¨ç·šç¨‹æ± ï¼Œå…±äº«å…§å­˜ï¼Œç„¡åºåˆ—åŒ–éœ€æ±‚
    - å¯ä»¥ç›´æ¥ä½¿ç”¨æ¨¡å¡Šç´š loggerï¼ˆç„¡ thread.lock å•é¡Œï¼‰
    - åƒæ•¸ä¿æŒæ‰å¹³åŒ–ä»¥ä¾¿èª¿ç”¨
    
    Args:
        symbol: äº¤æ˜“å°åç¨±ï¼ˆstrï¼‰
        market_data: å¸‚å ´æ•¸æ“šï¼ˆDict[str, Dict[str, Any]]ï¼‰ï¼Œå·²è½‰æ›ç‚ºç´”å­—å…¸
        config_dict: é…ç½®åƒæ•¸ï¼ˆDict[str, Union[int, float, str, bool]]ï¼‰ï¼ŒåªåŒ…å«åŸºæœ¬é¡å‹
    
    Returns:
        Optional[Dict]: äº¤æ˜“ä¿¡è™Ÿ
    """
    # ğŸ”¥ ç·šç¨‹æ± å¯ä»¥ç›´æ¥ä½¿ç”¨ loggerï¼ˆç„¡åºåˆ—åŒ–å•é¡Œï¼‰
    import logging
    import pandas as pd
    
    try:
        # ğŸ”¥ æ­¥é©Ÿ1ï¼šé‡å»º DataFrameï¼ˆå¾ç´”å­—å…¸æ¢å¾©ï¼‰
        reconstructed_data = {}
        for tf_key, tf_dict in market_data.items():
            if tf_dict is not None and isinstance(tf_dict, dict) and 'data' in tf_dict:
                # å¾ç´”å­—å…¸é‡å»º DataFrame
                df = pd.DataFrame(tf_dict['data'])
                if 'index' in tf_dict:
                    df.index = tf_dict['index']
                reconstructed_data[tf_key] = df
            else:
                reconstructed_data[tf_key] = None
        
        # ğŸ”¥ æ­¥é©Ÿ2ï¼šåœ¨ç·šç¨‹å…§é‡å»º Config å°è±¡
        from src.core.unified_config_manager import config_manager as config
        config = Config()
        # æ‡‰ç”¨å‚³å…¥çš„é…ç½®åƒæ•¸
        for key, value in config_dict.items():
            if hasattr(config, key):
                setattr(config, key, value)
        
        # ğŸ”¥ æ­¥é©Ÿ3ï¼šä½¿ç”¨ ICT ç­–ç•¥åŸ·è¡Œåˆ†æ
        result = None
        try:
            from src.strategies.ict_strategy import ICTStrategy
            trader = ICTStrategy()
            result = trader.analyze(symbol, reconstructed_data)
        except Exception as e:
            logger.error(f"âŒ {symbol} ICT ç­–ç•¥åˆ†æå¤±æ•—: {e}")
            result = None
        
        return result
        
    except MemoryError:
        logger.error(f"âŒ {symbol} è¨˜æ†¶é«”ä¸è¶³")
        return None
    except Exception as e:
        logger.error(f"âŒ {symbol} åˆ†æå¤±æ•—: {e}")
        return None


class ParallelAnalyzer:
    """ä¸¦è¡Œåˆ†æå™¨ - v3.17+ ThreadPool ç‰ˆæœ¬"""
    
    def __init__(self, max_workers: Optional[int] = None, perf_monitor=None):
        """
        åˆå§‹åŒ–ä¸¦è¡Œåˆ†æå™¨
        
        Args:
            max_workers: ç·šç¨‹æ± å·¥ä½œç·šç¨‹æ•¸ï¼ˆé è¨­ä½¿ç”¨ config.MAX_WORKERSï¼‰
            perf_monitor: æ€§èƒ½ç›£æ§å™¨
        """
        self.config = Config()
        self.max_workers = max_workers or config.MAX_WORKERS
        self.executor = ThreadPoolExecutor(max_workers=self.max_workers)
        
        # âœ¨ æ€§èƒ½ç›£æ§
        self.perf_monitor = perf_monitor
        
        logger.info("âœ… ä¸¦è¡Œåˆ†æå™¨åˆå§‹åŒ–ï¼ˆv3.17+ ThreadPoolï¼‰")
        logger.info(f"   ç·šç¨‹æ± å·¥ä½œç·šç¨‹: {self.max_workers}")
    
    async def analyze_batch(
        self,
        symbols_data: List[Dict],
        data_manager
    ) -> List[Dict]:
        """
        æ‰¹é‡ä¸¦è¡Œåˆ†æå¤šå€‹äº¤æ˜“å°ï¼ˆv3.16.1 å®‰å…¨ç‰ˆæœ¬ï¼‰
        
        Args:
            symbols_data: äº¤æ˜“å°åˆ—è¡¨
            data_manager: æ•¸æ“šç®¡ç†å™¨å¯¦ä¾‹
        
        Returns:
            List[Dict]: ç”Ÿæˆçš„äº¤æ˜“ä¿¡è™Ÿåˆ—è¡¨
        """
        try:
            start_time = time.time()
            total_symbols = len(symbols_data)
            
            logger.info(f"é–‹å§‹æ‰¹é‡åˆ†æ {total_symbols} å€‹äº¤æ˜“å°")
            
            signals = []
            loop = asyncio.get_event_loop()
            tasks = []
            
            # ğŸ”¥ æ­¥é©Ÿ1ï¼šä¸¦è¡Œç²å–æ‰€æœ‰æ•¸æ“š
            data_tasks = [
                data_manager.get_multi_timeframe_data(item['symbol'])
                for item in symbols_data
            ]
            
            multi_tf_data_list = await asyncio.gather(*data_tasks, return_exceptions=True)
            
            # ğŸ”¥ æ­¥é©Ÿ2ï¼šæäº¤æ‰€æœ‰åˆ†æä»»å‹™ï¼ˆä½¿ç”¨å®Œå…¨ç„¡é–‰åŒ…è¨­è¨ˆï¼‰
            for i, multi_tf_data in enumerate(multi_tf_data_list):
                # æª¢æŸ¥æ•¸æ“šæœ‰æ•ˆæ€§
                if isinstance(multi_tf_data, Exception) or multi_tf_data is None:
                    continue
                
                # ç¢ºä¿æ˜¯å­—å…¸é¡å‹
                if not isinstance(multi_tf_data, dict):
                    continue
                
                symbol = symbols_data[i]['symbol']
                
                # ğŸ”¥ è½‰æ› DataFrame ç‚ºç´”å­—å…¸ï¼ˆ100% å¯åºåˆ—åŒ–ï¼‰
                market_data = {}
                for tf_key, df in multi_tf_data.items():
                    if df is not None and hasattr(df, 'to_dict'):
                        # è½‰æ›ç‚ºç´” Python åŸºæœ¬é¡å‹
                        market_data[tf_key] = {
                            'data': df.to_dict('list'),  # list of lists/dicts
                            'index': df.index.tolist() if hasattr(df.index, 'tolist') else list(df.index)
                        }
                    else:
                        market_data[tf_key] = None
                
                # ğŸ”¥ å‰µå»ºé…ç½®å­—å…¸ï¼ˆåªåŒ…å«åŸºæœ¬é¡å‹ï¼šint, float, str, boolï¼‰
                config_dict = {
                    'MIN_CONFIDENCE': float(getattr(self.config, 'MIN_CONFIDENCE', 0.6)),
                    'TRADING_ENABLED': bool(getattr(self.config, 'TRADING_ENABLED', True))
                }
                
                # ğŸ”¥ v3.17+: ä½¿ç”¨æ¨™æº–ç·šç¨‹æ± æäº¤ä»»å‹™
                future = self.executor.submit(
                    _analyze_single_symbol_worker,
                    symbol,
                    market_data,
                    config_dict
                )
                tasks.append((symbol, future))
            
            # ğŸ”¥ æ­¥é©Ÿ3ï¼šæ”¶é›†çµæœï¼ˆå¸¶è¶…æ™‚æ©Ÿåˆ¶ï¼‰
            timeout_seconds = self.config.PROCESS_TIMEOUT_SECONDS
            for symbol, future in tasks:
                try:
                    # ä½¿ç”¨é…ç½®çš„è¶…æ™‚æ™‚é–“
                    result = await loop.run_in_executor(None, future.result, timeout_seconds)
                    if result:
                        signals.append(result)
                except TimeoutError:
                    logger.warning(f"âš ï¸ åˆ†æ {symbol} è¶…æ™‚ï¼ˆ{timeout_seconds}ç§’ï¼‰")
                except Exception as e:
                    logger.error(f"âŒ åˆ†æ {symbol} å¤±æ•—: {e}")
            
            # âœ¨ æ€§èƒ½çµ±è¨ˆ
            total_duration = time.time() - start_time
            avg_per_symbol = total_duration / max(total_symbols, 1)
            
            logger.info(
                f"âœ… æ‰¹é‡åˆ†æå®Œæˆ: åˆ†æ {total_symbols} å€‹äº¤æ˜“å°, "
                f"ç”Ÿæˆ {len(signals)} å€‹ä¿¡è™Ÿ "
                f"âš¡ ç¸½è€—æ™‚: {total_duration:.2f}s "
                f"(å¹³å‡ {avg_per_symbol*1000:.1f}ms/äº¤æ˜“å°)"
            )
            
            # âœ¨ è¨˜éŒ„æ€§èƒ½
            if self.perf_monitor:
                self.perf_monitor.record_operation("analyze_batch", total_duration)
            
            return signals
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡åˆ†æå¤±æ•—: {e}", exc_info=True)
            return []
    
    async def scan_concurrent(
        self,
        symbols: List[str],
        batch_data: Dict[str, Dict[str, Any]],
        analyzer: Any,
        concurrency_limit: Optional[int] = None
    ) -> List[Dict]:
        """
        ä¸¦ç™¼æƒæäº¤æ˜“å°ï¼ˆv4.6.0+ asyncioå„ªåŒ–ç‰ˆæœ¬ï¼‰
        
        ä½¿ç”¨ConcurrentMarketScanneré€²è¡ŒçœŸæ­£çš„asyncioä¸¦ç™¼åˆ†æï¼Œ
        æ€§èƒ½æå‡ï¼š60ç§’ â†’ 10ç§’ï¼ˆ6xåŠ é€Ÿï¼‰
        
        Args:
            symbols: äº¤æ˜“å°åˆ—è¡¨
            batch_data: æ‰¹é‡æ•¸æ“šå­—å…¸ {symbol: multi_tf_data}
            analyzer: åˆ†æå™¨å¯¦ä¾‹ï¼ˆéœ€è¦æœ‰analyzeæ–¹æ³•ï¼‰
            concurrency_limit: ä¸¦ç™¼é™åˆ¶ï¼ˆé»˜èªä½¿ç”¨config.CONCURRENT_SCAN_LIMITï¼‰
        
        Returns:
            List[Dict]: æœ‰æ•ˆä¿¡è™Ÿåˆ—è¡¨
        """
        try:
            start_time = time.time()
            
            # ä½¿ç”¨é…ç½®çš„ä¸¦ç™¼é™åˆ¶
            limit = concurrency_limit or self.config.CONCURRENT_SCAN_LIMIT
            
            logger.info(f"ğŸš€ é–‹å§‹ä¸¦ç™¼æƒæ {len(symbols)} å€‹äº¤æ˜“å°ï¼ˆä¸¦ç™¼é™åˆ¶: {limit}ï¼‰")
            
            # å‰µå»ºä¸¦ç™¼æƒæå™¨
            scanner = ConcurrentMarketScanner(concurrency_limit=limit)
            
            # åŸ·è¡Œä¸¦ç™¼æƒæ
            signals = await scanner.scan_batch_concurrent(
                symbols=symbols,
                batch_data=batch_data,
                analyzer=analyzer,
                timeout=self.config.PROCESS_TIMEOUT_SECONDS
            )
            
            # âœ¨ æ€§èƒ½çµ±è¨ˆ
            total_duration = time.time() - start_time
            avg_per_symbol = (total_duration / len(symbols) * 1000) if symbols else 0
            
            logger.info(
                f"âœ… ä¸¦ç™¼æƒæå®Œæˆ: åˆ†æ {len(symbols)} å€‹äº¤æ˜“å°, "
                f"ç”Ÿæˆ {len(signals)} å€‹ä¿¡è™Ÿ "
                f"âš¡ ç¸½è€—æ™‚: {total_duration:.2f}s "
                f"(å¹³å‡ {avg_per_symbol:.1f}ms/äº¤æ˜“å°) "
                f"âš¡ æ€§èƒ½æå‡: {60/total_duration:.1f}x"
            )
            
            # âœ¨ è¨˜éŒ„æ€§èƒ½
            if self.perf_monitor:
                self.perf_monitor.record_operation("scan_concurrent", total_duration)
            
            return signals
            
        except Exception as e:
            logger.error(f"âŒ ä¸¦ç™¼æƒæå¤±æ•—: {e}", exc_info=True)
            return []
    
    async def close(self):
        """é—œé–‰åŸ·è¡Œå™¨ï¼ˆv3.17+ï¼‰"""
        if self.executor:
            self.executor.shutdown(wait=True)
            logger.info("âœ… ä¸¦è¡Œåˆ†æå™¨ç·šç¨‹æ± å·²é—œé–‰")
