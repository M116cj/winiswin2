"""
ä¸¦è¡Œåˆ†æå™¨ï¼ˆv3.16.2 åºåˆ—åŒ–ä¿®å¾©ç‰ˆï¼‰
è·è²¬ï¼šæ‰¹é‡è™•ç†å¤§é‡äº¤æ˜“å°åˆ†æã€è‡ªå‹•é‡å»ºæå£é€²ç¨‹æ± ã€å…§å­˜ç›£æ§

v3.16.2 ä¿®å¾©ï¼ˆ2025-10-28ï¼‰ï¼š
- ä¿®å¾©å­é€²ç¨‹ logger åºåˆ—åŒ–å•é¡Œï¼ˆthread.lock éŒ¯èª¤ï¼‰
- åœ¨å­é€²ç¨‹å…§éƒ¨å‰µå»ºç¨ç«‹ loggerï¼ˆé¿å…åºåˆ—åŒ–æ¨¡å¡Šç´šåˆ¥ loggerï¼‰

v3.16.1 ä¿®å¾©ï¼š
- é‡æ–°å•Ÿç”¨é€²ç¨‹æ± ï¼ˆä½¿ç”¨å®‰å…¨æäº¤æ©Ÿåˆ¶ï¼‰
- æ·»åŠ  BrokenProcessPool è‡ªå‹•æ¢å¾©
- æ·»åŠ å­é€²ç¨‹å…§å­˜ç›£æ§
- æ·»åŠ  fallback é™ç´šç­–ç•¥
- æ·»åŠ è¶…æ™‚æ©Ÿåˆ¶ï¼ˆ30ç§’/ä»»å‹™ï¼‰
"""

import asyncio
from typing import List, Dict, Optional
import logging
import time
from concurrent.futures import TimeoutError
from concurrent.futures.process import BrokenProcessPool

from src.core.global_pool import GlobalProcessPool
from src.config import Config

logger = logging.getLogger(__name__)


# ğŸ”¥ v3.16.2 ä¿®å¾©ï¼šæ¨¡å¡Šç´šåˆ¥å·¥ä½œå‡½æ•¸ï¼ˆé¿å…åºåˆ—åŒ–é¡æ™‚åŒ…å« thread.lockï¼‰
def _analyze_single_symbol_worker(symbol_data: Dict, model_path: Optional[str], config_dict: Dict) -> Optional[Dict]:
    """
    å–®å€‹äº¤æ˜“å°åˆ†æï¼ˆå·¥ä½œé€²ç¨‹å‡½æ•¸ï¼‰
    
    ğŸ”¥ v3.16.2 é—œéµä¿®å¾©ï¼š
    - å¿…é ˆæ˜¯æ¨¡å¡Šç´šåˆ¥å‡½æ•¸ï¼ˆä¸èƒ½æ˜¯é¡æ–¹æ³•ï¼‰
    - é¿å…åºåˆ—åŒ–é¡æ™‚åŒ…å«æ¨¡å¡Šç´š loggerï¼ˆå« thread.lockï¼‰
    - è¼¸å…¥æ•¸æ“šå·²è½‰æ›ç‚ºç´” Python å­—å…¸ï¼ˆç„¡ DataFrameï¼‰
    
    Args:
        symbol_data: {'symbol': str, 'data': Dict[str, Dict]}
        model_path: ML æ¨¡å‹è·¯å¾‘ï¼ˆå¯é¸ï¼‰
        config_dict: é…ç½®å­—å…¸ï¼ˆç´”æ•¸æ“šï¼‰
    
    Returns:
        Optional[Dict]: äº¤æ˜“ä¿¡è™Ÿ
    """
    # ğŸ”¥ å­é€²ç¨‹å…§éƒ¨å‰µå»ºç¨ç«‹ logger
    import logging
    import pandas as pd
    proc_logger = logging.getLogger(f"{__name__}.subprocess")
    
    try:
        # ğŸ”¥ æ­¥é©Ÿ1ï¼šé‡å»º DataFrameï¼ˆå¾ç´”å­—å…¸æ¢å¾©ï¼‰
        reconstructed_data = {}
        for tf_key, tf_dict in symbol_data['data'].items():
            if tf_dict is not None and 'data' in tf_dict:
                # å¾å­—å…¸é‡å»º DataFrame
                df = pd.DataFrame(tf_dict['data'])
                if 'index' in tf_dict:
                    df.index = tf_dict['index']
                reconstructed_data[tf_key] = df
            else:
                reconstructed_data[tf_key] = None
        
        # ğŸ”¥ æ­¥é©Ÿ2ï¼šæ·»åŠ è¨˜æ†¶é«”ç›£æ§
        process = None
        try:
            import psutil
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        except ImportError:
            initial_memory = None
        
        # ğŸ”¥ æ­¥é©Ÿ3ï¼šé‡å»ºé…ç½®
        from src.config import Config
        config = Config()
        for key, value in config_dict.items():
            if hasattr(config, key):
                setattr(config, key, value)
        
        # ğŸ”¥ æ­¥é©Ÿ4ï¼šå˜—è©¦ä½¿ç”¨è‡ªæˆ‘å­¸ç¿’äº¤æ˜“å“¡
        try:
            from src.strategies.self_learning_trader import SelfLearningTrader
            
            trader = SelfLearningTrader(config=config)
            result = trader.analyze(symbol_data['symbol'], reconstructed_data)
            
        except Exception as e:
            # ğŸ”¥ é™ç´šåˆ° ICT ç­–ç•¥
            proc_logger.warning(f"âš ï¸ è‡ªæˆ‘å­¸ç¿’äº¤æ˜“å“¡ä¸å¯ç”¨ ({e})ï¼Œä½¿ç”¨é™ç´šç­–ç•¥")
            from src.strategies.ict_strategy import ICTStrategy
            trader = ICTStrategy()
            result = trader.analyze(symbol_data['symbol'], reconstructed_data)
        
        # ğŸ”¥ è¨˜æ†¶é«”ç›£æ§
        if initial_memory is not None and process is not None:
            try:
                final_memory = process.memory_info().rss / 1024 / 1024  # MB
                memory_increase = final_memory - initial_memory
                
                if memory_increase > 500:  # è¨˜æ†¶é«”å¢åŠ è¶…é 500MB
                    proc_logger.warning(
                        f"âš ï¸ è¨˜æ†¶é«”æ´©æ¼è­¦å‘Š {symbol_data['symbol']}: +{memory_increase:.1f}MB"
                    )
            except Exception:
                pass
        
        return result
        
    except MemoryError:
        proc_logger.error(f"âŒ è¨˜æ†¶é«”ä¸è¶³ {symbol_data.get('symbol', 'UNKNOWN')}")
        return None
    except Exception as e:
        proc_logger.error(f"âŒ åˆ†æå¤±æ•— {symbol_data.get('symbol', 'UNKNOWN')}: {e}")
        return None


class ParallelAnalyzer:
    """ä¸¦è¡Œåˆ†æå™¨ - v3.16.1 BrokenProcessPool ä¿®å¾©ç‰ˆ"""
    
    def __init__(self, max_workers: Optional[int] = None, perf_monitor=None):
        """
        åˆå§‹åŒ–ä¸¦è¡Œåˆ†æå™¨
        
        Args:
            max_workers: æœ€å¤§å·¥ä½œé€²ç¨‹æ•¸ï¼ˆæœªä½¿ç”¨ï¼Œç”± GlobalProcessPool ç®¡ç†ï¼‰
            perf_monitor: æ€§èƒ½ç›£æ§å™¨
        """
        self.config = Config()  # ğŸ”¥ ä¿®å¤ï¼šå®ä¾‹åŒ– Config å¯¹è±¡
        self.global_pool = GlobalProcessPool()
        self._model_path = "data/models/model.onnx"
        
        # âœ¨ æ€§èƒ½ç›£æ§
        self.perf_monitor = perf_monitor
        
        logger.info("âœ… ä¸¦è¡Œåˆ†æå™¨åˆå§‹åŒ–: ä½¿ç”¨å…¨å±€é€²ç¨‹æ± ï¼ˆv3.16.1 å®‰å…¨ç‰ˆæœ¬ï¼‰")
        logger.info(f"   é€²ç¨‹æ± ç‹€æ…‹: {self.global_pool.get_pool_health()}")
    
    async def analyze_batch(
        self,
        symbols_data: List[Dict],
        data_manager
    ) -> List[Dict]:
        """
        æ‰¹é‡ä¸¦è¡Œåˆ†æå¤šå€‹äº¤æ˜“å°ï¼ˆv3.16.1 å®‰å…¨ç‰ˆæœ¬ï¼‰
        
        Args:
            symbols_data: äº¤æ˜“å°åˆ—è¡¨
            data_manager: æ•¸æ“šç®¡ç†å™¨å¯¦ä¾‹
        
        Returns:
            List[Dict]: ç”Ÿæˆçš„äº¤æ˜“ä¿¡è™Ÿåˆ—è¡¨
        """
        try:
            start_time = time.time()
            total_symbols = len(symbols_data)
            
            logger.info(f"é–‹å§‹æ‰¹é‡åˆ†æ {total_symbols} å€‹äº¤æ˜“å°")
            
            signals = []
            loop = asyncio.get_event_loop()
            tasks = []
            
            # ğŸ”¥ æ­¥é©Ÿ1ï¼šä¸¦è¡Œç²å–æ‰€æœ‰æ•¸æ“š
            data_tasks = [
                data_manager.get_multi_timeframe_data(item['symbol'])
                for item in symbols_data
            ]
            
            multi_tf_data_list = await asyncio.gather(*data_tasks, return_exceptions=True)
            
            # ğŸ”¥ æ­¥é©Ÿ2ï¼šæäº¤æ‰€æœ‰åˆ†æä»»å‹™ï¼ˆä½¿ç”¨å®‰å…¨æäº¤ï¼‰
            for i, multi_tf_data in enumerate(multi_tf_data_list):
                # æª¢æŸ¥æ•¸æ“šæœ‰æ•ˆæ€§
                if isinstance(multi_tf_data, Exception) or multi_tf_data is None:
                    continue
                
                # ç¢ºä¿æ˜¯å­—å…¸é¡å‹
                if not isinstance(multi_tf_data, dict):
                    continue
                
                symbol = symbols_data[i]['symbol']
                
                # ğŸ”¥ v3.16.2 é—œéµä¿®å¾©ï¼šå°‡ DataFrame è½‰æ›ç‚ºç´”å­—å…¸ï¼ˆé¿å…åºåˆ—åŒ–å•é¡Œï¼‰
                # DataFrame åœ¨æŸäº›ç’°å¢ƒä¸‹åºåˆ—åŒ–å¯èƒ½å¤±æ•—ï¼Œè½‰æ›ç‚ºç´” Python é¡å‹æœ€å®‰å…¨
                serializable_data = {}
                for tf_key, df in multi_tf_data.items():
                    if df is not None and hasattr(df, 'to_dict'):
                        # è½‰æ›ç‚ºç´”å­—å…¸æ ¼å¼
                        serializable_data[tf_key] = {
                            'data': df.to_dict('list'),  # è½‰æ›ç‚ºåˆ—è¡¨å­—å…¸
                            'index': df.index.tolist() if hasattr(df.index, 'tolist') else list(df.index)
                        }
                    else:
                        serializable_data[tf_key] = None
                
                symbol_data = {
                    'symbol': symbol,
                    'data': serializable_data  # ç´” Python å­—å…¸
                }
                
                # ğŸ”¥ å‰µå»ºå¯åºåˆ—åŒ–çš„é…ç½®å­—å…¸ï¼ˆåªåŒ…å«åŸºæœ¬é¡å‹ï¼‰
                config_dict = {
                    'MIN_CONFIDENCE': self.config.MIN_CONFIDENCE,
                    'MAX_LEVERAGE': self.config.MAX_LEVERAGE,
                    'MIN_LEVERAGE': self.config.MIN_LEVERAGE,
                    'BASE_MARGIN_PCT': self.config.BASE_MARGIN_PCT,
                    'MIN_MARGIN_PCT': self.config.MIN_MARGIN_PCT,
                    'MAX_MARGIN_PCT': self.config.MAX_MARGIN_PCT,
                    'RISK_REWARD_RATIO': self.config.RISK_REWARD_RATIO,
                    'TRADING_ENABLED': self.config.TRADING_ENABLED
                }
                
                # ğŸ”¥ v3.16.2: ä½¿ç”¨æ¨¡å¡Šç´šå‡½æ•¸ï¼ˆé¿å…åºåˆ—åŒ–é¡ï¼‰
                future = self.global_pool.submit_safe(
                    _analyze_single_symbol_worker,
                    symbol_data,
                    self._model_path,
                    config_dict
                )
                tasks.append((symbol, future))
            
            # ğŸ”¥ æ­¥é©Ÿ3ï¼šæ”¶é›†çµæœï¼ˆå¸¶è¶…æ™‚æ©Ÿåˆ¶ï¼‰
            timeout_seconds = self.config.PROCESS_TIMEOUT_SECONDS
            for symbol, future in tasks:
                try:
                    # ä½¿ç”¨é…ç½®çš„è¶…æ™‚æ™‚é–“
                    result = await loop.run_in_executor(None, future.result, timeout_seconds)
                    if result:
                        signals.append(result)
                except TimeoutError:
                    logger.warning(f"âš ï¸ åˆ†æ {symbol} è¶…æ™‚ï¼ˆ{timeout_seconds}ç§’ï¼‰")
                except BrokenProcessPool:
                    logger.error(f"âŒ é€²ç¨‹æ± æå£ï¼ˆåˆ†æ {symbol}ï¼‰ï¼Œè·³éå‰©é¤˜ä»»å‹™")
                    break
                except Exception as e:
                    logger.error(f"âŒ åˆ†æ {symbol} å¤±æ•—: {e}")
            
            # âœ¨ æ€§èƒ½çµ±è¨ˆ
            total_duration = time.time() - start_time
            avg_per_symbol = total_duration / max(total_symbols, 1)
            
            logger.info(
                f"âœ… æ‰¹é‡åˆ†æå®Œæˆ: åˆ†æ {total_symbols} å€‹äº¤æ˜“å°, "
                f"ç”Ÿæˆ {len(signals)} å€‹ä¿¡è™Ÿ "
                f"âš¡ ç¸½è€—æ™‚: {total_duration:.2f}s "
                f"(å¹³å‡ {avg_per_symbol*1000:.1f}ms/äº¤æ˜“å°)"
            )
            
            # âœ¨ è¨˜éŒ„æ€§èƒ½
            if self.perf_monitor:
                self.perf_monitor.record_operation("analyze_batch", total_duration)
            
            return signals
            
        except BrokenProcessPool:
            logger.error("âŒ é€²ç¨‹æ± æå£ï¼Œè·³éæœ¬æ¬¡åˆ†æ")
            return []
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡åˆ†æå¤±æ•—: {e}", exc_info=True)
            return []
    
    async def close(self):
        """
        é—œé–‰åŸ·è¡Œå™¨
        
        æ³¨æ„ï¼šv3.16.1 ä¸é—œé–‰å…¨å±€é€²ç¨‹æ± ï¼ˆç”±æ‡‰ç”¨ç”Ÿå‘½é€±æœŸç®¡ç†ï¼‰
        """
        logger.info("ä¸¦è¡Œåˆ†æå™¨é—œé–‰ï¼ˆå…¨å±€é€²ç¨‹æ± ç¹¼çºŒé‹è¡Œï¼‰")
