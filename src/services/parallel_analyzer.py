"""
ä¸¦è¡Œåˆ†æå™¨ï¼ˆv3.16.2 åºåˆ—åŒ–ä¿®å¾©ç‰ˆï¼‰
è·è²¬ï¼šæ‰¹é‡è™•ç†å¤§é‡äº¤æ˜“å°åˆ†æã€è‡ªå‹•é‡å»ºæå£é€²ç¨‹æ± ã€å…§å­˜ç›£æ§

v3.16.2 ä¿®å¾©ï¼ˆ2025-10-28ï¼‰ï¼š
- ä¿®å¾©å­é€²ç¨‹ logger åºåˆ—åŒ–å•é¡Œï¼ˆthread.lock éŒ¯èª¤ï¼‰
- åœ¨å­é€²ç¨‹å…§éƒ¨å‰µå»ºç¨ç«‹ loggerï¼ˆé¿å…åºåˆ—åŒ–æ¨¡å¡Šç´šåˆ¥ loggerï¼‰

v3.16.1 ä¿®å¾©ï¼š
- é‡æ–°å•Ÿç”¨é€²ç¨‹æ± ï¼ˆä½¿ç”¨å®‰å…¨æäº¤æ©Ÿåˆ¶ï¼‰
- æ·»åŠ  BrokenProcessPool è‡ªå‹•æ¢å¾©
- æ·»åŠ å­é€²ç¨‹å…§å­˜ç›£æ§
- æ·»åŠ  fallback é™ç´šç­–ç•¥
- æ·»åŠ è¶…æ™‚æ©Ÿåˆ¶ï¼ˆ30ç§’/ä»»å‹™ï¼‰
"""

import asyncio
from typing import List, Dict, Optional
import logging
import time
from concurrent.futures import TimeoutError
from concurrent.futures.process import BrokenProcessPool

from src.core.global_pool import GlobalProcessPool
from src.config import Config

logger = logging.getLogger(__name__)


# ğŸ”¥ v3.16.2 ä¿®å¾©ï¼šæ¨¡å¡Šç´šåˆ¥å·¥ä½œå‡½æ•¸ï¼ˆå®Œå…¨ç„¡é–‰åŒ…è¨­è¨ˆï¼‰
def _analyze_single_symbol_worker(symbol: str, market_data: dict, config_dict: dict) -> Optional[Dict]:
    """
    å–®å€‹äº¤æ˜“å°åˆ†æï¼ˆç¨ç«‹å·¥ä½œå‡½æ•¸ï¼Œç„¡ä»»ä½•å¤–éƒ¨ä¾è³´ï¼‰
    
    ğŸ”¥ v3.16.2 åš´æ ¼ä¿®å¾©ï¼ˆGlobalProcessPool æ–¹æ¡ˆï¼‰ï¼š
    - å®Œå…¨ç¨ç«‹çš„æ¨¡å¡Šç´šå‡½æ•¸ï¼ˆä¸ä¾è³´ä»»ä½•é¡æˆ–æ¨¡å¡Šç‹€æ…‹ï¼‰
    - åƒæ•¸å®Œå…¨æ‰å¹³åŒ–ï¼ˆsymbol, market_data, config_dictï¼‰
    - æ‰€æœ‰åƒæ•¸éƒ½æ˜¯åŸºæœ¬é¡å‹ï¼ˆstr, dictï¼‰
    - åœ¨å­é€²ç¨‹å…§éƒ¨é‡å»ºæ‰€æœ‰è¤‡é›œå°è±¡ï¼ˆlogger, DataFrame, Config, Strategyï¼‰
    
    Args:
        symbol: äº¤æ˜“å°åç¨±ï¼ˆstrï¼‰
        market_data: å¸‚å ´æ•¸æ“šï¼ˆDict[str, Dict[str, Any]]ï¼‰ï¼Œå·²è½‰æ›ç‚ºç´”å­—å…¸
        config_dict: é…ç½®åƒæ•¸ï¼ˆDict[str, Union[int, float, str, bool]]ï¼‰ï¼ŒåªåŒ…å«åŸºæœ¬é¡å‹
    
    Returns:
        Optional[Dict]: äº¤æ˜“ä¿¡è™Ÿ
    """
    # ğŸ”¥ æ­¥é©Ÿ1ï¼šåœ¨å­é€²ç¨‹å…§éƒ¨å‰µå»ºç¨ç«‹ loggerï¼ˆé¿å…åºåˆ—åŒ–ä¸»é€²ç¨‹ loggerï¼‰
    import logging
    import pandas as pd
    proc_logger = logging.getLogger(f"worker.{symbol}")
    
    try:
        # ğŸ”¥ æ­¥é©Ÿ2ï¼šé‡å»º DataFrameï¼ˆå¾ç´”å­—å…¸æ¢å¾©ï¼‰
        reconstructed_data = {}
        for tf_key, tf_dict in market_data.items():
            if tf_dict is not None and isinstance(tf_dict, dict) and 'data' in tf_dict:
                # å¾ç´”å­—å…¸é‡å»º DataFrame
                df = pd.DataFrame(tf_dict['data'])
                if 'index' in tf_dict:
                    df.index = tf_dict['index']
                reconstructed_data[tf_key] = df
            else:
                reconstructed_data[tf_key] = None
        
        # ğŸ”¥ æ­¥é©Ÿ3ï¼šæ·»åŠ è¨˜æ†¶é«”ç›£æ§
        process = None
        initial_memory = None
        try:
            import psutil
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        except ImportError:
            pass
        
        # ğŸ”¥ æ­¥é©Ÿ4ï¼šåœ¨å­é€²ç¨‹å…§é‡å»º Config å°è±¡
        from src.config import Config
        config = Config()
        # åªæ‡‰ç”¨å‚³å…¥çš„é…ç½®åƒæ•¸
        for key, value in config_dict.items():
            if hasattr(config, key):
                setattr(config, key, value)
        
        # ğŸ”¥ æ­¥é©Ÿ5ï¼šåœ¨å­é€²ç¨‹å…§å‰µå»ºç­–ç•¥å¯¦ä¾‹ä¸¦åŸ·è¡Œåˆ†æ
        result = None
        try:
            from src.strategies.self_learning_trader import SelfLearningTrader
            trader = SelfLearningTrader(config=config)
            result = trader.analyze(symbol, reconstructed_data)
            
        except Exception as e:
            # ğŸ”¥ é™ç´šåˆ° ICT ç­–ç•¥
            proc_logger.warning(f"âš ï¸ è‡ªæˆ‘å­¸ç¿’äº¤æ˜“å“¡ä¸å¯ç”¨ ({e})ï¼Œä½¿ç”¨é™ç´šç­–ç•¥")
            try:
                from src.strategies.ict_strategy import ICTStrategy
                trader = ICTStrategy()
                result = trader.analyze(symbol, reconstructed_data)
            except Exception as fallback_error:
                proc_logger.error(f"âŒ é™ç´šç­–ç•¥ä¹Ÿå¤±æ•—: {fallback_error}")
                result = None
        
        # ğŸ”¥ æ­¥é©Ÿ6ï¼šè¨˜æ†¶é«”ç›£æ§
        if initial_memory is not None and process is not None:
            try:
                final_memory = process.memory_info().rss / 1024 / 1024  # MB
                memory_increase = final_memory - initial_memory
                
                if memory_increase > 500:  # è¨˜æ†¶é«”å¢åŠ è¶…é 500MB
                    proc_logger.warning(
                        f"âš ï¸ è¨˜æ†¶é«”æ´©æ¼è­¦å‘Š {symbol}: +{memory_increase:.1f}MB"
                    )
            except Exception:
                pass
        
        return result
        
    except MemoryError:
        proc_logger.error(f"âŒ è¨˜æ†¶é«”ä¸è¶³ {symbol}")
        return None
    except Exception as e:
        proc_logger.error(f"âŒ åˆ†æå¤±æ•— {symbol}: {e}")
        return None


class ParallelAnalyzer:
    """ä¸¦è¡Œåˆ†æå™¨ - v3.16.1 BrokenProcessPool ä¿®å¾©ç‰ˆ"""
    
    def __init__(self, max_workers: Optional[int] = None, perf_monitor=None):
        """
        åˆå§‹åŒ–ä¸¦è¡Œåˆ†æå™¨
        
        Args:
            max_workers: æœ€å¤§å·¥ä½œé€²ç¨‹æ•¸ï¼ˆæœªä½¿ç”¨ï¼Œç”± GlobalProcessPool ç®¡ç†ï¼‰
            perf_monitor: æ€§èƒ½ç›£æ§å™¨
        """
        self.config = Config()  # ğŸ”¥ ä¿®å¤ï¼šå®ä¾‹åŒ– Config å¯¹è±¡
        self.global_pool = GlobalProcessPool()
        self._model_path = "data/models/model.onnx"
        
        # âœ¨ æ€§èƒ½ç›£æ§
        self.perf_monitor = perf_monitor
        
        logger.info("âœ… ä¸¦è¡Œåˆ†æå™¨åˆå§‹åŒ–: ä½¿ç”¨å…¨å±€é€²ç¨‹æ± ï¼ˆv3.16.1 å®‰å…¨ç‰ˆæœ¬ï¼‰")
        logger.info(f"   é€²ç¨‹æ± ç‹€æ…‹: {self.global_pool.get_pool_health()}")
    
    async def analyze_batch(
        self,
        symbols_data: List[Dict],
        data_manager
    ) -> List[Dict]:
        """
        æ‰¹é‡ä¸¦è¡Œåˆ†æå¤šå€‹äº¤æ˜“å°ï¼ˆv3.16.1 å®‰å…¨ç‰ˆæœ¬ï¼‰
        
        Args:
            symbols_data: äº¤æ˜“å°åˆ—è¡¨
            data_manager: æ•¸æ“šç®¡ç†å™¨å¯¦ä¾‹
        
        Returns:
            List[Dict]: ç”Ÿæˆçš„äº¤æ˜“ä¿¡è™Ÿåˆ—è¡¨
        """
        try:
            start_time = time.time()
            total_symbols = len(symbols_data)
            
            logger.info(f"é–‹å§‹æ‰¹é‡åˆ†æ {total_symbols} å€‹äº¤æ˜“å°")
            
            signals = []
            loop = asyncio.get_event_loop()
            tasks = []
            
            # ğŸ”¥ æ­¥é©Ÿ1ï¼šä¸¦è¡Œç²å–æ‰€æœ‰æ•¸æ“š
            data_tasks = [
                data_manager.get_multi_timeframe_data(item['symbol'])
                for item in symbols_data
            ]
            
            multi_tf_data_list = await asyncio.gather(*data_tasks, return_exceptions=True)
            
            # ğŸ”¥ æ­¥é©Ÿ2ï¼šæäº¤æ‰€æœ‰åˆ†æä»»å‹™ï¼ˆä½¿ç”¨å®Œå…¨ç„¡é–‰åŒ…è¨­è¨ˆï¼‰
            for i, multi_tf_data in enumerate(multi_tf_data_list):
                # æª¢æŸ¥æ•¸æ“šæœ‰æ•ˆæ€§
                if isinstance(multi_tf_data, Exception) or multi_tf_data is None:
                    continue
                
                # ç¢ºä¿æ˜¯å­—å…¸é¡å‹
                if not isinstance(multi_tf_data, dict):
                    continue
                
                symbol = symbols_data[i]['symbol']
                
                # ğŸ”¥ è½‰æ› DataFrame ç‚ºç´”å­—å…¸ï¼ˆ100% å¯åºåˆ—åŒ–ï¼‰
                market_data = {}
                for tf_key, df in multi_tf_data.items():
                    if df is not None and hasattr(df, 'to_dict'):
                        # è½‰æ›ç‚ºç´” Python åŸºæœ¬é¡å‹
                        market_data[tf_key] = {
                            'data': df.to_dict('list'),  # list of lists/dicts
                            'index': df.index.tolist() if hasattr(df.index, 'tolist') else list(df.index)
                        }
                    else:
                        market_data[tf_key] = None
                
                # ğŸ”¥ å‰µå»ºé…ç½®å­—å…¸ï¼ˆåªåŒ…å«åŸºæœ¬é¡å‹ï¼šint, float, str, boolï¼‰
                config_dict = {
                    'MIN_CONFIDENCE': float(self.config.MIN_CONFIDENCE),
                    'MAX_LEVERAGE': int(self.config.MAX_LEVERAGE),
                    'MIN_LEVERAGE': int(self.config.MIN_LEVERAGE),
                    'BASE_MARGIN_PCT': float(self.config.BASE_MARGIN_PCT),
                    'MIN_MARGIN_PCT': float(self.config.MIN_MARGIN_PCT),
                    'MAX_MARGIN_PCT': float(self.config.MAX_MARGIN_PCT),
                    'RISK_REWARD_RATIO': float(self.config.RISK_REWARD_RATIO),
                    'TRADING_ENABLED': bool(self.config.TRADING_ENABLED)
                }
                
                # ğŸ”¥ v3.16.2 åš´æ ¼é©—è­‰ï¼šæäº¤å‰æª¢æŸ¥æ‰€æœ‰åƒæ•¸å¯åºåˆ—åŒ–
                try:
                    import pickle
                    # é©—è­‰å‡½æ•¸æœ¬èº«
                    pickle.dumps(_analyze_single_symbol_worker)
                    # é©—è­‰æ‰€æœ‰åƒæ•¸
                    pickle.dumps(symbol)  # str
                    pickle.dumps(market_data)  # dict
                    pickle.dumps(config_dict)  # dict
                except Exception as pickle_error:
                    logger.error(f"âŒ åºåˆ—åŒ–é©—è­‰å¤±æ•— {symbol}: {pickle_error}")
                    logger.error(f"   å‡½æ•¸: _analyze_single_symbol_worker")
                    logger.error(f"   symbol é¡å‹: {type(symbol)}")
                    logger.error(f"   market_data é¡å‹: {type(market_data)}")
                    logger.error(f"   config_dict é¡å‹: {type(config_dict)}")
                    continue  # è·³éç„¡æ³•åºåˆ—åŒ–çš„ä»»å‹™
                
                # ğŸ”¥ ä½¿ç”¨å®Œå…¨æ‰å¹³åŒ–çš„åƒæ•¸ï¼ˆç„¡åµŒå¥—ï¼Œç„¡é–‰åŒ…ï¼‰
                future = self.global_pool.submit_safe(
                    _analyze_single_symbol_worker,  # æ¨¡å¡Šç´šå‡½æ•¸
                    symbol,                         # str (æ‰å¹³åƒæ•¸1)
                    market_data,                    # dict (æ‰å¹³åƒæ•¸2)
                    config_dict                     # dict (æ‰å¹³åƒæ•¸3)
                )
                tasks.append((symbol, future))
            
            # ğŸ”¥ æ­¥é©Ÿ3ï¼šæ”¶é›†çµæœï¼ˆå¸¶è¶…æ™‚æ©Ÿåˆ¶ï¼‰
            timeout_seconds = self.config.PROCESS_TIMEOUT_SECONDS
            for symbol, future in tasks:
                try:
                    # ä½¿ç”¨é…ç½®çš„è¶…æ™‚æ™‚é–“
                    result = await loop.run_in_executor(None, future.result, timeout_seconds)
                    if result:
                        signals.append(result)
                except TimeoutError:
                    logger.warning(f"âš ï¸ åˆ†æ {symbol} è¶…æ™‚ï¼ˆ{timeout_seconds}ç§’ï¼‰")
                except BrokenProcessPool:
                    logger.error(f"âŒ é€²ç¨‹æ± æå£ï¼ˆåˆ†æ {symbol}ï¼‰ï¼Œè·³éå‰©é¤˜ä»»å‹™")
                    break
                except Exception as e:
                    logger.error(f"âŒ åˆ†æ {symbol} å¤±æ•—: {e}")
            
            # âœ¨ æ€§èƒ½çµ±è¨ˆ
            total_duration = time.time() - start_time
            avg_per_symbol = total_duration / max(total_symbols, 1)
            
            logger.info(
                f"âœ… æ‰¹é‡åˆ†æå®Œæˆ: åˆ†æ {total_symbols} å€‹äº¤æ˜“å°, "
                f"ç”Ÿæˆ {len(signals)} å€‹ä¿¡è™Ÿ "
                f"âš¡ ç¸½è€—æ™‚: {total_duration:.2f}s "
                f"(å¹³å‡ {avg_per_symbol*1000:.1f}ms/äº¤æ˜“å°)"
            )
            
            # âœ¨ è¨˜éŒ„æ€§èƒ½
            if self.perf_monitor:
                self.perf_monitor.record_operation("analyze_batch", total_duration)
            
            return signals
            
        except BrokenProcessPool:
            logger.error("âŒ é€²ç¨‹æ± æå£ï¼Œè·³éæœ¬æ¬¡åˆ†æ")
            return []
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡åˆ†æå¤±æ•—: {e}", exc_info=True)
            return []
    
    async def close(self):
        """
        é—œé–‰åŸ·è¡Œå™¨
        
        æ³¨æ„ï¼šv3.16.1 ä¸é—œé–‰å…¨å±€é€²ç¨‹æ± ï¼ˆç”±æ‡‰ç”¨ç”Ÿå‘½é€±æœŸç®¡ç†ï¼‰
        """
        logger.info("ä¸¦è¡Œåˆ†æå™¨é—œé–‰ï¼ˆå…¨å±€é€²ç¨‹æ± ç¹¼çºŒé‹è¡Œï¼‰")
