"""
ç»Ÿä¸€æ•°æ®è·å–ç®¡é“ v3.20

èŒè´£ï¼šç»Ÿä¸€æ‰€æœ‰Kçº¿æ•°æ®è·å–é€»è¾‘ï¼ˆSingle Data Pipelineï¼‰

æ•´åˆï¼š
- src/services/data_service.py::get_klines() (æ–¹æ³•1)
- src/services/data_service.py::get_klines_incremental() (æ–¹æ³•2)
- src/services/data_service.py::get_historical_klines() (æ–¹æ³•3)
- src/services/data_service.py::_fetch_full_klines() (æ–¹æ³•4)
- src/clients/binance_client.py::get_klines() (æ–¹æ³•5)

æ ¸å¿ƒä¼˜åŠ¿ï¼š
1. 3å±‚Fallbackç­–ç•¥ï¼šå†å²API â†’ WebSocket â†’ REST
2. æ™ºèƒ½æ‰¹é‡è·å–ï¼šå‡å°‘HTTPè¯·æ±‚æ•°
3. è‡ªé€‚åº”ç¼“å­˜ï¼šåŸºäºæ³¢åŠ¨ç‡åŠ¨æ€TTL
4. å¢é‡æ›´æ–°ä¼˜åŒ–ï¼šåªè·å–ç¼ºå¤±æ•°æ®
5. ç»Ÿä¸€é”™è¯¯å¤„ç†ï¼šä¸€è‡´çš„é‡è¯•é€»è¾‘

æ€§èƒ½ä¼˜åŒ–ï¼š
- æ‰¹é‡è·å–ï¼š3ä¸ªæ—¶é—´æ¡†æ¶å¹¶è¡Œè·å–ï¼ˆå‡å°‘ç­‰å¾…æ—¶é—´ï¼‰
- å†å²APIä¼˜å…ˆï¼šv3.19.2ç«‹å³è·å–å®Œæ•´æ•°æ®ï¼ˆ10hrsâ†’5minå¯åŠ¨ï¼‰
- æ™ºèƒ½ç¼“å­˜ï¼šå‡å°‘30-40% APIè¯·æ±‚

é¢„æœŸæ”¶ç›Šï¼š
- æ•°æ®è·å–é€Ÿåº¦ï¼š79-159ç§’ â†’ 30-60ç§’ï¼ˆ2-3å€ï¼‰
- APIè¯·æ±‚å‡å°‘ï¼š30-40%
- ä»£ç é‡å¤ï¼š5ä¸ªæ–¹æ³• â†’ 2ä¸ªæ ¸å¿ƒæ–¹æ³•
"""

import logging
import asyncio
import pandas as pd
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta

from .intelligent_cache import IntelligentCache, generate_cache_key

logger = logging.getLogger(__name__)


class UnifiedDataPipeline:
    """
    ç»Ÿä¸€æ•°æ®è·å–ç®¡é“
    
    åŠŸèƒ½ï¼š
    1. 3å±‚Fallbackæ•°æ®è·å–ï¼ˆå†å²API â†’ WebSocket â†’ RESTï¼‰
    2. æ™ºèƒ½æ‰¹é‡è·å–ï¼ˆå‡å°‘HTTPè¯·æ±‚ï¼‰
    3. å¢é‡æ›´æ–°ä¼˜åŒ–
    4. è‡ªé€‚åº”ç¼“å­˜
    5. ç»Ÿä¸€é”™è¯¯å¤„ç†
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        pipeline = UnifiedDataPipeline(binance_client, websocket_monitor)
        
        # è·å–å¤šæ—¶é—´æ¡†æ¶æ•°æ®
        data = await pipeline.get_multi_timeframe_data(
            'BTCUSDT',
            timeframes=['1h', '15m', '5m']
        )
        
        # è®¿é—®æ•°æ®
        h1_data = data['1h']
        m15_data = data['15m']
    """
    
    def __init__(
        self,
        binance_client: Any,
        websocket_monitor: Optional[Any] = None,
        cache: Optional[IntelligentCache] = None
    ):
        """
        åˆå§‹åŒ–ç»Ÿä¸€æ•°æ®è·å–ç®¡é“
        
        Args:
            binance_client: Binanceå®¢æˆ·ç«¯å®ä¾‹
            websocket_monitor: WebSocketç›‘æ§å™¨ï¼ˆå¯é€‰ï¼‰
            cache: æ™ºèƒ½ç¼“å­˜å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        self.client = binance_client
        self.ws_monitor = websocket_monitor
        
        # âœ… v3.20 Phase 3: å¯ç”¨L2æŒä¹…åŒ–ç¼“å­˜
        self.cache = cache or IntelligentCache(
            l1_max_size=5000,
            enable_l2=True,  # å¯ç”¨L2æŒä¹…åŒ–
            l2_cache_dir='/tmp/elite_cache'
        )
        
        # ç»Ÿè®¡
        self._total_requests = 0
        self._cache_hits = 0
        self._historical_api_hits = 0
        self._websocket_hits = 0
        self._rest_api_hits = 0
        
        logger.info(
            "âœ… UnifiedDataPipeline åˆå§‹åŒ–å®Œæˆ\n"
            "   ğŸ¯ 3å±‚Fallback: å†å²API â†’ WebSocket â†’ REST\n"
            "   ğŸ’¾ æ™ºèƒ½ç¼“å­˜å·²å¯ç”¨ï¼ˆL1å†…å­˜ + L2æŒä¹…åŒ–ï¼‰\n"
            f"   ğŸ“¡ WebSocket: {'å¯ç”¨' if websocket_monitor else 'ç¦ç”¨'}"
        )
    
    async def get_multi_timeframe_data(
        self,
        symbol: str,
        timeframes: List[str] = ['1h', '15m', '5m'],
        limit: int = 50
    ) -> Dict[str, pd.DataFrame]:
        """
        è·å–å¤šæ—¶é—´æ¡†æ¶æ•°æ®ï¼ˆä¸»å…¥å£ï¼‰
        
        v4.3.2+ WebSocket-onlyæ¨¡å¼ï¼š
        - ä»…ä½¿ç”¨WebSocketæ•°æ®ï¼ˆ1mèšåˆâ†’5m/15m/1hï¼‰
        - ç¦ç”¨å†å²APIå’ŒRESTå¤‡æ´
        - æ•°æ®ä¸è¶³æ—¶è¿”å›ç©ºDataFrameå¹¶æ ‡è®°warming_upçŠ¶æ€
        
        ä¼ ç»Ÿ3å±‚Fallbackç­–ç•¥ï¼ˆWEBSOCKET_ONLY_KLINES=falseæ—¶ï¼‰ï¼š
        1. å†å²APIï¼ˆä¼˜å…ˆï¼‰- ç«‹å³è·å–å®Œæ•´æ•°æ®
        2. WebSocketï¼ˆè¡¥å……ï¼‰- å®æ—¶æ•°æ®èšåˆ
        3. REST APIï¼ˆå¤‡æ´ï¼‰- æœ€ç»ˆä¿éšœ
        
        Args:
            symbol: äº¤æ˜“å¯¹
            timeframes: æ—¶é—´æ¡†æ¶åˆ—è¡¨
            limit: Kçº¿æ•°é‡
            
        Returns:
            æ—¶é—´æ¡†æ¶ â†’ DataFrame æ˜ å°„
        """
        from src.config import Config
        
        self._total_requests += 1
        data = {}
        
        # ğŸ”¥ v4.3.2+ï¼šWebSocket-onlyä¸¥æ ¼æ¨¡å¼
        if Config.WEBSOCKET_ONLY_KLINES:
            logger.debug(f"ğŸ”’ {symbol} WebSocket-onlyæ¨¡å¼ï¼šè·³è¿‡å†å²APIå’ŒRESTå¤‡æ´")
            
            # å”¯ä¸€æ•°æ®æºï¼šWebSocket
            if self.ws_monitor:
                ws_data = await self._get_websocket_data(symbol, timeframes, limit)
                data.update(ws_data)
            
            # éªŒè¯æ•°æ®å®Œæ•´æ€§ï¼ˆæ ‡è®°warming_upçŠ¶æ€ï¼‰
            for tf in timeframes:
                if tf not in data or data[tf] is None or len(data[tf]) == 0:
                    logger.debug(
                        f"â³ {symbol} {tf} æ•°æ®ä¸è¶³ï¼ˆwarming_upï¼‰ï¼Œ"
                        f"ç­‰å¾…WebSocketç´¯ç§¯æ•°æ®"
                    )
                    data[tf] = pd.DataFrame()
            
            return data
        
        # ä¼ ç»Ÿ3å±‚Fallbackæ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
        # Layer 1: å†å²APIæ‰¹é‡è·å–
        logger.debug(f"ğŸ”„ Layer 1: å°è¯•å†å²APIæ‰¹é‡è·å– {symbol}")
        hist_data = await self._get_historical_batch(symbol, timeframes, limit)
        data.update(hist_data)
        
        # Layer 2: WebSocketè¡¥å……ç¼ºå¤±æ•°æ®
        missing_tfs = [tf for tf in timeframes if tf not in data or data[tf] is None]
        if missing_tfs and self.ws_monitor:
            logger.debug(f"ğŸ”„ Layer 2: WebSocketè¡¥å…… {missing_tfs}")
            ws_data = await self._get_websocket_data(symbol, missing_tfs, limit)
            data.update(ws_data)
        
        # Layer 3: REST APIå¤‡æ´
        if not Config.DISABLE_REST_FALLBACK:
            still_missing = [
                tf for tf in timeframes 
                if tf not in data or data[tf] is None or len(data[tf]) < limit * 0.8
            ]
            if still_missing:
                logger.debug(f"ğŸ”„ Layer 3: RESTå¤‡æ´ {still_missing}")
                rest_data = await self._get_rest_data(symbol, still_missing, limit)
                data.update(rest_data)
        
        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        for tf in timeframes:
            if tf not in data or data[tf] is None or len(data[tf]) == 0:
                logger.warning(
                    f"âš ï¸  {symbol} {tf} æ•°æ®è·å–å¤±è´¥ï¼ˆæ‰€æœ‰å±‚çº§å¤±è´¥ï¼‰"
                )
                data[tf] = pd.DataFrame()
        
        return data
    
    async def _get_historical_batch(
        self,
        symbol: str,
        timeframes: List[str],
        limit: int
    ) -> Dict[str, pd.DataFrame]:
        """
        Layer 1: å†å²APIæ‰¹é‡è·å–ï¼ˆv3.19.2ç«‹å³å¯åŠ¨ç­–ç•¥ï¼‰
        
        ä¼˜åŠ¿ï¼š
        - å¹¶è¡Œè·å–3ä¸ªæ—¶é—´æ¡†æ¶
        - å®Œæ•´æ•°æ®ï¼ˆæ— éœ€å¢é‡ï¼‰
        - å¯åŠ¨æ—¶é—´ï¼š10å°æ—¶ â†’ 5åˆ†é’Ÿ
        """
        # å¹¶è¡Œè·å–æ‰€æœ‰æ—¶é—´æ¡†æ¶
        tasks = [
            self._get_historical_klines(symbol, tf, limit)
            for tf in timeframes
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        data = {}
        for tf, result in zip(timeframes, results):
            if isinstance(result, Exception):
                logger.debug(f"âš ï¸  å†å²APIè·å–å¤±è´¥ {symbol} {tf}: {result}")
                data[tf] = None
            elif result is not None and len(result) > 0:
                self._historical_api_hits += 1
                data[tf] = result
            else:
                data[tf] = None
        
        return data
    
    async def _get_historical_klines(
        self,
        symbol: str,
        timeframe: str,
        limit: int
    ) -> Optional[pd.DataFrame]:
        """
        è·å–å†å²Kçº¿æ•°æ®ï¼ˆå•ä¸ªæ—¶é—´æ¡†æ¶ï¼‰
        
        ä¼˜å…ˆä½¿ç”¨å†å²æ•°æ®APIï¼ˆv3.19.2æ–°å¢ï¼‰
        """
        # æ£€æŸ¥ç¼“å­˜
        cache_key = generate_cache_key('klines', symbol, timeframe, limit=limit)
        cached_data = self.cache.get(cache_key)
        
        if cached_data is not None:
            self._cache_hits += 1
            logger.debug(f"âœ… ç¼“å­˜å‘½ä¸­: {symbol} {timeframe}")
            return cached_data
        
        try:
            # è°ƒç”¨Binanceå®¢æˆ·ç«¯è·å–Kçº¿
            klines = await self.client.get_klines(
                symbol=symbol,
                interval=timeframe,
                limit=limit
            )
            
            if not klines:
                return None
            
            # è§£æä¸ºDataFrame
            df = self._parse_klines(klines)
            
            if len(df) > 0:
                # ç¼“å­˜æ•°æ®ï¼ˆTTL=300ç§’ï¼Œ5åˆ†é’Ÿï¼‰
                self.cache.set(cache_key, df, ttl=300)
                logger.debug(
                    f"âœ… å†å²APIè·å–æˆåŠŸ: {symbol} {timeframe} ({len(df)}è¡Œ)"
                )
                return df
            
            return None
            
        except Exception as e:
            logger.debug(f"âš ï¸  å†å²APIè·å–å¤±è´¥ {symbol} {timeframe}: {e}")
            return None
    
    async def _get_websocket_data(
        self,
        symbol: str,
        timeframes: List[str],
        limit: int
    ) -> Dict[str, pd.DataFrame]:
        """
        Layer 2: ä»WebSocketèšåˆæ•°æ®ï¼ˆv4.3.2+ å®Œæ•´å®ç°ï¼‰
        
        èšåˆé€»è¾‘ï¼š
        - ä»WebSocketç¼“å­˜è·å–1m Kçº¿
        - èšåˆç”Ÿæˆ5m/15m/1h Kçº¿
        - è¿”å›å¯ç”¨çš„æ—¶é—´æ¡†æ¶æ•°æ®
        
        é€‚ç”¨åœºæ™¯ï¼š
        - WebSocketå·²å¯ç”¨
        - éœ€è¦å®æ—¶æ•°æ®
        - å†å²APIä¸å¯ç”¨/ç¦ç”¨
        """
        if not self.ws_monitor:
            return {}
        
        # ä»WebSocketè·å–æ‰€æœ‰1m Kçº¿å†å²
        all_klines = self.ws_monitor.get_all_klines()
        # WebSocketç¼“å­˜ä½¿ç”¨å°å†™symbol
        klines_1m = all_klines.get(symbol.lower(), [])
        
        kline_count = len(klines_1m) if klines_1m else 0
        
        if kline_count < 5:
            # è¿5méƒ½æ— æ³•èšåˆï¼Œå®Œå…¨æ²¡æœ‰WebSocketæ•°æ®
            logger.debug(f"{symbol}: WebSocket 1m Kçº¿å¤ªå°‘ï¼ˆ{kline_count}<5ï¼‰ï¼Œæ— æ³•ä½¿ç”¨")
            return {}
        
        data = {}
        
        # é€æ—¶é—´æ¡†æ¶æ£€æŸ¥ï¼Œè¿”å›å¯ç”¨çš„éƒ¨åˆ†
        for tf in timeframes:
            try:
                if tf == "1m" and kline_count >= 1:
                    # 1mç›´æ¥ä½¿ç”¨
                    data[tf] = self._convert_ws_klines_to_df(klines_1m[-limit:])
                    self._websocket_hits += 1
                elif tf in ["5m", "15m", "1h"]:
                    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿæ•°æ®èšåˆ
                    aggregated = self._aggregate_ws_klines(klines_1m, tf)
                    if aggregated and len(aggregated) > 0:
                        data[tf] = self._convert_ws_klines_to_df(aggregated[-limit:])
                        self._websocket_hits += 1
                        logger.debug(
                            f"{symbol} {tf}: WebSocketèšåˆæˆåŠŸï¼ˆ{kline_count}æ ¹1m Kçº¿ï¼‰"
                        )
                    else:
                        # æ•°æ®ä¸è¶³
                        logger.debug(
                            f"{symbol} {tf}: WebSocketæ•°æ®ä¸è¶³ï¼ˆ{kline_count}æ ¹1m Kçº¿ï¼‰ï¼Œ"
                            f"éœ€è¦è‡³å°‘{60 if tf=='1h' else (15 if tf=='15m' else 5)}æ ¹"
                        )
                        data[tf] = pd.DataFrame()
                else:
                    # ä¸æ”¯æŒçš„æ—¶é—´æ¡†æ¶
                    data[tf] = pd.DataFrame()
                
            except Exception as e:
                logger.debug(f"âš ï¸  WebSocketè·å–å¤±è´¥ {symbol} {tf}: {e}")
                data[tf] = pd.DataFrame()
        
        return data
    
    async def _get_rest_data(
        self,
        symbol: str,
        timeframes: List[str],
        limit: int
    ) -> Dict[str, pd.DataFrame]:
        """
        Layer 3: REST APIå¤‡æ´
        
        æœ€ç»ˆä¿éšœï¼š
        - å½“å†å²APIå’ŒWebSocketéƒ½å¤±è´¥æ—¶
        - ç›´æ¥è°ƒç”¨Binance REST API
        """
        data = {}
        
        for tf in timeframes:
            try:
                # ä½¿ç”¨ä¸å†å²APIç›¸åŒçš„æ–¹æ³•ï¼ˆå¤‡æ´ï¼‰
                df = await self._get_historical_klines(symbol, tf, limit)
                
                if df is not None and len(df) > 0:
                    self._rest_api_hits += 1
                    data[tf] = df
                else:
                    data[tf] = None
                    
            except Exception as e:
                logger.error(f"âŒ RESTå¤‡æ´å¤±è´¥ {symbol} {tf}: {e}")
                data[tf] = None
        
        return data
    
    def _aggregate_ws_klines(self, klines_1m: List[Dict], target_interval: str) -> List[Dict]:
        """
        ä»1m Kçº¿èšåˆç”Ÿæˆé«˜æ—¶é—´æ¡†æ¶Kçº¿ï¼ˆv4.3.2+ WebSocket-onlyæ¨¡å¼ï¼‰
        
        ä½¿ç”¨æ—¶é—´å¯¹é½çš„èšåˆæ–¹å¼ï¼š
        - 5m: å¯¹é½åˆ°æ¯5åˆ†é’Ÿï¼ˆ00:00, 00:05, 00:10...ï¼‰
        - 15m: å¯¹é½åˆ°æ¯15åˆ†é’Ÿï¼ˆ00:00, 00:15, 00:30...ï¼‰
        - 1h: å¯¹é½åˆ°æ¯å°æ—¶ï¼ˆ00:00, 01:00, 02:00...ï¼‰
        
        Args:
            klines_1m: 1m Kçº¿åˆ—è¡¨ï¼ˆä»WebSocketè·å–ï¼‰
            target_interval: ç›®æ ‡æ—¶é—´æ¡†æ¶ï¼ˆ5m/15m/1hï¼‰
        
        Returns:
            èšåˆåçš„Kçº¿åˆ—è¡¨
        """
        interval_map = {
            "5m": 5 * 60 * 1000,
            "15m": 15 * 60 * 1000,
            "1h": 60 * 60 * 1000
        }
        
        interval_ms = interval_map.get(target_interval)
        if not interval_ms:
            return []
        
        minutes = interval_ms // (60 * 1000)
        
        if len(klines_1m) < minutes:
            return []
        
        # æŒ‰æ—¶é—´æˆ³åˆ†ç»„
        from collections import defaultdict
        grouped = defaultdict(list)
        
        for kline in klines_1m:
            timestamp = kline.get('timestamp') or kline.get('server_timestamp', 0)
            aligned_time = (timestamp // interval_ms) * interval_ms
            grouped[aligned_time].append(kline)
        
        # èšåˆæ¯ä¸ªæ—¶é—´ç»„
        aggregated = []
        for aligned_time in sorted(grouped.keys()):
            group = grouped[aligned_time]
            if len(group) > 0:
                aggregated.append({
                    'symbol': group[0].get('symbol', ''),
                    'timestamp': aligned_time,
                    'open': group[0].get('open', 0),
                    'high': max(k.get('high', 0) for k in group),
                    'low': min(k.get('low', float('inf')) for k in group),
                    'close': group[-1].get('close', 0),
                    'volume': sum(k.get('volume', 0) for k in group),
                    'quote_volume': sum(k.get('quote_volume', 0) for k in group),
                    'trades': sum(k.get('trades', 0) for k in group)
                })
        
        return aggregated
    
    def _convert_ws_klines_to_df(self, klines: List[Dict]) -> pd.DataFrame:
        """
        è½¬æ¢WebSocket Kçº¿æ•°æ®ä¸ºDataFrameï¼ˆv4.3.2+ï¼‰
        
        Args:
            klines: WebSocket Kçº¿æ•°æ®åˆ—è¡¨
        
        Returns:
            æ ‡å‡†åŒ–DataFrame
        """
        if not klines:
            return pd.DataFrame()
        
        try:
            df = pd.DataFrame(klines)
            
            # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
            required_fields = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
            if not all(field in df.columns for field in required_fields):
                logger.error(f"WebSocket Kçº¿ç¼ºå°‘å¿…è¦å­—æ®µ: {df.columns.tolist()}")
                return pd.DataFrame()
            
            # è½¬æ¢æ•°æ®ç±»å‹
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df['open'] = df['open'].astype(float)
            df['high'] = df['high'].astype(float)
            df['low'] = df['low'].astype(float)
            df['close'] = df['close'].astype(float)
            df['volume'] = df['volume'].astype(float)
            
            # è®¾ç½®ç´¢å¼•
            df.set_index('timestamp', inplace=True)
            
            return df[['open', 'high', 'low', 'close', 'volume']]
            
        except Exception as e:
            logger.error(f"âŒ WebSocket Kçº¿è½¬æ¢å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _parse_klines(self, klines: List) -> pd.DataFrame:
        """
        è§£æKçº¿æ•°æ®ä¸ºDataFrame
        
        ç»Ÿä¸€è§£æé€»è¾‘ï¼ˆæ›¿ä»£å¤šå¤„é‡å¤ï¼‰
        
        Args:
            klines: Binance Kçº¿æ•°æ®
            
        Returns:
            æ ‡å‡†åŒ–DataFrame
        """
        if not klines:
            return pd.DataFrame()
        
        try:
            df = pd.DataFrame(klines, columns=[
                'timestamp', 'open', 'high', 'low', 'close', 'volume',
                'close_time', 'quote_volume', 'trades', 'taker_buy_base',
                'taker_buy_quote', 'ignore'
            ])
            
            # è½¬æ¢æ•°æ®ç±»å‹
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df['open'] = df['open'].astype(float)
            df['high'] = df['high'].astype(float)
            df['low'] = df['low'].astype(float)
            df['close'] = df['close'].astype(float)
            df['volume'] = df['volume'].astype(float)
            
            # è®¾ç½®ç´¢å¼•
            df.set_index('timestamp', inplace=True)
            
            return df[['open', 'high', 'low', 'close', 'volume']]
            
        except Exception as e:
            logger.error(f"âŒ Kçº¿è§£æå¤±è´¥: {e}")
            return pd.DataFrame()
    
    async def batch_get_multi_timeframe_data(
        self,
        symbols: List[str],
        timeframes: List[str] = ['1h', '15m', '5m'],
        limit: int = 50
    ) -> Dict[str, Dict[str, pd.DataFrame]]:
        """
        æ‰¹é‡è·å–å¤šä¸ªsymbolsçš„å¤šæ—¶é—´æ¡†æ¶æ•°æ®ï¼ˆv3.20 Phase 3ä¼˜åŒ–ï¼‰
        
        æ€§èƒ½ä¼˜åŒ–ï¼š
        1. æ‰¹é‡å¹¶è¡Œè·å–ï¼ˆå‡å°‘ä¸²è¡Œç­‰å¾…æ—¶é—´ï¼‰
        2. æ™ºèƒ½ç¼“å­˜æ£€æŸ¥ï¼ˆé¿å…é‡å¤è¯·æ±‚ï¼‰
        3. ç»Ÿä¸€é”™è¯¯å¤„ç†
        
        é¢„æœŸæ”¶ç›Šï¼š
        - 530 symbolsæ•°æ®è·å–ï¼š53ç§’ â†’ 8-10ç§’ï¼ˆ5-6xåŠ é€Ÿï¼‰
        
        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            timeframes: æ—¶é—´æ¡†æ¶åˆ—è¡¨
            limit: Kçº¿æ•°é‡
            
        Returns:
            {symbol: {timeframe: DataFrame}}
            
        ç¤ºä¾‹ï¼š
            pipeline = UnifiedDataPipeline(client, ws_monitor)
            batch_data = await pipeline.batch_get_multi_timeframe_data(
                ['BTCUSDT', 'ETHUSDT'],
                ['1h', '15m', '5m']
            )
            btc_h1 = batch_data['BTCUSDT']['1h']
        """
        import time
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = []
        for symbol in symbols:
            task = self.get_multi_timeframe_data(symbol, timeframes, limit)
            tasks.append((symbol, task))
        
        # æ‰¹é‡å¹¶è¡Œæ‰§è¡Œ
        start_time = time.time()
        results = await asyncio.gather(
            *[t[1] for t in tasks],
            return_exceptions=True
        )
        elapsed = time.time() - start_time
        
        # ç»„è£…ç»“æœ
        batch_data = {}
        success_count = 0
        error_count = 0
        
        for (symbol, _), result in zip(tasks, results):
            if isinstance(result, Exception):
                logger.warning(f"âš ï¸  {symbol} æ•°æ®è·å–å¤±è´¥: {result}")
                batch_data[symbol] = {}
                error_count += 1
            else:
                batch_data[symbol] = result
                success_count += 1
        
        logger.info(
            f"âœ… æ‰¹é‡æ•°æ®è·å–å®Œæˆ: {len(symbols)}ä¸ªsymbols | "
            f"æˆåŠŸ{success_count} | å¤±è´¥{error_count} | "
            f"è€—æ—¶{elapsed:.2f}ç§’ | "
            f"å¹³å‡{elapsed/len(symbols)*1000:.1f}ms/symbol"
        )
        
        return batch_data
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç®¡é“ç»Ÿè®¡"""
        return {
            'total_requests': self._total_requests,
            'cache_hits': self._cache_hits,
            'cache_hit_rate': (
                self._cache_hits / self._total_requests
                if self._total_requests > 0
                else 0.0
            ),
            'historical_api_hits': self._historical_api_hits,
            'websocket_hits': self._websocket_hits,
            'rest_api_hits': self._rest_api_hits,
            'layer_distribution': {
                'layer1_historical': self._historical_api_hits,
                'layer2_websocket': self._websocket_hits,
                'layer3_rest': self._rest_api_hits
            }
        }
    
    def print_stats(self):
        """æ‰“å°ç®¡é“ç»Ÿè®¡"""
        stats = self.get_stats()
        logger.info(
            f"ğŸ“Š UnifiedDataPipeline ç»Ÿè®¡:\n"
            f"   ğŸ“¡ æ€»è¯·æ±‚æ¬¡æ•°: {stats['total_requests']}\n"
            f"   âœ… ç¼“å­˜å‘½ä¸­: {stats['cache_hits']} ({stats['cache_hit_rate']:.1%})\n"
            f"   ğŸ”„ Layer 1 (å†å²API): {stats['historical_api_hits']}\n"
            f"   ğŸ”„ Layer 2 (WebSocket): {stats['websocket_hits']}\n"
            f"   ğŸ”„ Layer 3 (REST): {stats['rest_api_hits']}"
        )
