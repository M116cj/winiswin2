"""
ç»Ÿä¸€æ•°æ®è·å–ç®¡é“ v3.20

èŒè´£ï¼šç»Ÿä¸€æ‰€æœ‰Kçº¿æ•°æ®è·å–é€»è¾‘ï¼ˆSingle Data Pipelineï¼‰

æ•´åˆï¼š
- src/services/data_service.py::get_klines() (æ–¹æ³•1)
- src/services/data_service.py::get_klines_incremental() (æ–¹æ³•2)
- src/services/data_service.py::get_historical_klines() (æ–¹æ³•3)
- src/services/data_service.py::_fetch_full_klines() (æ–¹æ³•4)
- src/clients/binance_client.py::get_klines() (æ–¹æ³•5)

æ ¸å¿ƒä¼˜åŠ¿ï¼š
1. 3å±‚Fallbackç­–ç•¥ï¼šå†å²API â†’ WebSocket â†’ REST
2. æ™ºèƒ½æ‰¹é‡è·å–ï¼šå‡å°‘HTTPè¯·æ±‚æ•°
3. è‡ªé€‚åº”ç¼“å­˜ï¼šåŸºäºæ³¢åŠ¨ç‡åŠ¨æ€TTL
4. å¢é‡æ›´æ–°ä¼˜åŒ–ï¼šåªè·å–ç¼ºå¤±æ•°æ®
5. ç»Ÿä¸€é”™è¯¯å¤„ç†ï¼šä¸€è‡´çš„é‡è¯•é€»è¾‘

æ€§èƒ½ä¼˜åŒ–ï¼š
- æ‰¹é‡è·å–ï¼š3ä¸ªæ—¶é—´æ¡†æ¶å¹¶è¡Œè·å–ï¼ˆå‡å°‘ç­‰å¾…æ—¶é—´ï¼‰
- å†å²APIä¼˜å…ˆï¼šv3.19.2ç«‹å³è·å–å®Œæ•´æ•°æ®ï¼ˆ10hrsâ†’5minå¯åŠ¨ï¼‰
- æ™ºèƒ½ç¼“å­˜ï¼šå‡å°‘30-40% APIè¯·æ±‚

é¢„æœŸæ”¶ç›Šï¼š
- æ•°æ®è·å–é€Ÿåº¦ï¼š79-159ç§’ â†’ 30-60ç§’ï¼ˆ2-3å€ï¼‰
- APIè¯·æ±‚å‡å°‘ï¼š30-40%
- ä»£ç é‡å¤ï¼š5ä¸ªæ–¹æ³• â†’ 2ä¸ªæ ¸å¿ƒæ–¹æ³•
"""

import logging
import asyncio
import pandas as pd
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta

from .intelligent_cache import IntelligentCache, generate_cache_key

logger = logging.getLogger(__name__)


class UnifiedDataPipeline:
    """
    ç»Ÿä¸€æ•°æ®è·å–ç®¡é“
    
    åŠŸèƒ½ï¼š
    1. 3å±‚Fallbackæ•°æ®è·å–ï¼ˆå†å²API â†’ WebSocket â†’ RESTï¼‰
    2. æ™ºèƒ½æ‰¹é‡è·å–ï¼ˆå‡å°‘HTTPè¯·æ±‚ï¼‰
    3. å¢é‡æ›´æ–°ä¼˜åŒ–
    4. è‡ªé€‚åº”ç¼“å­˜
    5. ç»Ÿä¸€é”™è¯¯å¤„ç†
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        pipeline = UnifiedDataPipeline(binance_client, websocket_monitor)
        
        # è·å–å¤šæ—¶é—´æ¡†æ¶æ•°æ®
        data = await pipeline.get_multi_timeframe_data(
            'BTCUSDT',
            timeframes=['1h', '15m', '5m']
        )
        
        # è®¿é—®æ•°æ®
        h1_data = data['1h']
        m15_data = data['15m']
    """
    
    def __init__(
        self,
        binance_client: Any,
        websocket_monitor: Optional[Any] = None,
        cache: Optional[IntelligentCache] = None
    ):
        """
        åˆå§‹åŒ–ç»Ÿä¸€æ•°æ®è·å–ç®¡é“
        
        Args:
            binance_client: Binanceå®¢æˆ·ç«¯å®ä¾‹
            websocket_monitor: WebSocketç›‘æ§å™¨ï¼ˆå¯é€‰ï¼‰
            cache: æ™ºèƒ½ç¼“å­˜å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        self.client = binance_client
        self.ws_monitor = websocket_monitor
        self.cache = cache or IntelligentCache(l1_max_size=5000)
        
        # ç»Ÿè®¡
        self._total_requests = 0
        self._cache_hits = 0
        self._historical_api_hits = 0
        self._websocket_hits = 0
        self._rest_api_hits = 0
        
        logger.info(
            "âœ… UnifiedDataPipeline åˆå§‹åŒ–å®Œæˆ\n"
            "   ğŸ¯ 3å±‚Fallback: å†å²API â†’ WebSocket â†’ REST\n"
            "   ğŸ’¾ æ™ºèƒ½ç¼“å­˜å·²å¯ç”¨\n"
            f"   ğŸ“¡ WebSocket: {'å¯ç”¨' if websocket_monitor else 'ç¦ç”¨'}"
        )
    
    async def get_multi_timeframe_data(
        self,
        symbol: str,
        timeframes: List[str] = ['1h', '15m', '5m'],
        limit: int = 50
    ) -> Dict[str, pd.DataFrame]:
        """
        è·å–å¤šæ—¶é—´æ¡†æ¶æ•°æ®ï¼ˆä¸»å…¥å£ï¼‰
        
        3å±‚Fallbackç­–ç•¥ï¼š
        1. å†å²APIï¼ˆä¼˜å…ˆï¼‰- ç«‹å³è·å–å®Œæ•´æ•°æ®
        2. WebSocketï¼ˆè¡¥å……ï¼‰- å®æ—¶æ•°æ®èšåˆ
        3. REST APIï¼ˆå¤‡æ´ï¼‰- æœ€ç»ˆä¿éšœ
        
        Args:
            symbol: äº¤æ˜“å¯¹
            timeframes: æ—¶é—´æ¡†æ¶åˆ—è¡¨
            limit: Kçº¿æ•°é‡
            
        Returns:
            æ—¶é—´æ¡†æ¶ â†’ DataFrame æ˜ å°„
        """
        self._total_requests += 1
        
        data = {}
        
        # Layer 1: å†å²APIæ‰¹é‡è·å–ï¼ˆv3.19.2ä¼˜å…ˆç­–ç•¥ï¼‰
        logger.debug(f"ğŸ”„ Layer 1: å°è¯•å†å²APIæ‰¹é‡è·å– {symbol}")
        hist_data = await self._get_historical_batch(symbol, timeframes, limit)
        data.update(hist_data)
        
        # Layer 2: WebSocketè¡¥å……ç¼ºå¤±æ•°æ®
        missing_tfs = [tf for tf in timeframes if tf not in data or data[tf] is None]
        if missing_tfs and self.ws_monitor:
            logger.debug(f"ğŸ”„ Layer 2: WebSocketè¡¥å…… {missing_tfs}")
            ws_data = await self._get_websocket_data(symbol, missing_tfs, limit)
            data.update(ws_data)
        
        # Layer 3: REST APIå¤‡æ´
        still_missing = [
            tf for tf in timeframes 
            if tf not in data or data[tf] is None or len(data[tf]) < limit * 0.8
        ]
        if still_missing:
            logger.debug(f"ğŸ”„ Layer 3: RESTå¤‡æ´ {still_missing}")
            rest_data = await self._get_rest_data(symbol, still_missing, limit)
            data.update(rest_data)
        
        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        for tf in timeframes:
            if tf not in data or data[tf] is None or len(data[tf]) == 0:
                logger.warning(
                    f"âš ï¸  {symbol} {tf} æ•°æ®è·å–å¤±è´¥ï¼ˆæ‰€æœ‰å±‚çº§å¤±è´¥ï¼‰"
                )
                data[tf] = pd.DataFrame()
        
        return data
    
    async def _get_historical_batch(
        self,
        symbol: str,
        timeframes: List[str],
        limit: int
    ) -> Dict[str, pd.DataFrame]:
        """
        Layer 1: å†å²APIæ‰¹é‡è·å–ï¼ˆv3.19.2ç«‹å³å¯åŠ¨ç­–ç•¥ï¼‰
        
        ä¼˜åŠ¿ï¼š
        - å¹¶è¡Œè·å–3ä¸ªæ—¶é—´æ¡†æ¶
        - å®Œæ•´æ•°æ®ï¼ˆæ— éœ€å¢é‡ï¼‰
        - å¯åŠ¨æ—¶é—´ï¼š10å°æ—¶ â†’ 5åˆ†é’Ÿ
        """
        # å¹¶è¡Œè·å–æ‰€æœ‰æ—¶é—´æ¡†æ¶
        tasks = [
            self._get_historical_klines(symbol, tf, limit)
            for tf in timeframes
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        data = {}
        for tf, result in zip(timeframes, results):
            if isinstance(result, Exception):
                logger.debug(f"âš ï¸  å†å²APIè·å–å¤±è´¥ {symbol} {tf}: {result}")
                data[tf] = None
            elif result is not None and len(result) > 0:
                self._historical_api_hits += 1
                data[tf] = result
            else:
                data[tf] = None
        
        return data
    
    async def _get_historical_klines(
        self,
        symbol: str,
        timeframe: str,
        limit: int
    ) -> Optional[pd.DataFrame]:
        """
        è·å–å†å²Kçº¿æ•°æ®ï¼ˆå•ä¸ªæ—¶é—´æ¡†æ¶ï¼‰
        
        ä¼˜å…ˆä½¿ç”¨å†å²æ•°æ®APIï¼ˆv3.19.2æ–°å¢ï¼‰
        """
        # æ£€æŸ¥ç¼“å­˜
        cache_key = generate_cache_key('klines', symbol, timeframe, limit=limit)
        cached_data = self.cache.get(cache_key)
        
        if cached_data is not None:
            self._cache_hits += 1
            logger.debug(f"âœ… ç¼“å­˜å‘½ä¸­: {symbol} {timeframe}")
            return cached_data
        
        try:
            # è°ƒç”¨Binanceå®¢æˆ·ç«¯è·å–Kçº¿
            klines = await self.client.get_klines(
                symbol=symbol,
                interval=timeframe,
                limit=limit
            )
            
            if not klines:
                return None
            
            # è§£æä¸ºDataFrame
            df = self._parse_klines(klines)
            
            if len(df) > 0:
                # ç¼“å­˜æ•°æ®ï¼ˆTTL=300ç§’ï¼Œ5åˆ†é’Ÿï¼‰
                self.cache.set(cache_key, df, ttl=300)
                logger.debug(
                    f"âœ… å†å²APIè·å–æˆåŠŸ: {symbol} {timeframe} ({len(df)}è¡Œ)"
                )
                return df
            
            return None
            
        except Exception as e:
            logger.debug(f"âš ï¸  å†å²APIè·å–å¤±è´¥ {symbol} {timeframe}: {e}")
            return None
    
    async def _get_websocket_data(
        self,
        symbol: str,
        timeframes: List[str],
        limit: int
    ) -> Dict[str, pd.DataFrame]:
        """
        Layer 2: ä»WebSocketèšåˆæ•°æ®
        
        é€‚ç”¨åœºæ™¯ï¼š
        - WebSocketå·²å¯ç”¨
        - éœ€è¦å®æ—¶æ•°æ®
        - å†å²APIä¸å¯ç”¨
        """
        if not self.ws_monitor:
            return {}
        
        data = {}
        
        for tf in timeframes:
            try:
                # ä»WebSocketè·å–èšåˆçš„Kçº¿æ•°æ®
                # TODO: å®ç°WebSocketæ•°æ®èšåˆé€»è¾‘
                # ws_klines = await self.ws_monitor.get_aggregated_klines(
                #     symbol, tf, limit
                # )
                
                # æš‚æ—¶è¿”å›ç©ºï¼ˆv3.21å®ç°ï¼‰
                data[tf] = None
                
            except Exception as e:
                logger.debug(f"âš ï¸  WebSocketè·å–å¤±è´¥ {symbol} {tf}: {e}")
                data[tf] = None
        
        return data
    
    async def _get_rest_data(
        self,
        symbol: str,
        timeframes: List[str],
        limit: int
    ) -> Dict[str, pd.DataFrame]:
        """
        Layer 3: REST APIå¤‡æ´
        
        æœ€ç»ˆä¿éšœï¼š
        - å½“å†å²APIå’ŒWebSocketéƒ½å¤±è´¥æ—¶
        - ç›´æ¥è°ƒç”¨Binance REST API
        """
        data = {}
        
        for tf in timeframes:
            try:
                # ä½¿ç”¨ä¸å†å²APIç›¸åŒçš„æ–¹æ³•ï¼ˆå¤‡æ´ï¼‰
                df = await self._get_historical_klines(symbol, tf, limit)
                
                if df is not None and len(df) > 0:
                    self._rest_api_hits += 1
                    data[tf] = df
                else:
                    data[tf] = None
                    
            except Exception as e:
                logger.error(f"âŒ RESTå¤‡æ´å¤±è´¥ {symbol} {tf}: {e}")
                data[tf] = None
        
        return data
    
    def _parse_klines(self, klines: List) -> pd.DataFrame:
        """
        è§£æKçº¿æ•°æ®ä¸ºDataFrame
        
        ç»Ÿä¸€è§£æé€»è¾‘ï¼ˆæ›¿ä»£å¤šå¤„é‡å¤ï¼‰
        
        Args:
            klines: Binance Kçº¿æ•°æ®
            
        Returns:
            æ ‡å‡†åŒ–DataFrame
        """
        if not klines:
            return pd.DataFrame()
        
        try:
            df = pd.DataFrame(klines, columns=[
                'timestamp', 'open', 'high', 'low', 'close', 'volume',
                'close_time', 'quote_volume', 'trades', 'taker_buy_base',
                'taker_buy_quote', 'ignore'
            ])
            
            # è½¬æ¢æ•°æ®ç±»å‹
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df['open'] = df['open'].astype(float)
            df['high'] = df['high'].astype(float)
            df['low'] = df['low'].astype(float)
            df['close'] = df['close'].astype(float)
            df['volume'] = df['volume'].astype(float)
            
            # è®¾ç½®ç´¢å¼•
            df.set_index('timestamp', inplace=True)
            
            return df[['open', 'high', 'low', 'close', 'volume']]
            
        except Exception as e:
            logger.error(f"âŒ Kçº¿è§£æå¤±è´¥: {e}")
            return pd.DataFrame()
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç®¡é“ç»Ÿè®¡"""
        return {
            'total_requests': self._total_requests,
            'cache_hits': self._cache_hits,
            'cache_hit_rate': (
                self._cache_hits / self._total_requests
                if self._total_requests > 0
                else 0.0
            ),
            'historical_api_hits': self._historical_api_hits,
            'websocket_hits': self._websocket_hits,
            'rest_api_hits': self._rest_api_hits,
            'layer_distribution': {
                'layer1_historical': self._historical_api_hits,
                'layer2_websocket': self._websocket_hits,
                'layer3_rest': self._rest_api_hits
            }
        }
    
    def print_stats(self):
        """æ‰“å°ç®¡é“ç»Ÿè®¡"""
        stats = self.get_stats()
        logger.info(
            f"ğŸ“Š UnifiedDataPipeline ç»Ÿè®¡:\n"
            f"   ğŸ“¡ æ€»è¯·æ±‚æ¬¡æ•°: {stats['total_requests']}\n"
            f"   âœ… ç¼“å­˜å‘½ä¸­: {stats['cache_hits']} ({stats['cache_hit_rate']:.1%})\n"
            f"   ğŸ”„ Layer 1 (å†å²API): {stats['historical_api_hits']}\n"
            f"   ğŸ”„ Layer 2 (WebSocket): {stats['websocket_hits']}\n"
            f"   ğŸ”„ Layer 3 (REST): {stats['rest_api_hits']}"
        )
