"""
v3.17+ æ¨¡å‹è‡ªå‹•åˆå§‹åŒ–ç³»çµ±
éƒ¨ç½²åˆ° Railway å¾Œç«‹å³è¨“ç·´ï¼Œç„¡éœ€æ‰‹å‹•å¹²é 

v4.0 Feature Unification:
- ä½¿ç”¨ç»Ÿä¸€çš„12ä¸ªICT/SMCç‰¹å¾ï¼ˆä¸é¢„æµ‹ä¿æŒä¸€è‡´ï¼‰
- è®­ç»ƒå’Œæ¨ç†ä½¿ç”¨ç›¸åŒçš„feature_schema
"""

import os
import asyncio
from src.utils.logger_factory import get_logger
from pathlib import Path
from typing import Optional, Dict, Any, List
from datetime import datetime, timedelta
import json
from src.core.unified_config_manager import config_manager as config
from src.ml.feature_schema import (
    CANONICAL_FEATURE_NAMES,
    extract_canonical_features,
    features_to_vector,
    FEATURE_DEFAULTS
)

logger = get_logger(__name__)


class ModelInitializer:
    """
    æ¨¡å‹è‡ªå‹•åˆå§‹åŒ–å™¨ï¼ˆv3.17+ï¼‰
    
    è·è²¬ï¼š
    1. æª¢æ¸¬æ¨¡å‹æ˜¯å¦å­˜åœ¨
    2. è‹¥ç„¡æ¨¡å‹ï¼Œè‡ªå‹•æ”¶é›†è¨“ç·´æ•¸æ“š
    3. ä½¿ç”¨ä¸­æ€§åƒæ•¸è¨“ç·´åˆå§‹æ¨¡å‹
    4. å‰µå»º initialized.flag æ¨™è¨˜
    5. é›¶é…ç½®å•Ÿå‹•ï¼Œç„¡äººå·¥å¹²é 
    """
    
    def __init__(
        self,
        binance_client=None,
        trade_recorder=None,
        config_profile=None,
        model_evaluator=None
    ):
        """
        åˆå§‹åŒ–æ¨¡å‹åˆå§‹åŒ–å™¨
        
        Args:
            binance_client: BinanceClient å¯¦ä¾‹ï¼ˆå¯é¸ï¼‰
            trade_recorder: TradeRecorder å¯¦ä¾‹ï¼ˆå¯é¸ï¼‰
            config_profile: ConfigProfile å¯¦ä¾‹ï¼ˆå¯é¸ï¼‰
            model_evaluator: ModelEvaluator å¯¦ä¾‹ï¼ˆå¯é¸ï¼Œv3.17.10+ï¼‰
        """
        self.binance = binance_client
        self.trade_recorder = trade_recorder
        self.config = config_profile
        self.model_evaluator = model_evaluator  # ğŸ”¥ v3.17.10+
        
        # æ¨¡å‹ç›®éŒ„
        self.model_dir = Path("models")
        self.model_dir.mkdir(exist_ok=True)
        
        self.flag_file = self.model_dir / "initialized.flag"
        self.model_file = self.model_dir / "xgboost_model.json"
        
        # ğŸ”¥ v4.1+ å„ªåŒ–ç´š XGBoost åƒæ•¸ï¼ˆé™ä½éæ“¬åˆé¢¨éšªï¼‰
        self.training_params = {
            # ğŸŒ± æ¨¹çµæ§‹ï¼ˆæ§åˆ¶è¤‡é›œåº¦ï¼‰- OPTIMIZED
            'n_estimators': int(os.getenv("XGBOOST_N_ESTIMATORS", "30")),        # æ¨¹æ•¸é‡ï¼š100â†’30 (-70%)
            'max_depth': int(os.getenv("XGBOOST_MAX_DEPTH", "3")),               # æ¨¹æ·±åº¦ï¼š6â†’3 (-50%)
            'min_child_weight': int(os.getenv("XGBOOST_MIN_CHILD_WEIGHT", "50")), # è‘‰ç¯€é»æœ€å°æ¨£æœ¬ï¼š10â†’50 (5xï¼Œå…¼å®¹200æ¨£æœ¬)
            
            # âš–ï¸ æ­£å‰‡åŒ–ï¼ˆæå‡æ³›åŒ–ï¼‰- ENHANCED
            'gamma': float(os.getenv("XGBOOST_GAMMA", "0.2")),                   # åˆ†è£‚æœ€å°æå¤±ï¼š0.1â†’0.2
            'subsample': float(os.getenv("XGBOOST_SUBSAMPLE", "0.6")),           # è¨“ç·´æ¨£æœ¬æ¡æ¨£ï¼š0.8â†’0.6
            'colsample_bytree': float(os.getenv("XGBOOST_COLSAMPLE", "0.6")),    # ç‰¹å¾µæ¡æ¨£ï¼š0.8â†’0.6
            
            # ğŸš€ å­¸ç¿’ç‡ï¼ˆç©©å®šæ”¶æ–‚ï¼‰- MORE STABLE
            'learning_rate': float(os.getenv("XGBOOST_LEARNING_RATE", "0.05")),  # å­¸ç¿’æ­¥é•·ï¼š0.1â†’0.05
            
            # ğŸ¯ ç›®æ¨™å‡½æ•¸ï¼ˆäºŒåˆ†é¡ï¼‰
            'objective': 'binary:logistic',     # é‚è¼¯è¿´æ­¸æå¤±
            'eval_metric': 'logloss',           # è©•ä¼°æŒ‡æ¨™ï¼šå°æ¦‚ç‡é æ¸¬æ›´æ•æ„Ÿ
            
            # ğŸ§  å…¶ä»–é…ç½®
            'random_state': 42,                 # å¯é‡ç¾æ€§
            'n_jobs': -1,                       # å¤šæ ¸å¿ƒåŠ é€Ÿ
            'verbosity': 0,                     # éœé»˜æ¨¡å¼ï¼ˆé©åˆç”Ÿç”¢ï¼‰
            
            # è¨“ç·´æ•¸æ“šé…ç½®
            'min_samples': int(os.getenv("INITIAL_TRAINING_SAMPLES", "200")),
            'lookback_days': int(os.getenv("INITIAL_TRAINING_LOOKBACK_DAYS", "30")),
        }
        
        # âœ… STEP 1 VALIDATION
        logger.info("âœ… XGBooståƒæ•¸å·²å„ªåŒ–ï¼ˆv4.1 ä¿®æ­£ç‰ˆï¼‰:")
        logger.info(f"   æ¨¹æ•¸é‡: 100 â†’ {self.training_params['n_estimators']}")
        logger.info(f"   æ¨¹æ·±åº¦: 6 â†’ {self.training_params['max_depth']}")
        logger.info(f"   æœ€å°å­ç¯€é»æ¬Šé‡: 10 â†’ {self.training_params['min_child_weight']} (å…¼å®¹200æ¨£æœ¬)")
        
        logger.info("=" * 60)
        logger.info("âœ… æ¨¡å‹è‡ªå‹•åˆå§‹åŒ–å™¨å·²å‰µå»ºï¼ˆv3.18.6+ç”Ÿç”¢ç´šï¼‰")
        logger.info(f"   ğŸ“ æ¨¡å‹ç›®éŒ„: {self.model_dir}")
        logger.info(f"   ğŸ¯ è¨“ç·´åƒæ•¸: n_estimators={self.training_params['n_estimators']}, "
                   f"max_depth={self.training_params['max_depth']}, "
                   f"min_child_weight={self.training_params['min_child_weight']}, "
                   f"gamma={self.training_params['gamma']}")
        logger.info(f"   ğŸ¯ ç›®æ¨™å‡½æ•¸: {self.training_params['objective']}, "
                   f"è©•ä¼°æŒ‡æ¨™: {self.training_params['eval_metric']}")
        logger.info("=" * 60)
    
    async def check_and_initialize(self) -> bool:
        """
        æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²åˆå§‹åŒ–ï¼Œè‹¥ç„¡å‰‡è‡ªå‹•è¨“ç·´
        
        Returns:
            æ˜¯å¦å·²åˆå§‹åŒ–ï¼ˆTrue=å·²å­˜åœ¨æˆ–è¨“ç·´æˆåŠŸï¼ŒFalse=è¨“ç·´å¤±æ•—ï¼‰
        """
        # ğŸ”’ v3.18.7+ï¼šæª¢æŸ¥æ¨¡å‹è¨“ç·´é–å®šé–‹é—œ
        if getattr(Config, 'DISABLE_MODEL_TRAINING', False):
            logger.info("ğŸ”’ æ¨¡å‹è¨“ç·´å·²é–å®šï¼ˆDISABLE_MODEL_TRAINING=Trueï¼‰")
            logger.info("   âœ… ç³»çµ±å°‡ä½¿ç”¨ç¾æœ‰æ¨¡å‹ï¼Œä¸é€²è¡Œåˆå§‹è¨“ç·´æˆ–é‡è¨“ç·´")
            
            # æª¢æŸ¥æ˜¯å¦å·²æœ‰æ¨¡å‹æ–‡ä»¶
            if self.model_file.exists():
                logger.info(f"   âœ… æª¢æ¸¬åˆ°ç¾æœ‰æ¨¡å‹: {self.model_file}")
                # å³ä½¿æ²’æœ‰flagæ–‡ä»¶ï¼Œä¹Ÿå‰µå»ºä¸€å€‹ï¼ˆé˜²æ­¢ä¸‹æ¬¡æª¢æŸ¥ï¼‰
                if not self.flag_file.exists():
                    self._create_flag_file()
                return True
            else:
                logger.warning(f"   âš ï¸ æœªæª¢æ¸¬åˆ°æ¨¡å‹æ–‡ä»¶: {self.model_file}")
                logger.warning("   âš ï¸ è«‹ç¢ºä¿å·²æœ‰é è¨“ç·´æ¨¡å‹ï¼Œæˆ–è‡¨æ™‚é—œé–‰DISABLE_MODEL_TRAINING")
                return False
        
        # æª¢æŸ¥æ¨™è¨˜æ–‡ä»¶
        if self.flag_file.exists():
            logger.info("âœ… æ¨¡å‹å·²åˆå§‹åŒ–ï¼ˆæª¢æ¸¬åˆ° initialized.flagï¼‰")
            return True
        
        logger.warning("âš ï¸ æœªæª¢æ¸¬åˆ°åˆå§‹åŒ–æ¨¡å‹ï¼Œé–‹å§‹è‡ªå‹•è¨“ç·´...")
        
        # åŸ·è¡Œåˆå§‹è¨“ç·´
        success = await self._initial_training()
        
        if success:
            # å‰µå»ºæ¨™è¨˜æ–‡ä»¶
            self._create_flag_file()
            logger.info("ğŸ‰ æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ç³»çµ±å·²å°±ç·’")
            return True
        else:
            logger.error("âŒ æ¨¡å‹åˆå§‹åŒ–å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ—¥èªŒ")
            return False
    
    async def _initial_training(self) -> bool:
        """
        åŸ·è¡Œåˆå§‹è¨“ç·´
        
        Returns:
            è¨“ç·´æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("ğŸš€ é–‹å§‹æ”¶é›†è¨“ç·´æ•¸æ“š...")
            
            # 1. æ”¶é›†é«˜å“è³ªäº¤æ˜“æ•¸æ“š
            training_data = await self._collect_training_data()
            
            if not training_data or len(training_data) < self.training_params['min_samples']:
                logger.error(
                    f"âŒ è¨“ç·´æ•¸æ“šä¸è¶³: {len(training_data) if training_data else 0} "
                    f"< {self.training_params['min_samples']}"
                )
                return False
            
            logger.info(f"âœ… æ”¶é›†åˆ° {len(training_data)} ç­†è¨“ç·´æ•¸æ“š")
            
            # 2. è¨“ç·´åˆå§‹æ¨¡å‹
            logger.info("ğŸ§  é–‹å§‹è¨“ç·´ XGBoost æ¨¡å‹...")
            model_success = await self._train_xgboost_model(training_data)
            
            if not model_success:
                logger.error("âŒ XGBoost æ¨¡å‹è¨“ç·´å¤±æ•—")
                return False
            
            logger.info("âœ… XGBoost æ¨¡å‹è¨“ç·´å®Œæˆ")
            
            # 3. åˆå§‹åŒ–ç‰¹å¾µæ¬Šé‡ï¼ˆç„¡å…ˆé©—åå¥½ï¼‰
            self._initialize_feature_weights()
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹è¨“ç·´å¤±æ•—: {e}", exc_info=True)
            return False
    
    async def _collect_training_data(self) -> List[Dict[str, Any]]:
        """
        ğŸ”¥ v4.6.0 Phase 2: æ”¶é›†è¨“ç·´æ•¸æ“šï¼ˆPostgreSQLå”¯ä¸€æ•¸æ“šæºï¼‰
        
        ç­–ç•¥ï¼š
        1. ğŸ”¥ å¾ PostgreSQL åŠ è¼‰çœŸå¯¦äº¤æ˜“æ•¸æ“šï¼ˆ12å€‹ICT/SMCç‰¹å¾µï¼‰
        2. è‹¥æ•¸æ“šä¸è¶³ï¼Œä½¿ç”¨å¸‚å ´æ•¸æ“šç”Ÿæˆåˆæˆæ¨£æœ¬
        
        Returns:
            è¨“ç·´æ•¸æ“šåˆ—è¡¨
        """
        training_data = []
        
        # ğŸ”¥ v4.6.0 Phase 2: å¾ PostgreSQL åŠ è¼‰çœŸå¯¦äº¤æ˜“æ•¸æ“šï¼ˆå·²ç§»é™¤JSONL fallbackï¼‰
        logger.info("ğŸ“Š åŠ è¼‰çœŸå¯¦äº¤æ˜“æ•¸æ“šï¼ˆPostgreSQLå”¯ä¸€æ•¸æ“šæºï¼‰...")
        real_trades = await self._load_training_data_from_trades()
        
        if real_trades:
            logger.info(f"âœ… åŠ è¼‰ {len(real_trades)} ç­†çœŸå¯¦äº¤æ˜“æ•¸æ“šï¼ˆ12ç‰¹å¾µï¼‰")
            training_data.extend(real_trades)
        else:
            logger.warning("âš ï¸ PostgreSQL ç„¡æ•¸æ“š")
        
        # ç­–ç•¥ 2: è‹¥æ•¸æ“šä¸è¶³ï¼Œç”Ÿæˆåˆæˆæ¨£æœ¬
        if len(training_data) < self.training_params['min_samples']:
            needed = self.training_params['min_samples'] - len(training_data)
            logger.info(f"ğŸ“Š æ•¸æ“šä¸è¶³ï¼Œå¾å¸‚å ´æ•¸æ“šç”Ÿæˆ {needed} å€‹åˆæˆæ¨£æœ¬...")
            synthetic_samples = await self._generate_synthetic_samples(target_count=needed)
            training_data.extend(synthetic_samples)
        
        logger.info(f"âœ… ç¸½è¨ˆæ”¶é›† {len(training_data)} ç­†è¨“ç·´æ•¸æ“š")
        logger.info(f"   çœŸå¯¦äº¤æ˜“: {len(real_trades)}")
        logger.info(f"   åˆæˆæ¨£æœ¬: {len(training_data) - len(real_trades)}")
        
        return training_data
    
    async def _get_historical_trades(self) -> List[Dict[str, Any]]:
        """ç²å–æ­·å²äº¤æ˜“è¨˜éŒ„"""
        try:
            if self.trade_recorder is None:
                return []
            
            if hasattr(self.trade_recorder, 'get_all_trades'):
                return await self.trade_recorder.get_all_trades()
            elif hasattr(self.trade_recorder, 'get_closed_trades'):
                return await self.trade_recorder.get_closed_trades()
            else:
                return []
        except Exception as e:
            logger.error(f"âŒ ç²å–æ­·å²äº¤æ˜“å¤±æ•—: {e}")
            return []
    
    async def _generate_synthetic_samples(self, target_count: int) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆä½¿ç”¨12ä¸ªICT/SMCç‰¹å¾çš„åˆæˆæ ·æœ¬ï¼ˆv4.5.0å…¼å®¹ï¼‰
        
        v4.5.0 P0ä¿®å¤ï¼šé‡æ–°å¯ç”¨åˆæˆæ ·æœ¬ç”Ÿæˆï¼Œè§£å†³æ–°éƒ¨ç½²ç¯å¢ƒæ— æ•°æ®é—®é¢˜
        ä½¿ç”¨12ä¸ªICT/SMCç‰¹å¾ï¼ˆä¸é¢„æµ‹ä¸€è‡´ï¼‰
        
        Args:
            target_count: ç›®æ¨™æ¨£æœ¬æ•¸é‡
            
        Returns:
            åˆæˆæ ·æœ¬åˆ—è¡¨ï¼ˆæ¯ä¸ªæ ·æœ¬åŒ…å«12ä¸ªICT/SMCç‰¹å¾ï¼‰
        """
        import random
        
        logger.info(f"âš™ï¸  ç”Ÿæˆ{target_count}ä¸ªåˆæˆæ ·æœ¬ï¼ˆä½¿ç”¨12ä¸ªICT/SMCç‰¹å¾ï¼‰")
        
        samples = []
        for i in range(target_count):
            # éšæœºç”ŸæˆWIN/LOSSæ ‡ç­¾
            label = random.choice([0, 1])
            
            # ç”Ÿæˆ12ä¸ªICT/SMCç‰¹å¾çš„åˆç†éšæœºå€¼
            features = {
                # åŸºç¡€ç‰¹å¾ï¼ˆ8ä¸ªï¼‰
                'market_structure': random.choice([-1, 0, 1]),  # çœ‹è·Œ/ä¸­æ€§/çœ‹æ¶¨
                'order_blocks_count': random.randint(0, 5),  # 0-5ä¸ªè®¢å•å—
                'institutional_candle': random.choice([0, 1]),  # æ˜¯å¦æœºæ„Kçº¿
                'liquidity_grab': random.choice([0, 1]),  # æ˜¯å¦æµåŠ¨æ€§æŠ“å–
                'order_flow': random.uniform(-1.0, 1.0),  # è®¢å•æµ -1åˆ°1
                'fvg_count': random.randint(0, 3),  # 0-3ä¸ªFVG
                'trend_alignment_enhanced': random.uniform(0.0, 1.0),  # è¶‹åŠ¿å¯¹é½åº¦
                'swing_high_distance': random.uniform(0.0, 1.0),  # æ‘†åŠ¨é«˜ç‚¹è·ç¦»
                
                # åˆæˆç‰¹å¾ï¼ˆ4ä¸ªï¼‰
                'structure_integrity': random.uniform(0.0, 1.0),  # ç»“æ„å®Œæ•´æ€§
                'institutional_participation': random.uniform(0.0, 1.0),  # æœºæ„å‚ä¸åº¦
                'timeframe_convergence': random.uniform(0.0, 1.0),  # æ—¶é—´æ¡†æ¶æ”¶æ•›åº¦
                'liquidity_context': random.uniform(0.0, 1.0),  # æµåŠ¨æ€§æƒ…å¢ƒ
            }
            
            # éªŒè¯ç‰¹å¾å®Œæ•´æ€§
            assert all(feat in features for feat in CANONICAL_FEATURE_NAMES), \
                f"åˆæˆæ ·æœ¬ç¼ºå°‘å¿…éœ€ç‰¹å¾"
            
            samples.append({
                'label': label,
                'features': features,
                'pnl': random.uniform(-5.0, 5.0) if label == 1 else random.uniform(-10.0, 0.0)
            })
        
        logger.info(f"âœ… æˆåŠŸç”Ÿæˆ{len(samples)}ä¸ªåˆæˆæ ·æœ¬ï¼ˆç‰¹å¾éªŒè¯é€šè¿‡ï¼‰")
        return samples
    
    async def _load_training_data_from_trades(self) -> List[Dict]:
        """
        ğŸ”¥ v4.6.0 Phase 2: å¾ PostgreSQL åŠ è¼‰çœŸå¯¦äº¤æ˜“æ•¸æ“šï¼ˆå”¯ä¸€æ•¸æ“šæºï¼‰
        
        ä½¿ç”¨ç»Ÿä¸€çš„12ä¸ªICT/SMCç‰¹å¾ï¼ˆä¸é¢„æµ‹ä¸€è‡´ï¼‰ï¼Œå¹¶éªŒè¯ç‰¹å¾å®Œæ•´æ€§
        
        Returns:
            è¨“ç·´æ•¸æ“šåˆ—è¡¨ï¼ˆæ¯å€‹å…ƒç´ åŒ…å«12å€‹æ¨™æº–ç‰¹å¾µ + labelï¼‰
        """
        training_data = []
        
        # ğŸ”¥ v4.6.0 Phase 2: PostgreSQLå”¯ä¸€æ•°æ®æºï¼ˆå·²ç§»é™¤trades.jsonl fallbackï¼‰
        if self.trade_recorder and hasattr(self.trade_recorder, 'data_service'):
            try:
                # ä½¿ç”¨ get_trade_history è·å–æ‰€æœ‰å·²å…³é—­äº¤æ˜“ï¼ˆç”¨äºè®­ç»ƒï¼‰
                trades = await self.trade_recorder.data_service.get_trade_history(
                    status='CLOSED',
                    limit=10000  # è¶³å¤Ÿå¤§çš„é™åˆ¶
                )
                
                for trade in trades:
                    # Phase 3: asyncpgè¿”å›dictï¼Œç›´æ¥è®¿é—®å­—æ®µ
                    if isinstance(trade, dict):
                        # æå–å…ƒæ•°æ®ä¸­çš„ç‰¹å¾
                        metadata = trade.get('metadata', {})
                        features_dict = metadata.get('features', {}) if isinstance(metadata, dict) else {}
                        
                        # v4.0: å³ä½¿ç¼ºå°‘featuresï¼Œä¹Ÿä½¿ç”¨é»˜è®¤å€¼ï¼ˆdefensiveï¼‰
                        if not features_dict:
                            logger.debug(f"âš ï¸ Trade {trade.get('id')} ç¼ºå°‘featuresï¼Œä½¿ç”¨é»˜è®¤å€¼")
                            features_dict = {}
                        
                        # æå–12ä¸ªæ ‡å‡†ç‰¹å¾ï¼ˆç¼ºå¤±å­—æ®µä½¿ç”¨FEATURE_DEFAULTSï¼‰
                        canonical = extract_canonical_features(features_dict)
                        
                        # ç¡®å®šæ ‡ç­¾ï¼ˆwon: True=1, False=0ï¼‰
                        label = 1 if trade.get('won') is True else 0
                        
                        training_data.append({
                            'features': canonical,
                            'label': label,
                            'pnl': float(trade.get('pnl', 0))
                        })
                    elif hasattr(trade, 'get'):
                        # å‘åå…¼å®¹ï¼šå¤„ç†_row_to_dictè¿”å›çš„æ•°æ®
                        raw_data = trade.get('raw_data')
                        logger.debug(f"âš ï¸ æ”¶åˆ°raw_dataæ ¼å¼ï¼Œå¯èƒ½éœ€è¦æ›´æ–°_row_to_dictå®ç°")
                
                if training_data:
                    logger.info(f"âœ… å¾ PostgreSQL åŠ è¼‰ {len(training_data)} ç­†è¨“ç·´æ•¸æ“šï¼ˆ12ç‰¹å¾µï¼‰")
                else:
                    logger.warning("âš ï¸ PostgreSQL ç„¡å¯ç”¨è¨“ç·´æ•¸æ“š")
                
            except Exception as e:
                logger.error(f"âŒ å¾ PostgreSQL åŠ è¼‰è¨“ç·´æ•¸æ“šå¤±æ•—: {e}", exc_info=True)
        else:
            logger.warning("âš ï¸ TradeRecorderæˆ–DataServiceæœªé…ç½®ï¼Œç„¡æ³•åŠ è¼‰è¨“ç·´æ•¸æ“š")
        
        # ğŸ”¥ v4.5.0 P1: SchemaéªŒè¯ - è¿‡æ»¤ä¸å…¼å®¹çš„æ—§æ•°æ®
        return self._validate_feature_schema(training_data)
    
    def _validate_feature_schema(self, training_data: List[Dict]) -> List[Dict]:
        """
        ğŸ”¥ v4.5.0 P1ä¿®å¤: éªŒè¯è®­ç»ƒæ•°æ®çš„ç‰¹å¾schema
        
        è¿‡æ»¤ä¸åŒ…å«æ‰€æœ‰12ä¸ªICT/SMCç‰¹å¾çš„æ•°æ®ï¼ˆé˜²æ­¢è®­ç»ƒå¤±è´¥ï¼‰
        
        Args:
            training_data: åŸå§‹è®­ç»ƒæ•°æ®
            
        Returns:
            ç»è¿‡schemaéªŒè¯çš„è®­ç»ƒæ•°æ®
        """
        if not training_data:
            return training_data
        
        valid_data = []
        invalid_count = 0
        
        for trade in training_data:
            features = trade.get('features', {})
            
            # éªŒè¯æ˜¯å¦åŒ…å«æ‰€æœ‰12ä¸ªICT/SMCç‰¹å¾
            missing_features = [f for f in CANONICAL_FEATURE_NAMES if f not in features]
            
            if not missing_features:
                # æ‰€æœ‰ç‰¹å¾éƒ½å­˜åœ¨
                valid_data.append(trade)
            else:
                # ç¼ºå°‘ç‰¹å¾ï¼Œè·³è¿‡æ­¤äº¤æ˜“
                invalid_count += 1
                if invalid_count <= 3:  # åªè®°å½•å‰3ä¸ªè­¦å‘Šï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                    logger.warning(
                        f"âš ï¸ è·³è¿‡ä¸å…¼å®¹äº¤æ˜“æ•°æ®ï¼ˆç¼ºå°‘ç‰¹å¾: {missing_features[:3]}...ï¼‰"
                    )
        
        if invalid_count > 0:
            logger.info(
                f"ğŸ“Š ç‰¹å¾schemaéªŒè¯: {len(valid_data)}æ¡æœ‰æ•ˆ, "
                f"{invalid_count}æ¡æ— æ•ˆï¼ˆå·²è¿‡æ»¤ï¼‰"
            )
        else:
            logger.info(f"âœ… ç‰¹å¾schemaéªŒè¯: {len(valid_data)}æ¡æ•°æ®å…¨éƒ¨æœ‰æ•ˆ")
        
        return valid_data
    
    async def _train_xgboost_model(self, training_data: List[Dict]) -> bool:
        """
        ğŸ”¥ v4.0 Feature Unification: è¨“ç·´ XGBoost æ¨¡å‹ï¼ˆä½¿ç”¨12å€‹ICT/SMCç‰¹å¾µï¼‰
        
        Args:
            training_data: è¨“ç·´æ•¸æ“šï¼ˆåŒ…å«12å€‹æ¨™æº–ç‰¹å¾µ + labelï¼‰
            
        Returns:
            è¨“ç·´æ˜¯å¦æˆåŠŸ
        """
        try:
            import xgboost as xgb
            import numpy as np
            
            # ğŸ”¥ v4.0: æå–12å€‹æ¨™æº–ç‰¹å¾µ
            X = []
            y = []
            
            for trade in training_data:
                # æå–12å€‹ç‰¹å¾µå‘é‡
                features_dict = trade.get('features', {})
                features_vector = features_to_vector(features_dict)
                
                # æå–æ¨™ç±¤
                label = int(trade.get('label', 0))
                
                X.append(features_vector)
                y.append(label)
            
            if len(X) < 10:
                logger.error(f"âŒ ç‰¹å¾µæ•¸æ“šä¸è¶³: {len(X)} < 10")
                return False
            
            X = np.array(X)
            y = np.array(y)
            
            logger.info(f"ğŸ“Š è¨“ç·´æ•¸æ“š: X.shape={X.shape}, y.shape={y.shape}")
            logger.info(f"   âœ… ä½¿ç”¨ {len(CANONICAL_FEATURE_NAMES)} å€‹ICT/SMCç‰¹å¾µï¼ˆèˆ‡é æ¸¬ä¸€è‡´ï¼‰")
            logger.info(f"   ğŸ“ˆ æ­£æ¨£æœ¬: {np.sum(y)} / {len(y)} ({np.mean(y)*100:.1f}%)")
            
            # å‰µå»º DMatrix
            dtrain = xgb.DMatrix(X, label=y)
            
            # ğŸ”¥ v3.18.6+ ç”Ÿç”¢ç´šè¨“ç·´åƒæ•¸
            params = {
                # ç›®æ¨™å‡½æ•¸èˆ‡è©•ä¼°
                'objective': self.training_params['objective'],
                'eval_metric': self.training_params['eval_metric'],
                
                # æ¨¹çµæ§‹
                'max_depth': self.training_params['max_depth'],
                'min_child_weight': self.training_params['min_child_weight'],
                
                # æ­£å‰‡åŒ–
                'gamma': self.training_params['gamma'],
                'subsample': self.training_params['subsample'],
                'colsample_bytree': self.training_params['colsample_bytree'],
                
                # å­¸ç¿’ç‡
                'learning_rate': self.training_params['learning_rate'],
                
                # å…¶ä»–
                'seed': self.training_params['random_state'],
                'n_jobs': self.training_params['n_jobs'],
                'verbosity': self.training_params['verbosity'],
            }
            
            # è¨“ç·´æ¨¡å‹
            logger.info(f"ğŸ§  é–‹å§‹è¨“ç·´: {self.training_params['n_estimators']} æ£µæ¨¹...")
            
            model = xgb.train(
                params,
                dtrain,
                num_boost_round=self.training_params['n_estimators'],
                verbose_eval=False
            )
            
            # ä¿å­˜æ¨¡å‹
            model.save_model(str(self.model_file))
            
            # æª¢æŸ¥æ¨¡å‹å¤§å°
            model_size = os.path.getsize(self.model_file) / 1024
            logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {self.model_file} ({model_size:.2f} KB)")
            
            if model_size > 100:
                logger.warning(f"âš ï¸ æ¨¡å‹è¼ƒå¤§ ({model_size:.2f} KB)ï¼Œå»ºè­°é‡åŒ–")
            
            # ğŸ”¥ v3.17.10+ï¼šè¨“ç·´å¾Œåˆ†æç‰¹å¾µé‡è¦æ€§ï¼ˆåé¥‹å¾ªç’°ï¼‰
            if self.model_evaluator:
                try:
                    logger.info("ğŸ“Š åˆ†ææ¨¡å‹ç‰¹å¾µé‡è¦æ€§...")
                    self.model_evaluator.analyze_feature_importance(model)
                except Exception as e:
                    logger.warning(f"âš ï¸ ç‰¹å¾µé‡è¦æ€§åˆ†æå¤±æ•—: {e}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ XGBoost è¨“ç·´å¤±æ•—: {e}", exc_info=True)
            return False
    
    def _initialize_feature_weights(self):
        """åˆå§‹åŒ–ç‰¹å¾µæ¬Šé‡ï¼ˆç„¡å…ˆé©—åå¥½ï¼‰"""
        weights_file = self.model_dir / "feature_weights.json"
        
        # æ‰€æœ‰ç‰¹å¾µæ¬Šé‡è¨­ç‚º 1.0ï¼ˆç„¡åå¥½ï¼‰
        default_weights = {
            'ema_20': 1.0,
            'ema_50': 1.0,
            'rsi': 1.0,
            'atr': 1.0,
            'volume': 1.0,
            'adx': 1.0,
            'macd': 1.0,
        }
        
        with open(weights_file, 'w') as f:
            json.dump(default_weights, f, indent=2)
        
        logger.info(f"âœ… ç‰¹å¾µæ¬Šé‡å·²åˆå§‹åŒ–: {weights_file}")
    
    def _create_flag_file(self):
        """å‰µå»ºåˆå§‹åŒ–æ¨™è¨˜æ–‡ä»¶"""
        flag_data = {
            'initialized_at': datetime.now().isoformat(),
            'training_params': self.training_params,
            'model_file': str(self.model_file),
            'version': 'v3.17+',
        }
        
        with open(self.flag_file, 'w') as f:
            json.dump(flag_data, f, indent=2)
        
        logger.info(f"âœ… åˆå§‹åŒ–æ¨™è¨˜å·²å‰µå»º: {self.flag_file}")
    
    async def force_retrain(self) -> bool:
        """
        å¼·åˆ¶é‡æ–°è¨“ç·´ï¼ˆåˆªé™¤æ¨™è¨˜æ–‡ä»¶ä¸¦é‡æ–°åˆå§‹åŒ–ï¼‰
        
        Returns:
            é‡æ–°è¨“ç·´æ˜¯å¦æˆåŠŸ
        """
        logger.warning("âš ï¸ å¼·åˆ¶é‡æ–°è¨“ç·´æ¨¡å‹...")
        
        # åˆªé™¤æ¨™è¨˜æ–‡ä»¶
        if self.flag_file.exists():
            self.flag_file.unlink()
            logger.info("ğŸ—‘ï¸ å·²åˆªé™¤ initialized.flag")
        
        # åˆªé™¤èˆŠæ¨¡å‹
        if self.model_file.exists():
            self.model_file.unlink()
            logger.info("ğŸ—‘ï¸ å·²åˆªé™¤èˆŠæ¨¡å‹æ–‡ä»¶")
        
        # é‡æ–°åˆå§‹åŒ–
        return await self.check_and_initialize()
    
    def should_retrain(self) -> bool:
        """
        å‹•æ…‹é‡è¨“ç·´è§¸ç™¼æ¢ä»¶ï¼ˆv3.17.10+ï¼‰
        
        è§£æ±ºã€Œå¸‚å ´é©æ‡‰æ…¢ã€å•é¡Œï¼š
        - å›ºå®š 50 ç­†è§¸ç™¼ç„¡æ³•æ‡‰å°å¸‚å ´ regime shift
        - å¾ trending â†’ choppy è½‰æ›æ™‚éœ€è¦ç«‹å³é‡è¨“ç·´
        
        Returns:
            æ˜¯å¦æ‡‰è©²é‡è¨“ç·´
        """
        try:
            # æ¢ä»¶ 1ï¼šæ€§èƒ½é©Ÿé™ï¼ˆSharpe æ¯”ç‡ä¸‹é™ 50%ï¼‰
            recent_trades = self._get_recent_trades(days=1)
            
            if len(recent_trades) >= 10:
                current_sharpe = self._calculate_sharpe(recent_trades)
                historical_sharpe = self._get_historical_sharpe()
                
                if historical_sharpe > 0 and current_sharpe < historical_sharpe * 0.5:
                    logger.warning(
                        f"âš ï¸ æ€§èƒ½é©Ÿé™è§¸ç™¼é‡è¨“ç·´: "
                        f"ç•¶å‰ Sharpe={current_sharpe:.2f} "
                        f"æ­·å² Sharpe={historical_sharpe:.2f} "
                        f"(ä¸‹é™ {(1 - current_sharpe/historical_sharpe)*100:.1f}%)"
                    )
                    return True
            
            # æ¢ä»¶ 2ï¼šå¸‚å ´ç‹€æ…‹åŠ‡è®Šï¼ˆregime shiftï¼‰
            current_regime = self._get_current_market_regime()
            last_regime = self._get_last_market_regime()
            
            if current_regime != last_regime and last_regime is not None:
                logger.warning(
                    f"âš ï¸ å¸‚å ´ç‹€æ…‹åŠ‡è®Šè§¸ç™¼é‡è¨“ç·´: "
                    f"{last_regime} â†’ {current_regime}"
                )
                self._update_last_market_regime(current_regime)
                return True
            
            # æ¢ä»¶ 3ï¼šç´¯ç©è¶³å¤ æ¨£æœ¬ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
            new_samples = self._count_new_samples()
            if new_samples >= 50:
                logger.info(
                    f"â„¹ï¸ ç´¯ç©æ¨£æœ¬è§¸ç™¼é‡è¨“ç·´: {new_samples} ç­†æ–°äº¤æ˜“"
                )
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"æª¢æŸ¥é‡è¨“ç·´æ¢ä»¶å¤±æ•—: {e}", exc_info=True)
            return False
    
    def _get_recent_trades(self, days: int = 1) -> List[Dict]:
        """
        ç²å–æœ€è¿‘ N å¤©çš„äº¤æ˜“è¨˜éŒ„
        
        Args:
            days: å¤©æ•¸
            
        Returns:
            äº¤æ˜“è¨˜éŒ„åˆ—è¡¨
        """
        try:
            if not self.trade_recorder:
                return []
            
            cutoff_time = datetime.now() - timedelta(days=days)
            all_trades = self.trade_recorder.completed_trades
            
            recent = [
                t for t in all_trades
                if datetime.fromisoformat(t.get('entry_timestamp', '1970-01-01')) > cutoff_time
            ]
            
            return recent
            
        except Exception as e:
            logger.error(f"ç²å–æœ€è¿‘äº¤æ˜“å¤±æ•—: {e}")
            return []
    
    def _calculate_sharpe(self, trades: List[Dict]) -> float:
        """
        è¨ˆç®— Sharpe æ¯”ç‡
        
        Args:
            trades: äº¤æ˜“è¨˜éŒ„åˆ—è¡¨
            
        Returns:
            Sharpe æ¯”ç‡
        """
        try:
            if not trades:
                return 0.0
            
            import numpy as np
            
            returns = [t.get('pnl_pct', 0.0) for t in trades]
            
            if not returns:
                return 0.0
            
            mean_return = float(np.mean(returns))
            std_return = float(np.std(returns))
            
            if std_return == 0:
                return 0.0
            
            sharpe = mean_return / std_return
            
            return float(sharpe)
            
        except Exception as e:
            logger.error(f"è¨ˆç®— Sharpe å¤±æ•—: {e}")
            return 0.0
    
    def _get_historical_sharpe(self) -> float:
        """
        ç²å–æ­·å² Sharpe æ¯”ç‡ï¼ˆéå» 7 å¤©ï¼‰
        
        Returns:
            æ­·å² Sharpe æ¯”ç‡
        """
        try:
            historical_trades = self._get_recent_trades(days=7)
            return self._calculate_sharpe(historical_trades)
        except Exception as e:
            logger.error(f"ç²å–æ­·å² Sharpe å¤±æ•—: {e}")
            return 0.0
    
    def _get_current_market_regime(self) -> str:
        """
        ç²å–ç•¶å‰å¸‚å ´ç‹€æ…‹
        
        Returns:
            'trending', 'choppy', 'volatile', 'calm'
        """
        try:
            # ç°¡åŒ–ç‰ˆï¼šåŸºæ–¼æœ€è¿‘äº¤æ˜“çš„å‹ç‡å’Œæ³¢å‹•æ€§
            recent_trades = self._get_recent_trades(days=1)
            
            if len(recent_trades) < 5:
                return 'unknown'
            
            import numpy as np
            
            # è¨ˆç®—å‹ç‡
            winners = sum(1 for t in recent_trades if t.get('pnl_pct', 0) > 0)
            win_rate = winners / len(recent_trades)
            
            # è¨ˆç®—æ³¢å‹•æ€§
            returns = [t.get('pnl_pct', 0.0) for t in recent_trades]
            volatility = np.std(returns)
            
            # ç°¡å–®åˆ†é¡
            if volatility > 0.05:  # é«˜æ³¢å‹•
                return 'volatile'
            elif win_rate > 0.6:  # é«˜å‹ç‡
                return 'trending'
            elif win_rate < 0.4:  # ä½å‹ç‡
                return 'choppy'
            else:
                return 'calm'
                
        except Exception as e:
            logger.error(f"ç²å–å¸‚å ´ç‹€æ…‹å¤±æ•—: {e}")
            return 'unknown'
    
    def _get_last_market_regime(self) -> Optional[str]:
        """
        ç²å–ä¸Šæ¬¡è¨˜éŒ„çš„å¸‚å ´ç‹€æ…‹
        
        Returns:
            ä¸Šæ¬¡å¸‚å ´ç‹€æ…‹æˆ– None
        """
        try:
            regime_file = self.model_dir / "market_regime.json"
            
            if not regime_file.exists():
                return None
            
            # ğŸ”¥ Stability Fix: Safe JSON read with corruption handling
            with open(regime_file, 'r') as f:
                content = f.read().strip()
                if not content:
                    logger.debug("å¸‚å ´ç‹€æ…‹æ–‡ä»¶ç‚ºç©ºï¼Œè¿”å›None")
                    return None
                data = json.loads(content)
                return data.get('regime')
                
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ å¸‚å ´ç‹€æ…‹JSONæå£ï¼ˆå·²å¿½ç•¥ï¼‰: {e}")
            return None
        except FileNotFoundError:
            return None
        except Exception as e:
            logger.error(f"è®€å–å¸‚å ´ç‹€æ…‹å¤±æ•—: {e}")
            return None
    
    def _update_last_market_regime(self, regime: str):
        """
        æ›´æ–°å¸‚å ´ç‹€æ…‹è¨˜éŒ„ï¼ˆä½¿ç”¨å®‰å…¨å¯«å…¥é˜²æ­¢æå£ï¼‰
        
        Args:
            regime: æ–°çš„å¸‚å ´ç‹€æ…‹
        """
        try:
            regime_file = self.model_dir / "market_regime.json"
            tmp_file = self.model_dir / "market_regime.json.tmp"
            
            data = {
                'regime': regime,
                'updated_at': datetime.now().isoformat()
            }
            
            # ğŸ”¥ Stability Fix: Safe write (tmp file + rename)
            with open(tmp_file, 'w') as f:
                json.dump(data, f, indent=2)
                f.flush()  # Ensure data is written to disk
                os.fsync(f.fileno())  # Force OS to write to disk
            
            # Atomic rename (prevents corruption during crashes)
            tmp_file.rename(regime_file)
                
        except Exception as e:
            logger.error(f"æ›´æ–°å¸‚å ´ç‹€æ…‹å¤±æ•—: {e}")
    
    def _count_new_samples(self) -> int:
        """
        è¨ˆç®—è‡ªä¸Šæ¬¡è¨“ç·´ä»¥ä¾†çš„æ–°æ¨£æœ¬æ•¸
        
        Returns:
            æ–°æ¨£æœ¬æ•¸é‡
        """
        try:
            if not self.trade_recorder:
                return 0
            
            # è®€å–ä¸Šæ¬¡è¨“ç·´æ™‚é–“
            if not self.flag_file.exists():
                return 0
            
            # ğŸ”¥ Stability Fix: Safe JSON read with corruption handling
            with open(self.flag_file, 'r') as f:
                content = f.read().strip()
                if not content:
                    logger.debug("Flagæ–‡ä»¶ç‚ºç©ºï¼Œè¿”å›0æ¨£æœ¬")
                    return 0
                flag_data = json.loads(content)
                last_trained = datetime.fromisoformat(flag_data.get('initialized_at', '1970-01-01'))
            
            # è¨ˆç®—æ–°äº¤æ˜“æ•¸
            all_trades = self.trade_recorder.completed_trades
            new_trades = [
                t for t in all_trades
                if datetime.fromisoformat(t.get('entry_timestamp', '1970-01-01')) > last_trained
            ]
            
            return len(new_trades)
            
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ Flagæ–‡ä»¶JSONæå£ï¼ˆå·²å¿½ç•¥ï¼Œè¿”å›0ï¼‰: {e}")
            return 0
        except FileNotFoundError:
            return 0
        except Exception as e:
            logger.error(f"è¨ˆç®—æ–°æ¨£æœ¬æ•¸å¤±æ•—: {e}")
            return 0
