"""
v3.17+ æ¨¡å‹è‡ªå‹•åˆå§‹åŒ–ç³»çµ±
éƒ¨ç½²åˆ° Railway å¾Œç«‹å³è¨“ç·´ï¼Œç„¡éœ€æ‰‹å‹•å¹²é 

v4.0 Feature Unification:
- ä½¿ç”¨ç»Ÿä¸€çš„12ä¸ªICT/SMCç‰¹å¾ï¼ˆä¸é¢„æµ‹ä¿æŒä¸€è‡´ï¼‰
- è®­ç»ƒå’Œæ¨ç†ä½¿ç”¨ç›¸åŒçš„feature_schema
"""

import os
import asyncio
from src.utils.logger_factory import get_logger
from pathlib import Path
from typing import Optional, Dict, Any, List
from datetime import datetime, timedelta
import json
from src.config import Config
from src.ml.feature_schema import (
    CANONICAL_FEATURE_NAMES,
    extract_canonical_features,
    features_to_vector,
    FEATURE_DEFAULTS
)

logger = get_logger(__name__)


class ModelInitializer:
    """
    æ¨¡å‹è‡ªå‹•åˆå§‹åŒ–å™¨ï¼ˆv3.17+ï¼‰
    
    è·è²¬ï¼š
    1. æª¢æ¸¬æ¨¡å‹æ˜¯å¦å­˜åœ¨
    2. è‹¥ç„¡æ¨¡å‹ï¼Œè‡ªå‹•æ”¶é›†è¨“ç·´æ•¸æ“š
    3. ä½¿ç”¨ä¸­æ€§åƒæ•¸è¨“ç·´åˆå§‹æ¨¡å‹
    4. å‰µå»º initialized.flag æ¨™è¨˜
    5. é›¶é…ç½®å•Ÿå‹•ï¼Œç„¡äººå·¥å¹²é 
    """
    
    def __init__(
        self,
        binance_client=None,
        trade_recorder=None,
        config_profile=None,
        model_evaluator=None
    ):
        """
        åˆå§‹åŒ–æ¨¡å‹åˆå§‹åŒ–å™¨
        
        Args:
            binance_client: BinanceClient å¯¦ä¾‹ï¼ˆå¯é¸ï¼‰
            trade_recorder: TradeRecorder å¯¦ä¾‹ï¼ˆå¯é¸ï¼‰
            config_profile: ConfigProfile å¯¦ä¾‹ï¼ˆå¯é¸ï¼‰
            model_evaluator: ModelEvaluator å¯¦ä¾‹ï¼ˆå¯é¸ï¼Œv3.17.10+ï¼‰
        """
        self.binance = binance_client
        self.trade_recorder = trade_recorder
        self.config = config_profile
        self.model_evaluator = model_evaluator  # ğŸ”¥ v3.17.10+
        
        # æ¨¡å‹ç›®éŒ„
        self.model_dir = Path("models")
        self.model_dir.mkdir(exist_ok=True)
        
        self.flag_file = self.model_dir / "initialized.flag"
        self.model_file = self.model_dir / "xgboost_model.json"
        
        # ğŸ”¥ v4.1+ å„ªåŒ–ç´š XGBoost åƒæ•¸ï¼ˆé™ä½éæ“¬åˆé¢¨éšªï¼‰
        self.training_params = {
            # ğŸŒ± æ¨¹çµæ§‹ï¼ˆæ§åˆ¶è¤‡é›œåº¦ï¼‰- OPTIMIZED
            'n_estimators': int(os.getenv("XGBOOST_N_ESTIMATORS", "30")),        # æ¨¹æ•¸é‡ï¼š100â†’30 (-70%)
            'max_depth': int(os.getenv("XGBOOST_MAX_DEPTH", "3")),               # æ¨¹æ·±åº¦ï¼š6â†’3 (-50%)
            'min_child_weight': int(os.getenv("XGBOOST_MIN_CHILD_WEIGHT", "50")), # è‘‰ç¯€é»æœ€å°æ¨£æœ¬ï¼š10â†’50 (5xï¼Œå…¼å®¹200æ¨£æœ¬)
            
            # âš–ï¸ æ­£å‰‡åŒ–ï¼ˆæå‡æ³›åŒ–ï¼‰- ENHANCED
            'gamma': float(os.getenv("XGBOOST_GAMMA", "0.2")),                   # åˆ†è£‚æœ€å°æå¤±ï¼š0.1â†’0.2
            'subsample': float(os.getenv("XGBOOST_SUBSAMPLE", "0.6")),           # è¨“ç·´æ¨£æœ¬æ¡æ¨£ï¼š0.8â†’0.6
            'colsample_bytree': float(os.getenv("XGBOOST_COLSAMPLE", "0.6")),    # ç‰¹å¾µæ¡æ¨£ï¼š0.8â†’0.6
            
            # ğŸš€ å­¸ç¿’ç‡ï¼ˆç©©å®šæ”¶æ–‚ï¼‰- MORE STABLE
            'learning_rate': float(os.getenv("XGBOOST_LEARNING_RATE", "0.05")),  # å­¸ç¿’æ­¥é•·ï¼š0.1â†’0.05
            
            # ğŸ¯ ç›®æ¨™å‡½æ•¸ï¼ˆäºŒåˆ†é¡ï¼‰
            'objective': 'binary:logistic',     # é‚è¼¯è¿´æ­¸æå¤±
            'eval_metric': 'logloss',           # è©•ä¼°æŒ‡æ¨™ï¼šå°æ¦‚ç‡é æ¸¬æ›´æ•æ„Ÿ
            
            # ğŸ§  å…¶ä»–é…ç½®
            'random_state': 42,                 # å¯é‡ç¾æ€§
            'n_jobs': -1,                       # å¤šæ ¸å¿ƒåŠ é€Ÿ
            'verbosity': 0,                     # éœé»˜æ¨¡å¼ï¼ˆé©åˆç”Ÿç”¢ï¼‰
            
            # è¨“ç·´æ•¸æ“šé…ç½®
            'min_samples': int(os.getenv("INITIAL_TRAINING_SAMPLES", "200")),
            'lookback_days': int(os.getenv("INITIAL_TRAINING_LOOKBACK_DAYS", "30")),
        }
        
        # âœ… STEP 1 VALIDATION
        logger.info("âœ… XGBooståƒæ•¸å·²å„ªåŒ–ï¼ˆv4.1 ä¿®æ­£ç‰ˆï¼‰:")
        logger.info(f"   æ¨¹æ•¸é‡: 100 â†’ {self.training_params['n_estimators']}")
        logger.info(f"   æ¨¹æ·±åº¦: 6 â†’ {self.training_params['max_depth']}")
        logger.info(f"   æœ€å°å­ç¯€é»æ¬Šé‡: 10 â†’ {self.training_params['min_child_weight']} (å…¼å®¹200æ¨£æœ¬)")
        
        logger.info("=" * 60)
        logger.info("âœ… æ¨¡å‹è‡ªå‹•åˆå§‹åŒ–å™¨å·²å‰µå»ºï¼ˆv3.18.6+ç”Ÿç”¢ç´šï¼‰")
        logger.info(f"   ğŸ“ æ¨¡å‹ç›®éŒ„: {self.model_dir}")
        logger.info(f"   ğŸ¯ è¨“ç·´åƒæ•¸: n_estimators={self.training_params['n_estimators']}, "
                   f"max_depth={self.training_params['max_depth']}, "
                   f"min_child_weight={self.training_params['min_child_weight']}, "
                   f"gamma={self.training_params['gamma']}")
        logger.info(f"   ğŸ¯ ç›®æ¨™å‡½æ•¸: {self.training_params['objective']}, "
                   f"è©•ä¼°æŒ‡æ¨™: {self.training_params['eval_metric']}")
        logger.info("=" * 60)
    
    async def check_and_initialize(self) -> bool:
        """
        æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²åˆå§‹åŒ–ï¼Œè‹¥ç„¡å‰‡è‡ªå‹•è¨“ç·´
        
        Returns:
            æ˜¯å¦å·²åˆå§‹åŒ–ï¼ˆTrue=å·²å­˜åœ¨æˆ–è¨“ç·´æˆåŠŸï¼ŒFalse=è¨“ç·´å¤±æ•—ï¼‰
        """
        # ğŸ”’ v3.18.7+ï¼šæª¢æŸ¥æ¨¡å‹è¨“ç·´é–å®šé–‹é—œ
        if getattr(Config, 'DISABLE_MODEL_TRAINING', False):
            logger.info("ğŸ”’ æ¨¡å‹è¨“ç·´å·²é–å®šï¼ˆDISABLE_MODEL_TRAINING=Trueï¼‰")
            logger.info("   âœ… ç³»çµ±å°‡ä½¿ç”¨ç¾æœ‰æ¨¡å‹ï¼Œä¸é€²è¡Œåˆå§‹è¨“ç·´æˆ–é‡è¨“ç·´")
            
            # æª¢æŸ¥æ˜¯å¦å·²æœ‰æ¨¡å‹æ–‡ä»¶
            if self.model_file.exists():
                logger.info(f"   âœ… æª¢æ¸¬åˆ°ç¾æœ‰æ¨¡å‹: {self.model_file}")
                # å³ä½¿æ²’æœ‰flagæ–‡ä»¶ï¼Œä¹Ÿå‰µå»ºä¸€å€‹ï¼ˆé˜²æ­¢ä¸‹æ¬¡æª¢æŸ¥ï¼‰
                if not self.flag_file.exists():
                    self._create_flag_file()
                return True
            else:
                logger.warning(f"   âš ï¸ æœªæª¢æ¸¬åˆ°æ¨¡å‹æ–‡ä»¶: {self.model_file}")
                logger.warning("   âš ï¸ è«‹ç¢ºä¿å·²æœ‰é è¨“ç·´æ¨¡å‹ï¼Œæˆ–è‡¨æ™‚é—œé–‰DISABLE_MODEL_TRAINING")
                return False
        
        # æª¢æŸ¥æ¨™è¨˜æ–‡ä»¶
        if self.flag_file.exists():
            logger.info("âœ… æ¨¡å‹å·²åˆå§‹åŒ–ï¼ˆæª¢æ¸¬åˆ° initialized.flagï¼‰")
            return True
        
        logger.warning("âš ï¸ æœªæª¢æ¸¬åˆ°åˆå§‹åŒ–æ¨¡å‹ï¼Œé–‹å§‹è‡ªå‹•è¨“ç·´...")
        
        # åŸ·è¡Œåˆå§‹è¨“ç·´
        success = await self._initial_training()
        
        if success:
            # å‰µå»ºæ¨™è¨˜æ–‡ä»¶
            self._create_flag_file()
            logger.info("ğŸ‰ æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ç³»çµ±å·²å°±ç·’")
            return True
        else:
            logger.error("âŒ æ¨¡å‹åˆå§‹åŒ–å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ—¥èªŒ")
            return False
    
    async def _initial_training(self) -> bool:
        """
        åŸ·è¡Œåˆå§‹è¨“ç·´
        
        Returns:
            è¨“ç·´æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("ğŸš€ é–‹å§‹æ”¶é›†è¨“ç·´æ•¸æ“š...")
            
            # 1. æ”¶é›†é«˜å“è³ªäº¤æ˜“æ•¸æ“š
            training_data = await self._collect_training_data()
            
            if not training_data or len(training_data) < self.training_params['min_samples']:
                logger.error(
                    f"âŒ è¨“ç·´æ•¸æ“šä¸è¶³: {len(training_data) if training_data else 0} "
                    f"< {self.training_params['min_samples']}"
                )
                return False
            
            logger.info(f"âœ… æ”¶é›†åˆ° {len(training_data)} ç­†è¨“ç·´æ•¸æ“š")
            
            # 2. è¨“ç·´åˆå§‹æ¨¡å‹
            logger.info("ğŸ§  é–‹å§‹è¨“ç·´ XGBoost æ¨¡å‹...")
            model_success = await self._train_xgboost_model(training_data)
            
            if not model_success:
                logger.error("âŒ XGBoost æ¨¡å‹è¨“ç·´å¤±æ•—")
                return False
            
            logger.info("âœ… XGBoost æ¨¡å‹è¨“ç·´å®Œæˆ")
            
            # 3. åˆå§‹åŒ–ç‰¹å¾µæ¬Šé‡ï¼ˆç„¡å…ˆé©—åå¥½ï¼‰
            self._initialize_feature_weights()
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹è¨“ç·´å¤±æ•—: {e}", exc_info=True)
            return False
    
    async def _collect_training_data(self) -> List[Dict[str, Any]]:
        """
        ğŸ”¥ v3.18.6+ Critical Fix: æ”¶é›†è¨“ç·´æ•¸æ“šï¼ˆå„ªå…ˆä½¿ç”¨trades.jsonlï¼‰
        
        ç­–ç•¥ï¼š
        1. ğŸ”¥ å„ªå…ˆå¾ trades.jsonl åŠ è¼‰çœŸå¯¦äº¤æ˜“æ•¸æ“šï¼ˆ44å€‹ç‰¹å¾µï¼‰
        2. è‹¥æ•¸æ“šä¸è¶³ï¼Œä½¿ç”¨å¸‚å ´æ•¸æ“šç”Ÿæˆåˆæˆæ¨£æœ¬
        
        Returns:
            è¨“ç·´æ•¸æ“šåˆ—è¡¨
        """
        training_data = []
        
        # ğŸ”¥ v4.0+: å¾ PostgreSQL/JSONL åŠ è¼‰çœŸå¯¦äº¤æ˜“æ•¸æ“š
        logger.info("ğŸ“Š åŠ è¼‰çœŸå¯¦äº¤æ˜“æ•¸æ“šï¼ˆPostgreSQLå„ªå…ˆï¼‰...")
        real_trades = await self._load_training_data_from_trades()
        
        if real_trades:
            logger.info(f"âœ… åŠ è¼‰ {len(real_trades)} ç­†çœŸå¯¦äº¤æ˜“æ•¸æ“šï¼ˆ12ç‰¹å¾µï¼‰")
            training_data.extend(real_trades)
        else:
            logger.warning("âš ï¸ PostgreSQL/JSONL ç„¡æ•¸æ“šæˆ–ä¸å­˜åœ¨")
        
        # ç­–ç•¥ 2: è‹¥æ•¸æ“šä¸è¶³ï¼Œç”Ÿæˆåˆæˆæ¨£æœ¬
        if len(training_data) < self.training_params['min_samples']:
            needed = self.training_params['min_samples'] - len(training_data)
            logger.info(f"ğŸ“Š æ•¸æ“šä¸è¶³ï¼Œå¾å¸‚å ´æ•¸æ“šç”Ÿæˆ {needed} å€‹åˆæˆæ¨£æœ¬...")
            synthetic_samples = await self._generate_synthetic_samples(target_count=needed)
            training_data.extend(synthetic_samples)
        
        logger.info(f"âœ… ç¸½è¨ˆæ”¶é›† {len(training_data)} ç­†è¨“ç·´æ•¸æ“š")
        logger.info(f"   çœŸå¯¦äº¤æ˜“: {len(real_trades)}")
        logger.info(f"   åˆæˆæ¨£æœ¬: {len(training_data) - len(real_trades)}")
        
        return training_data
    
    async def _get_historical_trades(self) -> List[Dict[str, Any]]:
        """ç²å–æ­·å²äº¤æ˜“è¨˜éŒ„"""
        try:
            if self.trade_recorder is None:
                return []
            
            if hasattr(self.trade_recorder, 'get_all_trades'):
                return await self.trade_recorder.get_all_trades()
            elif hasattr(self.trade_recorder, 'get_closed_trades'):
                return await self.trade_recorder.get_closed_trades()
            else:
                return []
        except Exception as e:
            logger.error(f"âŒ ç²å–æ­·å²äº¤æ˜“å¤±æ•—: {e}")
            return []
    
    async def _generate_synthetic_samples(self, target_count: int) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆåˆæˆè¨“ç·´æ¨£æœ¬
        
        ç­–ç•¥ï¼šå¾å¯¦æ™‚å¸‚å ´æ•¸æ“šæå–ç‰¹å¾µï¼Œä½¿ç”¨ç°¡å–®è¦å‰‡æ¨™è¨»
        
        Args:
            target_count: ç›®æ¨™æ¨£æœ¬æ•¸é‡
            
        Returns:
            åˆæˆæ¨£æœ¬åˆ—è¡¨
        """
        synthetic_samples = []
        
        if not self.binance:
            logger.warning("âš ï¸ ç„¡ BinanceClientï¼Œç„¡æ³•ç”Ÿæˆåˆæˆæ¨£æœ¬")
            return synthetic_samples
        
        try:
            # ç²å–ç†±é–€äº¤æ˜“å°
            symbols = await self._get_top_symbols(limit=20)
            
            logger.info(f"ğŸ“Š å¾ {len(symbols)} å€‹äº¤æ˜“å°ç”Ÿæˆåˆæˆæ¨£æœ¬...")
            
            # ç‚ºæ¯å€‹äº¤æ˜“å°ç”Ÿæˆæ¨£æœ¬
            samples_per_symbol = max(1, target_count // len(symbols))
            
            for symbol in symbols:
                try:
                    # ç²å– K ç·šæ•¸æ“šï¼ˆ1 å°æ™‚ï¼Œæœ€è¿‘ 200 æ ¹ï¼‰
                    klines = await self.binance.get_klines(
                        symbol=symbol,
                        interval='1h',
                        limit=200
                    )
                    
                    if not klines or len(klines) < 100:
                        continue
                    
                    # ç”Ÿæˆç‰¹å¾µä¸¦æ¨™è¨»
                    samples = self._extract_features_and_label(klines, samples_per_symbol)
                    synthetic_samples.extend(samples)
                    
                    if len(synthetic_samples) >= target_count:
                        break
                        
                except Exception as e:
                    logger.debug(f"âš ï¸ {symbol} ç”Ÿæˆæ¨£æœ¬å¤±æ•—: {e}")
                    continue
            
            logger.info(f"âœ… ç”Ÿæˆ {len(synthetic_samples)} ç­†åˆæˆæ¨£æœ¬")
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆåˆæˆæ¨£æœ¬å¤±æ•—: {e}")
        
        return synthetic_samples[:target_count]
    
    async def _get_top_symbols(self, limit: int = 20) -> List[str]:
        """ç²å–ç†±é–€äº¤æ˜“å°"""
        try:
            if self.binance is None:
                return ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'SOLUSDT', 'ADAUSDT']
            
            # ç²å– 24h äº¤æ˜“é‡æ’å
            tickers = await self.binance.get_24h_tickers()
            
            # éæ¿¾ USDT åˆç´„ï¼ŒæŒ‰äº¤æ˜“é‡æ’åº
            usdt_tickers = [
                t for t in tickers
                if t['symbol'].endswith('USDT')
            ]
            
            sorted_tickers = sorted(
                usdt_tickers,
                key=lambda x: float(x.get('quoteVolume', 0)),
                reverse=True
            )
            
            return [t['symbol'] for t in sorted_tickers[:limit]]
            
        except Exception as e:
            logger.error(f"âŒ ç²å–ç†±é–€äº¤æ˜“å°å¤±æ•—: {e}")
            # è¿”å›é»˜èªåˆ—è¡¨
            return ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'SOLUSDT', 'ADAUSDT']
    
    def _extract_features_and_label(
        self,
        klines: List[Dict],
        max_samples: int
    ) -> List[Dict[str, Any]]:
        """
        å¾ K ç·šæå–ç‰¹å¾µä¸¦æ¨™è¨»
        
        ç°¡å–®è¦å‰‡ï¼š
        - ä¸Šæ¼²è¶¨å‹¢ï¼ˆ20 EMA ä¸Šç©¿ 50 EMAï¼‰â†’ æ­£æ¨£æœ¬
        - ä¸‹è·Œè¶¨å‹¢ï¼ˆ20 EMA ä¸‹ç©¿ 50 EMAï¼‰â†’ è² æ¨£æœ¬
        
        Args:
            klines: K ç·šæ•¸æ“š
            max_samples: æœ€å¤§æ¨£æœ¬æ•¸
            
        Returns:
            ç‰¹å¾µæ¨£æœ¬åˆ—è¡¨
        """
        samples = []
        
        try:
            import pandas as pd
            import numpy as np
            
            # è½‰æ›ç‚º DataFrame
            df = pd.DataFrame(klines)
            df['close'] = df['close'].astype(float)
            df['high'] = df['high'].astype(float)
            df['low'] = df['low'].astype(float)
            df['volume'] = df['volume'].astype(float)
            
            # è¨ˆç®— EMA
            df['ema_20'] = df['close'].ewm(span=20, adjust=False).mean()
            df['ema_50'] = df['close'].ewm(span=50, adjust=False).mean()
            
            # è¨ˆç®— RSI
            delta = df['close'].diff()
            gain = delta.where(delta > 0, 0).rolling(window=14).mean()
            loss = -delta.where(delta < 0, 0).rolling(window=14).mean()
            rs = gain / loss
            df['rsi'] = 100 - (100 / (1 + rs))
            
            # è¨ˆç®— ATR
            high_low = df['high'] - df['low']
            high_close = abs(df['high'] - df['close'].shift())
            low_close = abs(df['low'] - df['close'].shift())
            ranges = pd.concat([high_low, high_close, low_close], axis=1)
            df['atr'] = ranges.max(axis=1).rolling(window=14).mean()
            
            # ç”Ÿæˆæ¨£æœ¬ï¼ˆé¸æ“‡è¶¨å‹¢æ˜ç¢ºçš„é»ï¼‰
            for i in range(60, len(df) - 10, 10):  # æ¯ 10 æ ¹ K ç·šå–ä¸€å€‹æ¨£æœ¬
                if len(samples) >= max_samples:
                    break
                
                row = df.iloc[i]
                
                # ç‰¹å¾µ
                features = {
                    'ema_20': row['ema_20'],
                    'ema_50': row['ema_50'],
                    'rsi': row['rsi'],
                    'atr': row['atr'],
                    'volume': row['volume'],
                    'close': row['close'],
                }
                
                # æ¨™è¨»ï¼ˆç°¡å–®è¦å‰‡ï¼‰
                ema_diff = row['ema_20'] - row['ema_50']
                future_return = (df.iloc[i + 10]['close'] - row['close']) / row['close']
                
                # æ­£æ¨£æœ¬ï¼šEMA å¤šé ­æ’åˆ— ä¸” æœªä¾†ä¸Šæ¼²
                # è² æ¨£æœ¬ï¼šEMA ç©ºé ­æ’åˆ— ä¸” æœªä¾†ä¸‹è·Œ
                if ema_diff > 0 and future_return > 0.01:
                    label = 1  # å‹åˆ©
                elif ema_diff < 0 and future_return < -0.01:
                    label = 1  # å‹åˆ©ï¼ˆç©ºé ­åˆ¤æ–·æ­£ç¢ºï¼‰
                else:
                    label = 0  # å¤±æ•—
                
                samples.append({
                    'features': features,
                    'label': label,
                    'pnl': future_return,
                })
        
        except Exception as e:
            logger.error(f"âŒ æå–ç‰¹å¾µå¤±æ•—: {e}")
        
        return samples
    
    async def _load_training_data_from_trades(self) -> List[Dict]:
        """
        ğŸ”¥ v4.0 Feature Unification: å¾ PostgreSQL åŠ è¼‰çœŸå¯¦äº¤æ˜“æ•¸æ“š
        
        ä½¿ç”¨ç»Ÿä¸€çš„12ä¸ªICT/SMCç‰¹å¾ï¼ˆä¸é¢„æµ‹ä¸€è‡´ï¼‰
        
        Returns:
            è¨“ç·´æ•¸æ“šåˆ—è¡¨ï¼ˆæ¯å€‹å…ƒç´ åŒ…å«12å€‹æ¨™æº–ç‰¹å¾µ + labelï¼‰
        """
        training_data = []
        
        # v4.0: ä¼˜å…ˆä»PostgreSQLè¯»å–
        if self.trade_recorder and hasattr(self.trade_recorder, 'data_service'):
            try:
                trades = await self.trade_recorder.data_service.get_all_trades()
                
                for trade in trades:
                    # æå–å…ƒæ•°æ®ä¸­çš„ç‰¹å¾
                    metadata = trade.get('metadata', {})
                    features_dict = metadata.get('features', {})
                    
                    # v4.0: å³ä½¿ç¼ºå°‘featuresï¼Œä¹Ÿä½¿ç”¨é»˜è®¤å€¼ï¼ˆdefensiveï¼‰
                    if not features_dict:
                        logger.debug(f"âš ï¸ Trade {trade.get('id')} ç¼ºå°‘featuresï¼Œä½¿ç”¨é»˜è®¤å€¼")
                        features_dict = {}
                    
                    # æå–12ä¸ªæ ‡å‡†ç‰¹å¾ï¼ˆç¼ºå¤±å­—æ®µä½¿ç”¨FEATURE_DEFAULTSï¼‰
                    canonical = extract_canonical_features(features_dict)
                    
                    # ç¡®å®šæ ‡ç­¾ï¼ˆoutcome: WIN=1, LOSS=0ï¼‰
                    label = 1 if trade.get('outcome') == 'WIN' else 0
                    
                    training_data.append({
                        'features': canonical,
                        'label': label,
                        'pnl': float(trade.get('pnl', 0))
                    })
                
                if training_data:
                    logger.info(f"âœ… å¾ PostgreSQL åŠ è¼‰ {len(training_data)} ç­†è¨“ç·´æ•¸æ“šï¼ˆ12ç‰¹å¾µï¼‰")
                    return training_data
                else:
                    logger.warning("âš ï¸ PostgreSQLç„¡å¯ç”¨è¨“ç·´æ•¸æ“šï¼Œå˜—è©¦JSONLå‚™æ´")
                
            except Exception as e:
                logger.warning(f"âš ï¸ å¾PostgreSQLåŠ è¼‰å¤±æ•—: {e}ï¼Œå˜—è©¦JSONLå‚™æ´")
        
        # Fallback: ä»trades.jsonlè¯»å–ï¼ˆå‘åå…¼å®¹ or PostgreSQLä¸è¶³ï¼‰
        trades_file = Path("data/trades.jsonl")
        
        if not trades_file.exists():
            logger.warning(f"âš ï¸ è¨“ç·´æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {trades_file}")
            return training_data  # è¿”å›PostgreSQLæ•°æ®ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
        
        try:
            with open(trades_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        try:
                            trade = json.loads(line)
                            
                            # v4.0: ä»æ—§æ ¼å¼æå–12ä¸ªæ ‡å‡†ç‰¹å¾
                            features_dict = trade.get('features', trade)
                            canonical = extract_canonical_features(features_dict)
                            
                            label = int(trade.get('label', trade.get('outcome') == 'WIN'))
                            
                            training_data.append({
                                'features': canonical,
                                'label': label,
                                'pnl': float(trade.get('pnl', 0))
                            })
                        except (json.JSONDecodeError, Exception) as e:
                            logger.debug(f"è·³éç„¡æ•ˆæ•¸æ“šè¡Œ: {e}")
                            continue
            
            if training_data:
                logger.info(f"âœ… å¾ {trades_file} åŠ è¼‰ {len(training_data)} ç­†è¨“ç·´æ•¸æ“šï¼ˆ12ç‰¹å¾µï¼‰")
            
        except Exception as e:
            logger.error(f"âŒ åŠ è¼‰è¨“ç·´æ•¸æ“šå¤±æ•—: {e}")
        
        return training_data
    
    def _extract_44_features_DEPRECATED(self, trade: Dict) -> Optional[List[float]]:
        """
        âš ï¸ DEPRECATED v4.0: This method is no longer used
        
        v4.0 now uses 12 canonical ICT/SMC features via feature_schema.py
        Kept for reference only
        """
        logger.warning("âš ï¸ _extract_44_features is deprecated, use feature_schema instead")
        return None  # No longer functional
        try:
            # ğŸ”¥ v3.18.6+ Critical Fix: æ‰€æœ‰å­—æ®µéƒ½ä½¿ç”¨é»˜èªå€¼ï¼Œç¢ºä¿æ­·å²æ•¸æ“šä¸è¢«è·³é
            features = [
                # åŸºæœ¬ç‰¹å¾µ (8) - æ ¸å¿ƒå­—æ®µå„ªå…ˆå¾tradeè®€å–
                float(trade.get('confidence', trade.get('confidence_score', 0.5))),
                float(trade.get('leverage', 1.0)),
                float(trade.get('position_value', 0.0)),
                float(trade.get('risk_reward_ratio', trade.get('rr_ratio', 1.5))),
                float(trade.get('order_blocks_count', trade.get('order_blocks', 0))),
                float(trade.get('liquidity_zones_count', trade.get('liquidity_zones', 0))),
                float(trade.get('entry_price', 0.0)),
                float(trade.get('win_probability', 0.5)),
                
                # æŠ€è¡“æŒ‡æ¨™ (10) - ä½¿ç”¨ä¸­æ€§é»˜èªå€¼
                float(trade.get('rsi', 50.0)),
                float(trade.get('macd', 0.0)),
                float(trade.get('macd_signal', 0.0)),
                float(trade.get('macd_histogram', 0.0)),
                float(trade.get('atr', 0.0)),
                float(trade.get('bb_width', 0.0)),
                float(trade.get('volume_sma_ratio', 1.0)),
                float(trade.get('ema50', 0.0)),
                float(trade.get('ema200', 0.0)),
                float(trade.get('volatility_24h', 0.0)),
                
                # è¶¨å‹¢ç‰¹å¾µ (6) - ä½¿ç”¨ä¸­æ€§é»˜èªå€¼
                float(trade.get('trend_1h', 0)),
                float(trade.get('trend_15m', 0)),
                float(trade.get('trend_5m', 0)),
                float(trade.get('market_structure', 0)),
                float(trade.get('direction', 1)),  # LONG=1, SHORT=-1
                float(trade.get('trend_alignment', 0.0)),
                
                # å…¶ä»–ç‰¹å¾µ (14) - æ‰€æœ‰å¯é¸å­—æ®µä½¿ç”¨é»˜èªå€¼
                float(trade.get('ema50_slope', 0.0)),
                float(trade.get('ema200_slope', 0.0)),
                float(trade.get('higher_highs', 0)),
                float(trade.get('lower_lows', 0)),
                float(trade.get('support_strength', 0.5)),
                float(trade.get('resistance_strength', 0.5)),
                float(trade.get('fvg_count', 0)),
                float(trade.get('swing_high_distance', 0.0)),
                float(trade.get('swing_low_distance', 0.0)),
                float(trade.get('volume_profile', 0.5)),
                float(trade.get('price_momentum', 0.0)),
                float(trade.get('order_flow', 0.0)),
                float(trade.get('liquidity_grab', 0)),
                float(trade.get('institutional_candle', 0)),
                
                # ç«¶åƒ¹ä¸Šä¸‹æ–‡ç‰¹å¾µ (3) - æ–°å­—æ®µä½¿ç”¨é»˜èªå€¼
                float(trade.get('competition_rank', 1)),
                float(trade.get('score_gap_to_best', 0.0)),
                float(trade.get('num_competing_signals', 1)),
                
                # WebSocketå°ˆå±¬ç‰¹å¾µ (3) - æ–°å­—æ®µä½¿ç”¨é»˜èªå€¼
                float(trade.get('latency_zscore', 0.0)),
                float(trade.get('shard_load', 0.0)),
                float(trade.get('timestamp_consistency', 1))
            ]
            
            # é©—è­‰é•·åº¦
            if len(features) != 44:
                logger.error(f"ç‰¹å¾µæ•¸é‡éŒ¯èª¤: {len(features)} != 44")
                return None
            
            return features
            
        except (ValueError, TypeError) as e:
            # åªåœ¨é¡å‹è½‰æ›å¤±æ•—æ™‚è¿”å›None
            logger.warning(f"ç‰¹å¾µæå–å¤±æ•—ï¼ˆæ•¸æ“šé¡å‹éŒ¯èª¤ï¼‰: {e}")
            return None
        except Exception as e:
            logger.error(f"ç‰¹å¾µæå–ç•°å¸¸: {e}")
            return None
    
    async def _train_xgboost_model(self, training_data: List[Dict]) -> bool:
        """
        ğŸ”¥ v4.0 Feature Unification: è¨“ç·´ XGBoost æ¨¡å‹ï¼ˆä½¿ç”¨12å€‹ICT/SMCç‰¹å¾µï¼‰
        
        Args:
            training_data: è¨“ç·´æ•¸æ“šï¼ˆåŒ…å«12å€‹æ¨™æº–ç‰¹å¾µ + labelï¼‰
            
        Returns:
            è¨“ç·´æ˜¯å¦æˆåŠŸ
        """
        try:
            import xgboost as xgb
            import numpy as np
            
            # ğŸ”¥ v4.0: æå–12å€‹æ¨™æº–ç‰¹å¾µ
            X = []
            y = []
            
            for trade in training_data:
                # æå–12å€‹ç‰¹å¾µå‘é‡
                features_dict = trade.get('features', {})
                features_vector = features_to_vector(features_dict)
                
                # æå–æ¨™ç±¤
                label = int(trade.get('label', 0))
                
                X.append(features_vector)
                y.append(label)
            
            if len(X) < 10:
                logger.error(f"âŒ ç‰¹å¾µæ•¸æ“šä¸è¶³: {len(X)} < 10")
                return False
            
            X = np.array(X)
            y = np.array(y)
            
            logger.info(f"ğŸ“Š è¨“ç·´æ•¸æ“š: X.shape={X.shape}, y.shape={y.shape}")
            logger.info(f"   âœ… ä½¿ç”¨ {len(CANONICAL_FEATURE_NAMES)} å€‹ICT/SMCç‰¹å¾µï¼ˆèˆ‡é æ¸¬ä¸€è‡´ï¼‰")
            logger.info(f"   ğŸ“ˆ æ­£æ¨£æœ¬: {np.sum(y)} / {len(y)} ({np.mean(y)*100:.1f}%)")
            
            # å‰µå»º DMatrix
            dtrain = xgb.DMatrix(X, label=y)
            
            # ğŸ”¥ v3.18.6+ ç”Ÿç”¢ç´šè¨“ç·´åƒæ•¸
            params = {
                # ç›®æ¨™å‡½æ•¸èˆ‡è©•ä¼°
                'objective': self.training_params['objective'],
                'eval_metric': self.training_params['eval_metric'],
                
                # æ¨¹çµæ§‹
                'max_depth': self.training_params['max_depth'],
                'min_child_weight': self.training_params['min_child_weight'],
                
                # æ­£å‰‡åŒ–
                'gamma': self.training_params['gamma'],
                'subsample': self.training_params['subsample'],
                'colsample_bytree': self.training_params['colsample_bytree'],
                
                # å­¸ç¿’ç‡
                'learning_rate': self.training_params['learning_rate'],
                
                # å…¶ä»–
                'seed': self.training_params['random_state'],
                'n_jobs': self.training_params['n_jobs'],
                'verbosity': self.training_params['verbosity'],
            }
            
            # è¨“ç·´æ¨¡å‹
            logger.info(f"ğŸ§  é–‹å§‹è¨“ç·´: {self.training_params['n_estimators']} æ£µæ¨¹...")
            
            model = xgb.train(
                params,
                dtrain,
                num_boost_round=self.training_params['n_estimators'],
                verbose_eval=False
            )
            
            # ä¿å­˜æ¨¡å‹
            model.save_model(str(self.model_file))
            
            # æª¢æŸ¥æ¨¡å‹å¤§å°
            model_size = os.path.getsize(self.model_file) / 1024
            logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {self.model_file} ({model_size:.2f} KB)")
            
            if model_size > 100:
                logger.warning(f"âš ï¸ æ¨¡å‹è¼ƒå¤§ ({model_size:.2f} KB)ï¼Œå»ºè­°é‡åŒ–")
            
            # ğŸ”¥ v3.17.10+ï¼šè¨“ç·´å¾Œåˆ†æç‰¹å¾µé‡è¦æ€§ï¼ˆåé¥‹å¾ªç’°ï¼‰
            if self.model_evaluator:
                try:
                    logger.info("ğŸ“Š åˆ†ææ¨¡å‹ç‰¹å¾µé‡è¦æ€§...")
                    self.model_evaluator.analyze_feature_importance(model)
                except Exception as e:
                    logger.warning(f"âš ï¸ ç‰¹å¾µé‡è¦æ€§åˆ†æå¤±æ•—: {e}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ XGBoost è¨“ç·´å¤±æ•—: {e}", exc_info=True)
            return False
    
    def _initialize_feature_weights(self):
        """åˆå§‹åŒ–ç‰¹å¾µæ¬Šé‡ï¼ˆç„¡å…ˆé©—åå¥½ï¼‰"""
        weights_file = self.model_dir / "feature_weights.json"
        
        # æ‰€æœ‰ç‰¹å¾µæ¬Šé‡è¨­ç‚º 1.0ï¼ˆç„¡åå¥½ï¼‰
        default_weights = {
            'ema_20': 1.0,
            'ema_50': 1.0,
            'rsi': 1.0,
            'atr': 1.0,
            'volume': 1.0,
            'adx': 1.0,
            'macd': 1.0,
        }
        
        with open(weights_file, 'w') as f:
            json.dump(default_weights, f, indent=2)
        
        logger.info(f"âœ… ç‰¹å¾µæ¬Šé‡å·²åˆå§‹åŒ–: {weights_file}")
    
    def _create_flag_file(self):
        """å‰µå»ºåˆå§‹åŒ–æ¨™è¨˜æ–‡ä»¶"""
        flag_data = {
            'initialized_at': datetime.now().isoformat(),
            'training_params': self.training_params,
            'model_file': str(self.model_file),
            'version': 'v3.17+',
        }
        
        with open(self.flag_file, 'w') as f:
            json.dump(flag_data, f, indent=2)
        
        logger.info(f"âœ… åˆå§‹åŒ–æ¨™è¨˜å·²å‰µå»º: {self.flag_file}")
    
    async def force_retrain(self) -> bool:
        """
        å¼·åˆ¶é‡æ–°è¨“ç·´ï¼ˆåˆªé™¤æ¨™è¨˜æ–‡ä»¶ä¸¦é‡æ–°åˆå§‹åŒ–ï¼‰
        
        Returns:
            é‡æ–°è¨“ç·´æ˜¯å¦æˆåŠŸ
        """
        logger.warning("âš ï¸ å¼·åˆ¶é‡æ–°è¨“ç·´æ¨¡å‹...")
        
        # åˆªé™¤æ¨™è¨˜æ–‡ä»¶
        if self.flag_file.exists():
            self.flag_file.unlink()
            logger.info("ğŸ—‘ï¸ å·²åˆªé™¤ initialized.flag")
        
        # åˆªé™¤èˆŠæ¨¡å‹
        if self.model_file.exists():
            self.model_file.unlink()
            logger.info("ğŸ—‘ï¸ å·²åˆªé™¤èˆŠæ¨¡å‹æ–‡ä»¶")
        
        # é‡æ–°åˆå§‹åŒ–
        return await self.check_and_initialize()
    
    def should_retrain(self) -> bool:
        """
        å‹•æ…‹é‡è¨“ç·´è§¸ç™¼æ¢ä»¶ï¼ˆv3.17.10+ï¼‰
        
        è§£æ±ºã€Œå¸‚å ´é©æ‡‰æ…¢ã€å•é¡Œï¼š
        - å›ºå®š 50 ç­†è§¸ç™¼ç„¡æ³•æ‡‰å°å¸‚å ´ regime shift
        - å¾ trending â†’ choppy è½‰æ›æ™‚éœ€è¦ç«‹å³é‡è¨“ç·´
        
        Returns:
            æ˜¯å¦æ‡‰è©²é‡è¨“ç·´
        """
        try:
            # æ¢ä»¶ 1ï¼šæ€§èƒ½é©Ÿé™ï¼ˆSharpe æ¯”ç‡ä¸‹é™ 50%ï¼‰
            recent_trades = self._get_recent_trades(days=1)
            
            if len(recent_trades) >= 10:
                current_sharpe = self._calculate_sharpe(recent_trades)
                historical_sharpe = self._get_historical_sharpe()
                
                if historical_sharpe > 0 and current_sharpe < historical_sharpe * 0.5:
                    logger.warning(
                        f"âš ï¸ æ€§èƒ½é©Ÿé™è§¸ç™¼é‡è¨“ç·´: "
                        f"ç•¶å‰ Sharpe={current_sharpe:.2f} "
                        f"æ­·å² Sharpe={historical_sharpe:.2f} "
                        f"(ä¸‹é™ {(1 - current_sharpe/historical_sharpe)*100:.1f}%)"
                    )
                    return True
            
            # æ¢ä»¶ 2ï¼šå¸‚å ´ç‹€æ…‹åŠ‡è®Šï¼ˆregime shiftï¼‰
            current_regime = self._get_current_market_regime()
            last_regime = self._get_last_market_regime()
            
            if current_regime != last_regime and last_regime is not None:
                logger.warning(
                    f"âš ï¸ å¸‚å ´ç‹€æ…‹åŠ‡è®Šè§¸ç™¼é‡è¨“ç·´: "
                    f"{last_regime} â†’ {current_regime}"
                )
                self._update_last_market_regime(current_regime)
                return True
            
            # æ¢ä»¶ 3ï¼šç´¯ç©è¶³å¤ æ¨£æœ¬ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
            new_samples = self._count_new_samples()
            if new_samples >= 50:
                logger.info(
                    f"â„¹ï¸ ç´¯ç©æ¨£æœ¬è§¸ç™¼é‡è¨“ç·´: {new_samples} ç­†æ–°äº¤æ˜“"
                )
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"æª¢æŸ¥é‡è¨“ç·´æ¢ä»¶å¤±æ•—: {e}", exc_info=True)
            return False
    
    def _get_recent_trades(self, days: int = 1) -> List[Dict]:
        """
        ç²å–æœ€è¿‘ N å¤©çš„äº¤æ˜“è¨˜éŒ„
        
        Args:
            days: å¤©æ•¸
            
        Returns:
            äº¤æ˜“è¨˜éŒ„åˆ—è¡¨
        """
        try:
            if not self.trade_recorder:
                return []
            
            cutoff_time = datetime.now() - timedelta(days=days)
            all_trades = self.trade_recorder.completed_trades
            
            recent = [
                t for t in all_trades
                if datetime.fromisoformat(t.get('entry_timestamp', '1970-01-01')) > cutoff_time
            ]
            
            return recent
            
        except Exception as e:
            logger.error(f"ç²å–æœ€è¿‘äº¤æ˜“å¤±æ•—: {e}")
            return []
    
    def _calculate_sharpe(self, trades: List[Dict]) -> float:
        """
        è¨ˆç®— Sharpe æ¯”ç‡
        
        Args:
            trades: äº¤æ˜“è¨˜éŒ„åˆ—è¡¨
            
        Returns:
            Sharpe æ¯”ç‡
        """
        try:
            if not trades:
                return 0.0
            
            import numpy as np
            
            returns = [t.get('pnl_pct', 0.0) for t in trades]
            
            if not returns:
                return 0.0
            
            mean_return = float(np.mean(returns))
            std_return = float(np.std(returns))
            
            if std_return == 0:
                return 0.0
            
            sharpe = mean_return / std_return
            
            return float(sharpe)
            
        except Exception as e:
            logger.error(f"è¨ˆç®— Sharpe å¤±æ•—: {e}")
            return 0.0
    
    def _get_historical_sharpe(self) -> float:
        """
        ç²å–æ­·å² Sharpe æ¯”ç‡ï¼ˆéå» 7 å¤©ï¼‰
        
        Returns:
            æ­·å² Sharpe æ¯”ç‡
        """
        try:
            historical_trades = self._get_recent_trades(days=7)
            return self._calculate_sharpe(historical_trades)
        except Exception as e:
            logger.error(f"ç²å–æ­·å² Sharpe å¤±æ•—: {e}")
            return 0.0
    
    def _get_current_market_regime(self) -> str:
        """
        ç²å–ç•¶å‰å¸‚å ´ç‹€æ…‹
        
        Returns:
            'trending', 'choppy', 'volatile', 'calm'
        """
        try:
            # ç°¡åŒ–ç‰ˆï¼šåŸºæ–¼æœ€è¿‘äº¤æ˜“çš„å‹ç‡å’Œæ³¢å‹•æ€§
            recent_trades = self._get_recent_trades(days=1)
            
            if len(recent_trades) < 5:
                return 'unknown'
            
            import numpy as np
            
            # è¨ˆç®—å‹ç‡
            winners = sum(1 for t in recent_trades if t.get('pnl_pct', 0) > 0)
            win_rate = winners / len(recent_trades)
            
            # è¨ˆç®—æ³¢å‹•æ€§
            returns = [t.get('pnl_pct', 0.0) for t in recent_trades]
            volatility = np.std(returns)
            
            # ç°¡å–®åˆ†é¡
            if volatility > 0.05:  # é«˜æ³¢å‹•
                return 'volatile'
            elif win_rate > 0.6:  # é«˜å‹ç‡
                return 'trending'
            elif win_rate < 0.4:  # ä½å‹ç‡
                return 'choppy'
            else:
                return 'calm'
                
        except Exception as e:
            logger.error(f"ç²å–å¸‚å ´ç‹€æ…‹å¤±æ•—: {e}")
            return 'unknown'
    
    def _get_last_market_regime(self) -> Optional[str]:
        """
        ç²å–ä¸Šæ¬¡è¨˜éŒ„çš„å¸‚å ´ç‹€æ…‹
        
        Returns:
            ä¸Šæ¬¡å¸‚å ´ç‹€æ…‹æˆ– None
        """
        try:
            regime_file = self.model_dir / "market_regime.json"
            
            if not regime_file.exists():
                return None
            
            with open(regime_file, 'r') as f:
                data = json.load(f)
                return data.get('regime')
                
        except Exception as e:
            logger.error(f"è®€å–å¸‚å ´ç‹€æ…‹å¤±æ•—: {e}")
            return None
    
    def _update_last_market_regime(self, regime: str):
        """
        æ›´æ–°å¸‚å ´ç‹€æ…‹è¨˜éŒ„
        
        Args:
            regime: æ–°çš„å¸‚å ´ç‹€æ…‹
        """
        try:
            regime_file = self.model_dir / "market_regime.json"
            
            data = {
                'regime': regime,
                'updated_at': datetime.now().isoformat()
            }
            
            with open(regime_file, 'w') as f:
                json.dump(data, f, indent=2)
                
        except Exception as e:
            logger.error(f"æ›´æ–°å¸‚å ´ç‹€æ…‹å¤±æ•—: {e}")
    
    def _count_new_samples(self) -> int:
        """
        è¨ˆç®—è‡ªä¸Šæ¬¡è¨“ç·´ä»¥ä¾†çš„æ–°æ¨£æœ¬æ•¸
        
        Returns:
            æ–°æ¨£æœ¬æ•¸é‡
        """
        try:
            if not self.trade_recorder:
                return 0
            
            # è®€å–ä¸Šæ¬¡è¨“ç·´æ™‚é–“
            if not self.flag_file.exists():
                return 0
            
            with open(self.flag_file, 'r') as f:
                flag_data = json.load(f)
                last_trained = datetime.fromisoformat(flag_data.get('initialized_at', '1970-01-01'))
            
            # è¨ˆç®—æ–°äº¤æ˜“æ•¸
            all_trades = self.trade_recorder.completed_trades
            new_trades = [
                t for t in all_trades
                if datetime.fromisoformat(t.get('entry_timestamp', '1970-01-01')) > last_trained
            ]
            
            return len(new_trades)
            
        except Exception as e:
            logger.error(f"è¨ˆç®—æ–°æ¨£æœ¬æ•¸å¤±æ•—: {e}")
            return 0
