"""
v3.17+ æ¨¡å‹è‡ªå‹•åˆå§‹åŒ–ç³»çµ±
éƒ¨ç½²åˆ° Railway å¾Œç«‹å³è¨“ç·´ï¼Œç„¡éœ€æ‰‹å‹•å¹²é 
"""

import os
import asyncio
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List
from datetime import datetime, timedelta
import json

logger = logging.getLogger(__name__)


class ModelInitializer:
    """
    æ¨¡å‹è‡ªå‹•åˆå§‹åŒ–å™¨ï¼ˆv3.17+ï¼‰
    
    è·è²¬ï¼š
    1. æª¢æ¸¬æ¨¡å‹æ˜¯å¦å­˜åœ¨
    2. è‹¥ç„¡æ¨¡å‹ï¼Œè‡ªå‹•æ”¶é›†è¨“ç·´æ•¸æ“š
    3. ä½¿ç”¨ä¸­æ€§åƒæ•¸è¨“ç·´åˆå§‹æ¨¡å‹
    4. å‰µå»º initialized.flag æ¨™è¨˜
    5. é›¶é…ç½®å•Ÿå‹•ï¼Œç„¡äººå·¥å¹²é 
    """
    
    def __init__(
        self,
        binance_client=None,
        trade_recorder=None,
        config_profile=None
    ):
        """
        åˆå§‹åŒ–æ¨¡å‹åˆå§‹åŒ–å™¨
        
        Args:
            binance_client: BinanceClient å¯¦ä¾‹ï¼ˆå¯é¸ï¼‰
            trade_recorder: TradeRecorder å¯¦ä¾‹ï¼ˆå¯é¸ï¼‰
            config_profile: ConfigProfile å¯¦ä¾‹ï¼ˆå¯é¸ï¼‰
        """
        self.binance = binance_client
        self.trade_recorder = trade_recorder
        self.config = config_profile
        
        # æ¨¡å‹ç›®éŒ„
        self.model_dir = Path("models")
        self.model_dir.mkdir(exist_ok=True)
        
        self.flag_file = self.model_dir / "initialized.flag"
        self.model_file = self.model_dir / "xgboost_model.json"
        
        # è¨“ç·´åƒæ•¸ï¼ˆä¸­æ€§åŒ–ï¼Œé›¶åè¦‹ï¼‰
        self.training_params = {
            # XGBoost ä¸­æ€§åƒæ•¸
            'n_estimators': int(os.getenv("XGBOOST_N_ESTIMATORS", "50")),
            'max_depth': int(os.getenv("XGBOOST_MAX_DEPTH", "4")),
            'learning_rate': float(os.getenv("XGBOOST_LEARNING_RATE", "0.1")),
            'subsample': float(os.getenv("XGBOOST_SUBSAMPLE", "0.8")),
            'colsample_bytree': float(os.getenv("XGBOOST_COLSAMPLE", "0.8")),
            'min_child_weight': int(os.getenv("XGBOOST_MIN_CHILD_WEIGHT", "1")),
            'gamma': float(os.getenv("XGBOOST_GAMMA", "0")),
            
            # è¨“ç·´é…ç½®
            'min_samples': int(os.getenv("INITIAL_TRAINING_SAMPLES", "200")),
            'lookback_days': int(os.getenv("INITIAL_TRAINING_LOOKBACK_DAYS", "30")),
        }
        
        logger.info("=" * 60)
        logger.info("âœ… æ¨¡å‹è‡ªå‹•åˆå§‹åŒ–å™¨å·²å‰µå»ºï¼ˆv3.17+ï¼‰")
        logger.info(f"   ğŸ“ æ¨¡å‹ç›®éŒ„: {self.model_dir}")
        logger.info(f"   ğŸ¯ è¨“ç·´åƒæ•¸: n_estimators={self.training_params['n_estimators']}, "
                   f"max_depth={self.training_params['max_depth']}")
        logger.info("=" * 60)
    
    async def check_and_initialize(self) -> bool:
        """
        æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²åˆå§‹åŒ–ï¼Œè‹¥ç„¡å‰‡è‡ªå‹•è¨“ç·´
        
        Returns:
            æ˜¯å¦å·²åˆå§‹åŒ–ï¼ˆTrue=å·²å­˜åœ¨æˆ–è¨“ç·´æˆåŠŸï¼ŒFalse=è¨“ç·´å¤±æ•—ï¼‰
        """
        # æª¢æŸ¥æ¨™è¨˜æ–‡ä»¶
        if self.flag_file.exists():
            logger.info("âœ… æ¨¡å‹å·²åˆå§‹åŒ–ï¼ˆæª¢æ¸¬åˆ° initialized.flagï¼‰")
            return True
        
        logger.warning("âš ï¸ æœªæª¢æ¸¬åˆ°åˆå§‹åŒ–æ¨¡å‹ï¼Œé–‹å§‹è‡ªå‹•è¨“ç·´...")
        
        # åŸ·è¡Œåˆå§‹è¨“ç·´
        success = await self._initial_training()
        
        if success:
            # å‰µå»ºæ¨™è¨˜æ–‡ä»¶
            self._create_flag_file()
            logger.info("ğŸ‰ æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ç³»çµ±å·²å°±ç·’")
            return True
        else:
            logger.error("âŒ æ¨¡å‹åˆå§‹åŒ–å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ—¥èªŒ")
            return False
    
    async def _initial_training(self) -> bool:
        """
        åŸ·è¡Œåˆå§‹è¨“ç·´
        
        Returns:
            è¨“ç·´æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("ğŸš€ é–‹å§‹æ”¶é›†è¨“ç·´æ•¸æ“š...")
            
            # 1. æ”¶é›†é«˜å“è³ªäº¤æ˜“æ•¸æ“š
            training_data = await self._collect_training_data()
            
            if not training_data or len(training_data) < self.training_params['min_samples']:
                logger.error(
                    f"âŒ è¨“ç·´æ•¸æ“šä¸è¶³: {len(training_data) if training_data else 0} "
                    f"< {self.training_params['min_samples']}"
                )
                return False
            
            logger.info(f"âœ… æ”¶é›†åˆ° {len(training_data)} ç­†è¨“ç·´æ•¸æ“š")
            
            # 2. è¨“ç·´åˆå§‹æ¨¡å‹
            logger.info("ğŸ§  é–‹å§‹è¨“ç·´ XGBoost æ¨¡å‹...")
            model_success = await self._train_xgboost_model(training_data)
            
            if not model_success:
                logger.error("âŒ XGBoost æ¨¡å‹è¨“ç·´å¤±æ•—")
                return False
            
            logger.info("âœ… XGBoost æ¨¡å‹è¨“ç·´å®Œæˆ")
            
            # 3. åˆå§‹åŒ–ç‰¹å¾µæ¬Šé‡ï¼ˆç„¡å…ˆé©—åå¥½ï¼‰
            self._initialize_feature_weights()
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹è¨“ç·´å¤±æ•—: {e}", exc_info=True)
            return False
    
    async def _collect_training_data(self) -> List[Dict[str, Any]]:
        """
        æ”¶é›†è¨“ç·´æ•¸æ“š
        
        ç­–ç•¥ï¼š
        1. å„ªå…ˆä½¿ç”¨æ­·å²äº¤æ˜“è¨˜éŒ„ï¼ˆè‹¥æœ‰ TradeRecorderï¼‰
        2. å¦å‰‡ä½¿ç”¨å¸‚å ´æ•¸æ“šç”Ÿæˆåˆæˆæ¨£æœ¬
        
        Returns:
            è¨“ç·´æ•¸æ“šåˆ—è¡¨
        """
        training_data = []
        
        # ç­–ç•¥ 1: å¾äº¤æ˜“è¨˜éŒ„æ”¶é›†
        if self.trade_recorder:
            try:
                cutoff_date = datetime.now() - timedelta(days=self.training_params['lookback_days'])
                
                # ç²å–æ‰€æœ‰äº¤æ˜“è¨˜éŒ„
                all_trades = await self._get_historical_trades()
                
                # éæ¿¾é«˜å“è³ªäº¤æ˜“ï¼ˆå·²å¹³å€‰ï¼Œæœ‰æ˜ç¢ºçµæœï¼‰
                quality_trades = [
                    t for t in all_trades
                    if t.get('status') == 'closed' and
                       t.get('pnl') is not None and
                       datetime.fromisoformat(str(t.get('timestamp', cutoff_date))) >= cutoff_date
                ]
                
                if quality_trades:
                    logger.info(f"ğŸ“Š å¾äº¤æ˜“è¨˜éŒ„æ”¶é›†åˆ° {len(quality_trades)} ç­†é«˜å“è³ªæ¨£æœ¬")
                    training_data.extend(quality_trades)
                
            except Exception as e:
                logger.warning(f"âš ï¸ ç„¡æ³•å¾äº¤æ˜“è¨˜éŒ„æ”¶é›†æ•¸æ“š: {e}")
        
        # ç­–ç•¥ 2: ç”Ÿæˆåˆæˆæ¨£æœ¬ï¼ˆè‹¥æ•¸æ“šä¸è¶³ï¼‰
        if len(training_data) < self.training_params['min_samples']:
            logger.info("ğŸ“Š å¾å¸‚å ´æ•¸æ“šç”Ÿæˆåˆæˆæ¨£æœ¬...")
            synthetic_samples = await self._generate_synthetic_samples(
                target_count=self.training_params['min_samples'] - len(training_data)
            )
            training_data.extend(synthetic_samples)
        
        return training_data
    
    async def _get_historical_trades(self) -> List[Dict[str, Any]]:
        """ç²å–æ­·å²äº¤æ˜“è¨˜éŒ„"""
        try:
            if self.trade_recorder is None:
                return []
            
            if hasattr(self.trade_recorder, 'get_all_trades'):
                return await self.trade_recorder.get_all_trades()
            elif hasattr(self.trade_recorder, 'get_closed_trades'):
                return await self.trade_recorder.get_closed_trades()
            else:
                return []
        except Exception as e:
            logger.error(f"âŒ ç²å–æ­·å²äº¤æ˜“å¤±æ•—: {e}")
            return []
    
    async def _generate_synthetic_samples(self, target_count: int) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆåˆæˆè¨“ç·´æ¨£æœ¬
        
        ç­–ç•¥ï¼šå¾å¯¦æ™‚å¸‚å ´æ•¸æ“šæå–ç‰¹å¾µï¼Œä½¿ç”¨ç°¡å–®è¦å‰‡æ¨™è¨»
        
        Args:
            target_count: ç›®æ¨™æ¨£æœ¬æ•¸é‡
            
        Returns:
            åˆæˆæ¨£æœ¬åˆ—è¡¨
        """
        synthetic_samples = []
        
        if not self.binance:
            logger.warning("âš ï¸ ç„¡ BinanceClientï¼Œç„¡æ³•ç”Ÿæˆåˆæˆæ¨£æœ¬")
            return synthetic_samples
        
        try:
            # ç²å–ç†±é–€äº¤æ˜“å°
            symbols = await self._get_top_symbols(limit=20)
            
            logger.info(f"ğŸ“Š å¾ {len(symbols)} å€‹äº¤æ˜“å°ç”Ÿæˆåˆæˆæ¨£æœ¬...")
            
            # ç‚ºæ¯å€‹äº¤æ˜“å°ç”Ÿæˆæ¨£æœ¬
            samples_per_symbol = max(1, target_count // len(symbols))
            
            for symbol in symbols:
                try:
                    # ç²å– K ç·šæ•¸æ“šï¼ˆ1 å°æ™‚ï¼Œæœ€è¿‘ 200 æ ¹ï¼‰
                    klines = await self.binance.get_klines(
                        symbol=symbol,
                        interval='1h',
                        limit=200
                    )
                    
                    if not klines or len(klines) < 100:
                        continue
                    
                    # ç”Ÿæˆç‰¹å¾µä¸¦æ¨™è¨»
                    samples = self._extract_features_and_label(klines, samples_per_symbol)
                    synthetic_samples.extend(samples)
                    
                    if len(synthetic_samples) >= target_count:
                        break
                        
                except Exception as e:
                    logger.debug(f"âš ï¸ {symbol} ç”Ÿæˆæ¨£æœ¬å¤±æ•—: {e}")
                    continue
            
            logger.info(f"âœ… ç”Ÿæˆ {len(synthetic_samples)} ç­†åˆæˆæ¨£æœ¬")
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆåˆæˆæ¨£æœ¬å¤±æ•—: {e}")
        
        return synthetic_samples[:target_count]
    
    async def _get_top_symbols(self, limit: int = 20) -> List[str]:
        """ç²å–ç†±é–€äº¤æ˜“å°"""
        try:
            if self.binance is None:
                return ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'SOLUSDT', 'ADAUSDT']
            
            # ç²å– 24h äº¤æ˜“é‡æ’å
            tickers = await self.binance.get_24h_tickers()
            
            # éæ¿¾ USDT åˆç´„ï¼ŒæŒ‰äº¤æ˜“é‡æ’åº
            usdt_tickers = [
                t for t in tickers
                if t['symbol'].endswith('USDT')
            ]
            
            sorted_tickers = sorted(
                usdt_tickers,
                key=lambda x: float(x.get('quoteVolume', 0)),
                reverse=True
            )
            
            return [t['symbol'] for t in sorted_tickers[:limit]]
            
        except Exception as e:
            logger.error(f"âŒ ç²å–ç†±é–€äº¤æ˜“å°å¤±æ•—: {e}")
            # è¿”å›é»˜èªåˆ—è¡¨
            return ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'SOLUSDT', 'ADAUSDT']
    
    def _extract_features_and_label(
        self,
        klines: List[Dict],
        max_samples: int
    ) -> List[Dict[str, Any]]:
        """
        å¾ K ç·šæå–ç‰¹å¾µä¸¦æ¨™è¨»
        
        ç°¡å–®è¦å‰‡ï¼š
        - ä¸Šæ¼²è¶¨å‹¢ï¼ˆ20 EMA ä¸Šç©¿ 50 EMAï¼‰â†’ æ­£æ¨£æœ¬
        - ä¸‹è·Œè¶¨å‹¢ï¼ˆ20 EMA ä¸‹ç©¿ 50 EMAï¼‰â†’ è² æ¨£æœ¬
        
        Args:
            klines: K ç·šæ•¸æ“š
            max_samples: æœ€å¤§æ¨£æœ¬æ•¸
            
        Returns:
            ç‰¹å¾µæ¨£æœ¬åˆ—è¡¨
        """
        samples = []
        
        try:
            import pandas as pd
            import numpy as np
            
            # è½‰æ›ç‚º DataFrame
            df = pd.DataFrame(klines)
            df['close'] = df['close'].astype(float)
            df['high'] = df['high'].astype(float)
            df['low'] = df['low'].astype(float)
            df['volume'] = df['volume'].astype(float)
            
            # è¨ˆç®— EMA
            df['ema_20'] = df['close'].ewm(span=20, adjust=False).mean()
            df['ema_50'] = df['close'].ewm(span=50, adjust=False).mean()
            
            # è¨ˆç®— RSI
            delta = df['close'].diff()
            gain = delta.where(delta > 0, 0).rolling(window=14).mean()
            loss = -delta.where(delta < 0, 0).rolling(window=14).mean()
            rs = gain / loss
            df['rsi'] = 100 - (100 / (1 + rs))
            
            # è¨ˆç®— ATR
            high_low = df['high'] - df['low']
            high_close = abs(df['high'] - df['close'].shift())
            low_close = abs(df['low'] - df['close'].shift())
            ranges = pd.concat([high_low, high_close, low_close], axis=1)
            df['atr'] = ranges.max(axis=1).rolling(window=14).mean()
            
            # ç”Ÿæˆæ¨£æœ¬ï¼ˆé¸æ“‡è¶¨å‹¢æ˜ç¢ºçš„é»ï¼‰
            for i in range(60, len(df) - 10, 10):  # æ¯ 10 æ ¹ K ç·šå–ä¸€å€‹æ¨£æœ¬
                if len(samples) >= max_samples:
                    break
                
                row = df.iloc[i]
                
                # ç‰¹å¾µ
                features = {
                    'ema_20': row['ema_20'],
                    'ema_50': row['ema_50'],
                    'rsi': row['rsi'],
                    'atr': row['atr'],
                    'volume': row['volume'],
                    'close': row['close'],
                }
                
                # æ¨™è¨»ï¼ˆç°¡å–®è¦å‰‡ï¼‰
                ema_diff = row['ema_20'] - row['ema_50']
                future_return = (df.iloc[i + 10]['close'] - row['close']) / row['close']
                
                # æ­£æ¨£æœ¬ï¼šEMA å¤šé ­æ’åˆ— ä¸” æœªä¾†ä¸Šæ¼²
                # è² æ¨£æœ¬ï¼šEMA ç©ºé ­æ’åˆ— ä¸” æœªä¾†ä¸‹è·Œ
                if ema_diff > 0 and future_return > 0.01:
                    label = 1  # å‹åˆ©
                elif ema_diff < 0 and future_return < -0.01:
                    label = 1  # å‹åˆ©ï¼ˆç©ºé ­åˆ¤æ–·æ­£ç¢ºï¼‰
                else:
                    label = 0  # å¤±æ•—
                
                samples.append({
                    'features': features,
                    'label': label,
                    'pnl': future_return,
                })
        
        except Exception as e:
            logger.error(f"âŒ æå–ç‰¹å¾µå¤±æ•—: {e}")
        
        return samples
    
    async def _train_xgboost_model(self, training_data: List[Dict]) -> bool:
        """
        è¨“ç·´ XGBoost æ¨¡å‹
        
        Args:
            training_data: è¨“ç·´æ•¸æ“š
            
        Returns:
            è¨“ç·´æ˜¯å¦æˆåŠŸ
        """
        try:
            import xgboost as xgb
            import numpy as np
            
            # æå–ç‰¹å¾µå’Œæ¨™ç±¤
            X = []
            y = []
            
            for sample in training_data:
                if 'features' in sample and 'label' in sample:
                    features = sample['features']
                    # è½‰æ›ç‚ºæ•¸å€¼åˆ—è¡¨
                    feature_vector = [
                        float(features.get('ema_20', 0)),
                        float(features.get('ema_50', 0)),
                        float(features.get('rsi', 50)),
                        float(features.get('atr', 0)),
                        float(features.get('volume', 0)),
                    ]
                    X.append(feature_vector)
                    y.append(int(sample['label']))
            
            if len(X) < 10:
                logger.error(f"âŒ ç‰¹å¾µæ•¸æ“šä¸è¶³: {len(X)} < 10")
                return False
            
            X = np.array(X)
            y = np.array(y)
            
            logger.info(f"ğŸ“Š è¨“ç·´æ•¸æ“š: X.shape={X.shape}, y.shape={y.shape}")
            
            # å‰µå»º DMatrix
            dtrain = xgb.DMatrix(X, label=y)
            
            # è¨“ç·´åƒæ•¸ï¼ˆä¸­æ€§åŒ–ï¼‰
            params = {
                'objective': 'binary:logistic',
                'eval_metric': 'logloss',
                'max_depth': self.training_params['max_depth'],
                'learning_rate': self.training_params['learning_rate'],
                'subsample': self.training_params['subsample'],
                'colsample_bytree': self.training_params['colsample_bytree'],
                'min_child_weight': self.training_params['min_child_weight'],
                'gamma': self.training_params['gamma'],
                'seed': 42,
            }
            
            # è¨“ç·´æ¨¡å‹
            logger.info(f"ğŸ§  é–‹å§‹è¨“ç·´: {self.training_params['n_estimators']} æ£µæ¨¹...")
            
            model = xgb.train(
                params,
                dtrain,
                num_boost_round=self.training_params['n_estimators'],
                verbose_eval=False
            )
            
            # ä¿å­˜æ¨¡å‹
            model.save_model(str(self.model_file))
            
            # æª¢æŸ¥æ¨¡å‹å¤§å°
            model_size = os.path.getsize(self.model_file) / 1024
            logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {self.model_file} ({model_size:.2f} KB)")
            
            if model_size > 100:
                logger.warning(f"âš ï¸ æ¨¡å‹è¼ƒå¤§ ({model_size:.2f} KB)ï¼Œå»ºè­°é‡åŒ–")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ XGBoost è¨“ç·´å¤±æ•—: {e}", exc_info=True)
            return False
    
    def _initialize_feature_weights(self):
        """åˆå§‹åŒ–ç‰¹å¾µæ¬Šé‡ï¼ˆç„¡å…ˆé©—åå¥½ï¼‰"""
        weights_file = self.model_dir / "feature_weights.json"
        
        # æ‰€æœ‰ç‰¹å¾µæ¬Šé‡è¨­ç‚º 1.0ï¼ˆç„¡åå¥½ï¼‰
        default_weights = {
            'ema_20': 1.0,
            'ema_50': 1.0,
            'rsi': 1.0,
            'atr': 1.0,
            'volume': 1.0,
            'adx': 1.0,
            'macd': 1.0,
        }
        
        with open(weights_file, 'w') as f:
            json.dump(default_weights, f, indent=2)
        
        logger.info(f"âœ… ç‰¹å¾µæ¬Šé‡å·²åˆå§‹åŒ–: {weights_file}")
    
    def _create_flag_file(self):
        """å‰µå»ºåˆå§‹åŒ–æ¨™è¨˜æ–‡ä»¶"""
        flag_data = {
            'initialized_at': datetime.now().isoformat(),
            'training_params': self.training_params,
            'model_file': str(self.model_file),
            'version': 'v3.17+',
        }
        
        with open(self.flag_file, 'w') as f:
            json.dump(flag_data, f, indent=2)
        
        logger.info(f"âœ… åˆå§‹åŒ–æ¨™è¨˜å·²å‰µå»º: {self.flag_file}")
    
    async def force_retrain(self) -> bool:
        """
        å¼·åˆ¶é‡æ–°è¨“ç·´ï¼ˆåˆªé™¤æ¨™è¨˜æ–‡ä»¶ä¸¦é‡æ–°åˆå§‹åŒ–ï¼‰
        
        Returns:
            é‡æ–°è¨“ç·´æ˜¯å¦æˆåŠŸ
        """
        logger.warning("âš ï¸ å¼·åˆ¶é‡æ–°è¨“ç·´æ¨¡å‹...")
        
        # åˆªé™¤æ¨™è¨˜æ–‡ä»¶
        if self.flag_file.exists():
            self.flag_file.unlink()
            logger.info("ğŸ—‘ï¸ å·²åˆªé™¤ initialized.flag")
        
        # åˆªé™¤èˆŠæ¨¡å‹
        if self.model_file.exists():
            self.model_file.unlink()
            logger.info("ğŸ—‘ï¸ å·²åˆªé™¤èˆŠæ¨¡å‹æ–‡ä»¶")
        
        # é‡æ–°åˆå§‹åŒ–
        return await self.check_and_initialize()
    
    def should_retrain(self) -> bool:
        """
        å‹•æ…‹é‡è¨“ç·´è§¸ç™¼æ¢ä»¶ï¼ˆv3.17.10+ï¼‰
        
        è§£æ±ºã€Œå¸‚å ´é©æ‡‰æ…¢ã€å•é¡Œï¼š
        - å›ºå®š 50 ç­†è§¸ç™¼ç„¡æ³•æ‡‰å°å¸‚å ´ regime shift
        - å¾ trending â†’ choppy è½‰æ›æ™‚éœ€è¦ç«‹å³é‡è¨“ç·´
        
        Returns:
            æ˜¯å¦æ‡‰è©²é‡è¨“ç·´
        """
        try:
            # æ¢ä»¶ 1ï¼šæ€§èƒ½é©Ÿé™ï¼ˆSharpe æ¯”ç‡ä¸‹é™ 50%ï¼‰
            recent_trades = self._get_recent_trades(days=1)
            
            if len(recent_trades) >= 10:
                current_sharpe = self._calculate_sharpe(recent_trades)
                historical_sharpe = self._get_historical_sharpe()
                
                if historical_sharpe > 0 and current_sharpe < historical_sharpe * 0.5:
                    logger.warning(
                        f"âš ï¸ æ€§èƒ½é©Ÿé™è§¸ç™¼é‡è¨“ç·´: "
                        f"ç•¶å‰ Sharpe={current_sharpe:.2f} "
                        f"æ­·å² Sharpe={historical_sharpe:.2f} "
                        f"(ä¸‹é™ {(1 - current_sharpe/historical_sharpe)*100:.1f}%)"
                    )
                    return True
            
            # æ¢ä»¶ 2ï¼šå¸‚å ´ç‹€æ…‹åŠ‡è®Šï¼ˆregime shiftï¼‰
            current_regime = self._get_current_market_regime()
            last_regime = self._get_last_market_regime()
            
            if current_regime != last_regime and last_regime is not None:
                logger.warning(
                    f"âš ï¸ å¸‚å ´ç‹€æ…‹åŠ‡è®Šè§¸ç™¼é‡è¨“ç·´: "
                    f"{last_regime} â†’ {current_regime}"
                )
                self._update_last_market_regime(current_regime)
                return True
            
            # æ¢ä»¶ 3ï¼šç´¯ç©è¶³å¤ æ¨£æœ¬ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
            new_samples = self._count_new_samples()
            if new_samples >= 50:
                logger.info(
                    f"â„¹ï¸ ç´¯ç©æ¨£æœ¬è§¸ç™¼é‡è¨“ç·´: {new_samples} ç­†æ–°äº¤æ˜“"
                )
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"æª¢æŸ¥é‡è¨“ç·´æ¢ä»¶å¤±æ•—: {e}", exc_info=True)
            return False
    
    def _get_recent_trades(self, days: int = 1) -> List[Dict]:
        """
        ç²å–æœ€è¿‘ N å¤©çš„äº¤æ˜“è¨˜éŒ„
        
        Args:
            days: å¤©æ•¸
            
        Returns:
            äº¤æ˜“è¨˜éŒ„åˆ—è¡¨
        """
        try:
            if not self.trade_recorder:
                return []
            
            cutoff_time = datetime.now() - timedelta(days=days)
            all_trades = self.trade_recorder.completed_trades
            
            recent = [
                t for t in all_trades
                if datetime.fromisoformat(t.get('entry_timestamp', '1970-01-01')) > cutoff_time
            ]
            
            return recent
            
        except Exception as e:
            logger.error(f"ç²å–æœ€è¿‘äº¤æ˜“å¤±æ•—: {e}")
            return []
    
    def _calculate_sharpe(self, trades: List[Dict]) -> float:
        """
        è¨ˆç®— Sharpe æ¯”ç‡
        
        Args:
            trades: äº¤æ˜“è¨˜éŒ„åˆ—è¡¨
            
        Returns:
            Sharpe æ¯”ç‡
        """
        try:
            if not trades:
                return 0.0
            
            import numpy as np
            
            returns = [t.get('pnl_pct', 0.0) for t in trades]
            
            if not returns:
                return 0.0
            
            mean_return = np.mean(returns)
            std_return = np.std(returns)
            
            if std_return == 0:
                return 0.0
            
            sharpe = mean_return / std_return
            
            return sharpe
            
        except Exception as e:
            logger.error(f"è¨ˆç®— Sharpe å¤±æ•—: {e}")
            return 0.0
    
    def _get_historical_sharpe(self) -> float:
        """
        ç²å–æ­·å² Sharpe æ¯”ç‡ï¼ˆéå» 7 å¤©ï¼‰
        
        Returns:
            æ­·å² Sharpe æ¯”ç‡
        """
        try:
            historical_trades = self._get_recent_trades(days=7)
            return self._calculate_sharpe(historical_trades)
        except Exception as e:
            logger.error(f"ç²å–æ­·å² Sharpe å¤±æ•—: {e}")
            return 0.0
    
    def _get_current_market_regime(self) -> str:
        """
        ç²å–ç•¶å‰å¸‚å ´ç‹€æ…‹
        
        Returns:
            'trending', 'choppy', 'volatile', 'calm'
        """
        try:
            # ç°¡åŒ–ç‰ˆï¼šåŸºæ–¼æœ€è¿‘äº¤æ˜“çš„å‹ç‡å’Œæ³¢å‹•æ€§
            recent_trades = self._get_recent_trades(days=1)
            
            if len(recent_trades) < 5:
                return 'unknown'
            
            import numpy as np
            
            # è¨ˆç®—å‹ç‡
            winners = sum(1 for t in recent_trades if t.get('pnl_pct', 0) > 0)
            win_rate = winners / len(recent_trades)
            
            # è¨ˆç®—æ³¢å‹•æ€§
            returns = [t.get('pnl_pct', 0.0) for t in recent_trades]
            volatility = np.std(returns)
            
            # ç°¡å–®åˆ†é¡
            if volatility > 0.05:  # é«˜æ³¢å‹•
                return 'volatile'
            elif win_rate > 0.6:  # é«˜å‹ç‡
                return 'trending'
            elif win_rate < 0.4:  # ä½å‹ç‡
                return 'choppy'
            else:
                return 'calm'
                
        except Exception as e:
            logger.error(f"ç²å–å¸‚å ´ç‹€æ…‹å¤±æ•—: {e}")
            return 'unknown'
    
    def _get_last_market_regime(self) -> Optional[str]:
        """
        ç²å–ä¸Šæ¬¡è¨˜éŒ„çš„å¸‚å ´ç‹€æ…‹
        
        Returns:
            ä¸Šæ¬¡å¸‚å ´ç‹€æ…‹æˆ– None
        """
        try:
            regime_file = self.model_dir / "market_regime.json"
            
            if not regime_file.exists():
                return None
            
            with open(regime_file, 'r') as f:
                data = json.load(f)
                return data.get('regime')
                
        except Exception as e:
            logger.error(f"è®€å–å¸‚å ´ç‹€æ…‹å¤±æ•—: {e}")
            return None
    
    def _update_last_market_regime(self, regime: str):
        """
        æ›´æ–°å¸‚å ´ç‹€æ…‹è¨˜éŒ„
        
        Args:
            regime: æ–°çš„å¸‚å ´ç‹€æ…‹
        """
        try:
            regime_file = self.model_dir / "market_regime.json"
            
            data = {
                'regime': regime,
                'updated_at': datetime.now().isoformat()
            }
            
            with open(regime_file, 'w') as f:
                json.dump(data, f, indent=2)
                
        except Exception as e:
            logger.error(f"æ›´æ–°å¸‚å ´ç‹€æ…‹å¤±æ•—: {e}")
    
    def _count_new_samples(self) -> int:
        """
        è¨ˆç®—è‡ªä¸Šæ¬¡è¨“ç·´ä»¥ä¾†çš„æ–°æ¨£æœ¬æ•¸
        
        Returns:
            æ–°æ¨£æœ¬æ•¸é‡
        """
        try:
            if not self.trade_recorder:
                return 0
            
            # è®€å–ä¸Šæ¬¡è¨“ç·´æ™‚é–“
            if not self.flag_file.exists():
                return 0
            
            with open(self.flag_file, 'r') as f:
                flag_data = json.load(f)
                last_trained = datetime.fromisoformat(flag_data.get('initialized_at', '1970-01-01'))
            
            # è¨ˆç®—æ–°äº¤æ˜“æ•¸
            all_trades = self.trade_recorder.completed_trades
            new_trades = [
                t for t in all_trades
                if datetime.fromisoformat(t.get('entry_timestamp', '1970-01-01')) > last_trained
            ]
            
            return len(new_trades)
            
        except Exception as e:
            logger.error(f"è¨ˆç®—æ–°æ¨£æœ¬æ•¸å¤±æ•—: {e}")
            return 0
