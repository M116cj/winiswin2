"""
æ¨¡å‹æ¼‚ç§»æ£€æµ‹å™¨
èŒè´£ï¼šç›‘æ§ç‰¹å¾åˆ†å¸ƒæ¼‚ç§»ã€è§¦å‘é‡è®­ç»ƒã€æ»‘åŠ¨çª—å£ç®¡ç†
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
import logging
from datetime import datetime, timedelta
from scipy import stats
import json
import os

logger = logging.getLogger(__name__)


class DriftDetector:
    """æ¨¡å‹æ¼‚ç§»æ£€æµ‹å™¨"""
    
    def __init__(self, window_size: int = 1000, drift_threshold: float = 0.05):
        """
        åˆå§‹åŒ–æ¼‚ç§»æ£€æµ‹å™¨
        
        Args:
            window_size: æ»‘åŠ¨çª—å£å¤§å°ï¼ˆä¿ç•™æœ€è¿‘Nç¬”æ•°æ®ï¼‰
            drift_threshold: KSæ£€éªŒpå€¼é˜ˆå€¼ï¼ˆ<thresholdåˆ™è®¤ä¸ºæ¼‚ç§»ï¼‰
        """
        self.window_size = window_size
        self.drift_threshold = drift_threshold
        self.baseline_stats = {}  # åŸºå‡†ç‰¹å¾ç»Ÿè®¡
        self.drift_history = []  # æ¼‚ç§»å†å²è®°å½•
        self.stats_path = "data/models/baseline_stats.json"
        
        os.makedirs(os.path.dirname(self.stats_path), exist_ok=True)
        self._load_baseline_stats()
    
    def apply_sliding_window(
        self,
        df: pd.DataFrame,
        window_size: Optional[int] = None
    ) -> pd.DataFrame:
        """
        åº”ç”¨æ»‘åŠ¨çª—å£ï¼ˆåªä¿ç•™æœ€è¿‘Nç¬”æ•°æ®ï¼‰
        
        Args:
            df: å®Œæ•´æ•°æ®é›†
            window_size: çª—å£å¤§å°ï¼ˆé»˜è®¤ä½¿ç”¨self.window_sizeï¼‰
        
        Returns:
            pd.DataFrame: çª—å£å†…æ•°æ®
        """
        if window_size is None:
            window_size = self.window_size
        
        original_size = len(df)
        
        if original_size <= window_size:
            logger.info(f"ğŸ“Š æ•°æ®é‡{original_size} <= çª—å£{window_size}ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®")
            return df
        
        # æŒ‰æ—¶é—´æ’åºï¼ˆå‡è®¾æœ‰timestampå­—æ®µï¼‰
        if 'timestamp' in df.columns:
            df_sorted = df.sort_values('timestamp', ascending=False)
        elif 'entry_time' in df.columns:
            df_sorted = df.sort_values('entry_time', ascending=False)
        else:
            # æ²¡æœ‰æ—¶é—´å­—æ®µï¼Œä½¿ç”¨æœ€åNè¡Œ
            df_sorted = df
        
        # å–æœ€è¿‘Nç¬”
        windowed_df = df_sorted.head(window_size).copy()
        
        logger.info(
            f"ğŸ“Š åº”ç”¨æ»‘åŠ¨çª—å£ï¼šä¿ç•™æœ€è¿‘{window_size}ç¬”æ•°æ®ï¼Œ"
            f"ä¸¢å¼ƒ{original_size - window_size}ç¬”æ—§æ•°æ®"
        )
        
        return windowed_df
    
    def calculate_sample_weights(
        self,
        df: pd.DataFrame,
        decay_factor: float = 0.95
    ) -> np.ndarray:
        """
        è®¡ç®—åŠ¨æ€æ ·æœ¬æƒé‡ï¼ˆæ–°æ ·æœ¬æƒé‡ > æ—§æ ·æœ¬ï¼‰
        
        ä½¿ç”¨æŒ‡æ•°è¡°å‡ï¼šweight = decay_factor ^ age
        
        Args:
            df: æ•°æ®é›†
            decay_factor: è¡°å‡å› å­ï¼ˆ0-1ï¼‰ï¼Œè¶Šæ¥è¿‘1è¡°å‡è¶Šæ…¢
        
        Returns:
            np.ndarray: æ ·æœ¬æƒé‡
        """
        n_samples = len(df)
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¹´é¾„ï¼ˆ0=æœ€æ–°ï¼Œn-1=æœ€æ—§ï¼‰
        if 'timestamp' in df.columns or 'entry_time' in df.columns:
            time_col = 'timestamp' if 'timestamp' in df.columns else 'entry_time'
            df_sorted = df.sort_values(time_col, ascending=False)
            ages = np.arange(n_samples)
        else:
            # æ²¡æœ‰æ—¶é—´å­—æ®µï¼Œå‡è®¾åé¢çš„è¡Œæ›´æ–°
            ages = np.arange(n_samples)[::-1]
        
        # æŒ‡æ•°è¡°å‡æƒé‡
        weights = decay_factor ** ages
        
        # å½’ä¸€åŒ–ï¼ˆä½¿æ€»æƒé‡ = n_samplesï¼‰
        weights = weights / weights.sum() * n_samples
        
        logger.info(
            f"ğŸ“Š åŠ¨æ€æƒé‡ï¼šæœ€æ–°æ ·æœ¬æƒé‡{weights[0]:.2f}ï¼Œ"
            f"æœ€æ—§æ ·æœ¬æƒé‡{weights[-1]:.2f}ï¼Œ"
            f"è¡°å‡å› å­{decay_factor}"
        )
        
        return weights
    
    def detect_feature_drift(
        self,
        current_data: pd.DataFrame,
        feature_columns: List[str],
        update_baseline: bool = False
    ) -> Dict:
        """
        æ£€æµ‹ç‰¹å¾åˆ†å¸ƒæ¼‚ç§»ï¼ˆä½¿ç”¨KSæ£€éªŒï¼‰
        
        Args:
            current_data: å½“å‰æ•°æ®
            feature_columns: è¦æ£€æµ‹çš„ç‰¹å¾åˆ—è¡¨
            update_baseline: æ˜¯å¦æ›´æ–°åŸºå‡†ç»Ÿè®¡
        
        Returns:
            Dict: æ¼‚ç§»æ£€æµ‹æŠ¥å‘Š
        """
        if not self.baseline_stats:
            logger.info("ğŸ“Š é¦–æ¬¡æ£€æµ‹ï¼Œå»ºç«‹åŸºå‡†ç»Ÿè®¡")
            self._build_baseline_stats(current_data, feature_columns)
            return {
                'has_drift': False,
                'reason': 'é¦–æ¬¡å»ºç«‹åŸºå‡†',
                'drifted_features': []
            }
        
        drifted_features = []
        drift_details = {}
        
        for feature in feature_columns:
            if feature not in current_data.columns:
                continue
            
            if feature not in self.baseline_stats:
                continue
            
            # è·å–å½“å‰æ•°æ®
            current_values = current_data[feature].dropna().values
            
            if len(current_values) < 30:  # æ ·æœ¬å¤ªå°‘ï¼Œè·³è¿‡
                continue
            
            # è·å–åŸºå‡†ç»Ÿè®¡
            baseline = self.baseline_stats[feature]
            
            # KSæ£€éªŒï¼ˆæ¯”è¾ƒåˆ†å¸ƒï¼‰
            ks_stat, p_value = self._ks_test(
                current_values,
                baseline['mean'],
                baseline['std']
            )
            
            # åˆ¤æ–­æ˜¯å¦æ¼‚ç§»
            if p_value < self.drift_threshold:
                drifted_features.append(feature)
                drift_details[feature] = {
                    'ks_statistic': float(ks_stat),
                    'p_value': float(p_value),
                    'baseline_mean': baseline['mean'],
                    'current_mean': float(np.mean(current_values)),
                    'baseline_std': baseline['std'],
                    'current_std': float(np.std(current_values))
                }
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'has_drift': len(drifted_features) > 0,
            'drifted_features': drifted_features,
            'drift_details': drift_details,
            'total_features_checked': len(feature_columns),
            'drift_threshold': self.drift_threshold
        }
        
        # è®°å½•æ¼‚ç§»å†å²
        self.drift_history.append(report)
        
        if report['has_drift']:
            logger.warning(
                f"âš ï¸ æ£€æµ‹åˆ°ç‰¹å¾åˆ†å¸ƒæ¼‚ç§»ï¼"
                f"æ¼‚ç§»ç‰¹å¾ï¼š{drifted_features}"
            )
            
            # å¦‚æœæ¼‚ç§»ä¸¥é‡ï¼ˆ>30%ç‰¹å¾æ¼‚ç§»ï¼‰ï¼Œå»ºè®®å®Œæ•´é‡è®­ç»ƒ
            drift_ratio = len(drifted_features) / len(feature_columns)
            if drift_ratio > 0.3:
                report['recommendation'] = 'full_retrain'
                logger.warning(
                    f"âš ï¸ æ¼‚ç§»ä¸¥é‡ï¼ˆ{drift_ratio:.1%}ç‰¹å¾æ¼‚ç§»ï¼‰ï¼Œ"
                    f"å»ºè®®å®Œæ•´é‡è®­ç»ƒ"
                )
            else:
                report['recommendation'] = 'incremental_retrain'
        else:
            logger.info("âœ… æœªæ£€æµ‹åˆ°ç‰¹å¾åˆ†å¸ƒæ¼‚ç§»")
            report['recommendation'] = 'continue'
        
        # æ›´æ–°åŸºå‡†ç»Ÿè®¡
        if update_baseline:
            self._build_baseline_stats(current_data, feature_columns)
        
        return report
    
    def _ks_test(
        self,
        current_values: np.ndarray,
        baseline_mean: float,
        baseline_std: float
    ) -> Tuple[float, float]:
        """
        Kolmogorov-Smirnovæ£€éªŒ
        
        æ¯”è¾ƒå½“å‰åˆ†å¸ƒä¸åŸºå‡†æ­£æ€åˆ†å¸ƒçš„å·®å¼‚
        
        Args:
            current_values: å½“å‰æ•°æ®
            baseline_mean: åŸºå‡†å‡å€¼
            baseline_std: åŸºå‡†æ ‡å‡†å·®
        
        Returns:
            Tuple[float, float]: (KSç»Ÿè®¡é‡, på€¼)
        """
        # ç”ŸæˆåŸºå‡†æ­£æ€åˆ†å¸ƒ
        baseline_dist = stats.norm(loc=baseline_mean, scale=baseline_std)
        
        # KSæ£€éªŒ
        ks_stat, p_value = stats.kstest(current_values, baseline_dist.cdf)
        
        return ks_stat, p_value
    
    def _build_baseline_stats(
        self,
        df: pd.DataFrame,
        feature_columns: List[str]
    ):
        """
        å»ºç«‹åŸºå‡†ç‰¹å¾ç»Ÿè®¡
        
        Args:
            df: æ•°æ®é›†
            feature_columns: ç‰¹å¾åˆ—è¡¨
        """
        self.baseline_stats = {}
        
        for feature in feature_columns:
            if feature not in df.columns:
                continue
            
            values = df[feature].dropna().values
            
            if len(values) < 10:  # æ ·æœ¬å¤ªå°‘ï¼Œè·³è¿‡
                continue
            
            self.baseline_stats[feature] = {
                'mean': float(np.mean(values)),
                'std': float(np.std(values)),
                'min': float(np.min(values)),
                'max': float(np.max(values)),
                'median': float(np.median(values)),
                'sample_count': len(values)
            }
        
        # ä¿å­˜åŸºå‡†ç»Ÿè®¡
        self._save_baseline_stats()
        
        logger.info(f"âœ… å»ºç«‹åŸºå‡†ç»Ÿè®¡ï¼š{len(self.baseline_stats)}ä¸ªç‰¹å¾")
    
    def _save_baseline_stats(self):
        """ä¿å­˜åŸºå‡†ç»Ÿè®¡åˆ°æ–‡ä»¶"""
        try:
            with open(self.stats_path, 'w') as f:
                json.dump(self.baseline_stats, f, indent=2)
            logger.debug(f"âœ… åŸºå‡†ç»Ÿè®¡å·²ä¿å­˜ï¼š{self.stats_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºå‡†ç»Ÿè®¡å¤±è´¥ï¼š{e}")
    
    def _load_baseline_stats(self):
        """ä»æ–‡ä»¶åŠ è½½åŸºå‡†ç»Ÿè®¡"""
        try:
            if os.path.exists(self.stats_path):
                with open(self.stats_path, 'r') as f:
                    self.baseline_stats = json.load(f)
                logger.info(f"âœ… åŠ è½½åŸºå‡†ç»Ÿè®¡ï¼š{len(self.baseline_stats)}ä¸ªç‰¹å¾")
            else:
                logger.info("ğŸ“Š åŸºå‡†ç»Ÿè®¡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åœ¨é¦–æ¬¡è®­ç»ƒæ—¶åˆ›å»º")
        except Exception as e:
            logger.error(f"åŠ è½½åŸºå‡†ç»Ÿè®¡å¤±è´¥ï¼š{e}")
            self.baseline_stats = {}
    
    def should_retrain(
        self,
        current_samples: int,
        last_training_samples: int,
        last_training_time: Optional[datetime],
        new_sample_threshold: int = 50,
        time_threshold_hours: int = 24,
        min_new_samples_for_time: int = 10
    ) -> Tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è®­ç»ƒ
        
        è§¦å‘æ¡ä»¶ï¼š
        1. ç´¯ç§¯Nç¬”æ–°äº¤æ˜“ï¼ˆå¦‚50ç¬”ï¼‰
        2. è·ä¸Šæ¬¡è®­ç»ƒè¶…è¿‡Tå°æ—¶ä¸”æœ‰è‡³å°‘Mç¬”æ–°æ•°æ®ï¼ˆå¦‚24å°æ—¶+10ç¬”ï¼‰
        3. æ£€æµ‹åˆ°ä¸¥é‡ç‰¹å¾æ¼‚ç§»
        
        Args:
            current_samples: å½“å‰æ ·æœ¬æ•°
            last_training_samples: ä¸Šæ¬¡è®­ç»ƒæ—¶çš„æ ·æœ¬æ•°
            last_training_time: ä¸Šæ¬¡è®­ç»ƒæ—¶é—´
            new_sample_threshold: æ–°æ ·æœ¬æ•°é˜ˆå€¼
            time_threshold_hours: æ—¶é—´é˜ˆå€¼ï¼ˆå°æ—¶ï¼‰
            min_new_samples_for_time: æ—¶é—´è§¦å‘çš„æœ€å°æ–°æ ·æœ¬æ•°
        
        Returns:
            Tuple[bool, str]: (æ˜¯å¦é‡è®­ç»ƒ, åŸå› )
        """
        new_samples = current_samples - last_training_samples
        
        # æ¡ä»¶1ï¼šç´¯ç§¯è¶³å¤Ÿæ–°æ ·æœ¬
        if new_samples >= new_sample_threshold:
            return True, f"ç´¯ç§¯{new_samples}ç¬”æ–°äº¤æ˜“ï¼ˆâ‰¥{new_sample_threshold}ï¼‰"
        
        # æ¡ä»¶2ï¼šæ—¶é—´+æœ€å°æ–°æ ·æœ¬
        if last_training_time is not None:
            hours_since_training = (datetime.now() - last_training_time).total_seconds() / 3600
            
            if hours_since_training >= time_threshold_hours and new_samples >= min_new_samples_for_time:
                return True, (
                    f"è·ä¸Šæ¬¡è®­ç»ƒ{hours_since_training:.1f}å°æ—¶ï¼ˆâ‰¥{time_threshold_hours}ï¼‰"
                    f"ä¸”æœ‰{new_samples}ç¬”æ–°æ•°æ®ï¼ˆâ‰¥{min_new_samples_for_time}ï¼‰"
                )
        
        # æ¡ä»¶3ï¼šæ£€æµ‹åˆ°ä¸¥é‡æ¼‚ç§»
        if self.drift_history:
            latest_drift = self.drift_history[-1]
            if latest_drift.get('recommendation') == 'full_retrain':
                return True, "æ£€æµ‹åˆ°ä¸¥é‡ç‰¹å¾æ¼‚ç§»"
        
        return False, f"ä¸éœ€è¦é‡è®­ç»ƒï¼ˆæ–°æ ·æœ¬{new_samples}ç¬”ï¼‰"
    
    def get_drift_summary(self) -> Dict:
        """
        è·å–æ¼‚ç§»æ£€æµ‹æ‘˜è¦
        
        Returns:
            Dict: æ¼‚ç§»æ‘˜è¦
        """
        if not self.drift_history:
            return {
                'total_checks': 0,
                'drift_events': 0,
                'most_drifted_features': []
            }
        
        # ç»Ÿè®¡æ¼‚ç§»äº‹ä»¶
        drift_events = [h for h in self.drift_history if h['has_drift']]
        
        # ç»Ÿè®¡å„ç‰¹å¾æ¼‚ç§»æ¬¡æ•°
        feature_drift_counts = {}
        for event in drift_events:
            for feature in event['drifted_features']:
                feature_drift_counts[feature] = feature_drift_counts.get(feature, 0) + 1
        
        # æ’åº
        most_drifted = sorted(
            feature_drift_counts.items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]
        
        return {
            'total_checks': len(self.drift_history),
            'drift_events': len(drift_events),
            'drift_rate': len(drift_events) / len(self.drift_history),
            'most_drifted_features': [
                {'feature': f, 'drift_count': c} for f, c in most_drifted
            ]
        }
