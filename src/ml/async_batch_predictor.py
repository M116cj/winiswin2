"""
å¼‚æ­¥æ‰¹é‡é¢„æµ‹å™¨
ä¼˜åŒ–3ï¼šå¼‚æ­¥æ¨¡å‹æ¨ç† + æ‰¹é‡å¤„ç†
"""
import asyncio
import logging
import time
import numpy as np
from typing import Any, Tuple

logger = logging.getLogger(__name__)


class AsyncBatchPredictor:
    """å¼‚æ­¥æ‰¹é‡é¢„æµ‹å™¨"""
    
    def __init__(self, model, batch_size: int = 32, max_delay: float = 0.1):
        """
        åˆå§‹åŒ–å¼‚æ­¥æ‰¹é‡é¢„æµ‹å™¨
        
        Args:
            model: é¢„æµ‹æ¨¡å‹
            batch_size: æ‰¹æ¬¡å¤§å°
            max_delay: æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
        """
        self.model = model
        self.batch_size = batch_size
        self.max_delay = max_delay
        self.prediction_queue = asyncio.Queue()
        self.results = {}
        self.batch_task = None
        self._running = False
        
        logger.info(f"âœ… å¼‚æ­¥æ‰¹é‡é¢„æµ‹å™¨åˆå§‹åŒ– (batch_size={batch_size}, max_delay={max_delay}s)")
    
    async def start_batch_processing(self):
        """å¯åŠ¨æ‰¹é‡å¤„ç†ä»»åŠ¡"""
        if self._running:
            logger.warning("âš ï¸ æ‰¹é‡å¤„ç†ä»»åŠ¡å·²åœ¨è¿è¡Œ")
            return
        
        self._running = True
        self.batch_task = asyncio.create_task(self._process_batches())
        logger.info("ğŸš€ æ‰¹é‡å¤„ç†ä»»åŠ¡å·²å¯åŠ¨")
    
    async def stop_batch_processing(self):
        """åœæ­¢æ‰¹é‡å¤„ç†ä»»åŠ¡"""
        self._running = False
        if self.batch_task:
            self.batch_task.cancel()
            try:
                await self.batch_task
            except asyncio.CancelledError:
                pass
        logger.info("ğŸ›‘ æ‰¹é‡å¤„ç†ä»»åŠ¡å·²åœæ­¢")
    
    async def predict_async(self, position_id: str, features):
        """
        å¼‚æ­¥é¢„æµ‹
        
        Args:
            position_id: ä»“ä½ID
            features: ç‰¹å¾å‘é‡
        
        Returns:
            é¢„æµ‹ç»“æœ
        """
        future = asyncio.Future()
        await self.prediction_queue.put((position_id, features, future))
        return await future
    
    async def _process_batches(self):
        """æ‰¹é‡å¤„ç†é¢„æµ‹è¯·æ±‚"""
        logger.info("ğŸ”„ æ‰¹é‡å¤„ç†å¾ªç¯å¼€å§‹")
        
        while self._running:
            try:
                batch = []
                start_time = time.time()
                
                # æ”¶é›†æ‰¹æ¬¡
                while len(batch) < self.batch_size and (time.time() - start_time) < self.max_delay:
                    try:
                        item = await asyncio.wait_for(
                            self.prediction_queue.get(), 
                            timeout=self.max_delay
                        )
                        batch.append(item)
                    except asyncio.TimeoutError:
                        break
                
                if batch:
                    # æ‰¹é‡æ¨ç†
                    position_ids = [item[0] for item in batch]
                    features_batch = np.array([item[1] for item in batch])
                    
                    logger.debug(f"ğŸ”® æ‰¹é‡é¢„æµ‹ {len(batch)} ä¸ªä»“ä½...")
                    
                    try:
                        # æ‰§è¡Œæ‰¹é‡é¢„æµ‹
                        predictions = self.model.predict(features_batch)
                        
                        # è¿”å›ç»“æœ
                        for i, (position_id, _, future) in enumerate(batch):
                            if not future.done():
                                future.set_result(predictions[i])
                        
                        logger.debug(f"âœ… æ‰¹é‡é¢„æµ‹å®Œæˆ ({len(batch)} ä¸ª)")
                    
                    except Exception as e:
                        logger.error(f"âŒ æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}")
                        # è¿”å›é”™è¯¯ç»™æ‰€æœ‰ç­‰å¾…çš„future
                        for _, _, future in batch:
                            if not future.done():
                                future.set_exception(e)
                
                else:
                    # æ²¡æœ‰å¾…å¤„ç†çš„è¯·æ±‚ï¼ŒçŸ­æš‚ä¼‘çœ 
                    await asyncio.sleep(0.01)
            
            except asyncio.CancelledError:
                logger.info("æ‰¹é‡å¤„ç†ä»»åŠ¡è¢«å–æ¶ˆ")
                break
            except Exception as e:
                logger.error(f"æ‰¹é‡å¤„ç†é”™è¯¯: {e}", exc_info=True)
                await asyncio.sleep(0.1)
        
        logger.info("ğŸ›‘ æ‰¹é‡å¤„ç†å¾ªç¯ç»“æŸ")
    
    def get_queue_size(self) -> int:
        """è·å–é˜Ÿåˆ—å¤§å°"""
        return self.prediction_queue.qsize()
    
    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "queue_size": self.prediction_queue.qsize(),
            "batch_size": self.batch_size,
            "max_delay": self.max_delay,
            "running": self._running
        }
