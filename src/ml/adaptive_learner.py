"""
è‡ªé€‚åº”å­¦ä¹ å™¨ï¼ˆv3.4.0ï¼‰
èŒè´£ï¼šæ ¹æ®æ¨¡å‹æ€§èƒ½åŠ¨æ€è°ƒæ•´å­¦ä¹ å‚æ•°å’Œè®­ç»ƒç­–ç•¥
"""

import logging
from typing import Dict, Optional
from collections import deque
from datetime import datetime

logger = logging.getLogger(__name__)


class AdaptiveLearner:
    """è‡ªé€‚åº”å­¦ä¹ ç®¡ç†å™¨"""
    
    def __init__(
        self,
        base_lr: float = 0.1,
        min_lr: float = 0.01,
        max_lr: float = 0.3,
        history_window: int = 10
    ):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”å­¦ä¹ å™¨
        
        Args:
            base_lr: åŸºç¡€å­¦ä¹ ç‡
            min_lr: æœ€å°å­¦ä¹ ç‡
            max_lr: æœ€å¤§å­¦ä¹ ç‡
            history_window: æ€§èƒ½å†å²çª—å£å¤§å°
        """
        self.base_lr = base_lr
        self.min_lr = min_lr
        self.max_lr = max_lr
        
        # æ€§èƒ½å†å²ï¼ˆæœ€è¿‘Næ¬¡è®­ç»ƒï¼‰
        self.performance_history = deque(maxlen=history_window)
        
        # å½“å‰å‚æ•°
        self.current_lr = base_lr
        self.current_n_estimators = 200
        
        # è‡ªé€‚åº”çŠ¶æ€
        self.consecutive_improvements = 0
        self.consecutive_degradations = 0
    
    def update_performance(self, accuracy: float, timestamp: Optional[datetime] = None):
        """
        æ›´æ–°æ€§èƒ½å†å²
        
        Args:
            accuracy: æ¨¡å‹å‡†ç¡®ç‡
            timestamp: æ—¶é—´æˆ³
        """
        if timestamp is None:
            timestamp = datetime.now()
        
        self.performance_history.append({
            'accuracy': accuracy,
            'timestamp': timestamp,
            'learning_rate': self.current_lr,
            'n_estimators': self.current_n_estimators
        })
        
        logger.debug(f"æ€§èƒ½å†å²æ›´æ–°: å‡†ç¡®ç‡={accuracy:.4f}, LR={self.current_lr}")
    
    def adjust_learning_rate(self, current_accuracy: float) -> float:
        """
        æ ¹æ®æ€§èƒ½åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
        
        Args:
            current_accuracy: å½“å‰å‡†ç¡®ç‡
        
        Returns:
            float: è°ƒæ•´åçš„å­¦ä¹ ç‡
        """
        if len(self.performance_history) == 0:
            return self.base_lr
        
        # è·å–æœ€è¿‘çš„å‡†ç¡®ç‡
        recent_accuracies = [p['accuracy'] for p in list(self.performance_history)[-3:]]
        avg_recent = sum(recent_accuracies) / len(recent_accuracies)
        
        # æ€§èƒ½æå‡
        if current_accuracy > avg_recent + 0.02:
            self.consecutive_improvements += 1
            self.consecutive_degradations = 0
            
            # è¿ç»­æå‡ï¼Œå¢åŠ å­¦ä¹ ç‡ï¼ˆæ¢ç´¢ï¼‰
            if self.consecutive_improvements >= 2:
                new_lr = min(self.current_lr * 1.2, self.max_lr)
                logger.info(f"ğŸ“ˆ æ€§èƒ½æŒç»­æå‡ï¼Œå¢åŠ å­¦ä¹ ç‡: {self.current_lr:.4f} â†’ {new_lr:.4f}")
                self.current_lr = new_lr
        
        # æ€§èƒ½ä¸‹é™
        elif current_accuracy < avg_recent - 0.03:
            self.consecutive_degradations += 1
            self.consecutive_improvements = 0
            
            # è¿ç»­ä¸‹é™ï¼Œé™ä½å­¦ä¹ ç‡ï¼ˆä¿å®ˆï¼‰
            if self.consecutive_degradations >= 2:
                new_lr = max(self.current_lr * 0.7, self.min_lr)
                logger.warning(f"ğŸ“‰ æ€§èƒ½ä¸‹é™ï¼Œé™ä½å­¦ä¹ ç‡: {self.current_lr:.4f} â†’ {new_lr:.4f}")
                self.current_lr = new_lr
        
        # æ€§èƒ½ç¨³å®š
        else:
            self.consecutive_improvements = 0
            self.consecutive_degradations = 0
        
        return self.current_lr
    
    def adjust_n_estimators(self, current_accuracy: float, training_time: float) -> int:
        """
        æ ¹æ®æ€§èƒ½å’Œè®­ç»ƒæ—¶é—´è°ƒæ•´æ ‘çš„æ•°é‡
        
        Args:
            current_accuracy: å½“å‰å‡†ç¡®ç‡
            training_time: è®­ç»ƒè€—æ—¶ï¼ˆç§’ï¼‰
        
        Returns:
            int: è°ƒæ•´åçš„æ ‘æ•°é‡
        """
        # è®­ç»ƒæ—¶é—´è¿‡é•¿ï¼ˆ>60ç§’ï¼‰ï¼Œå‡å°‘æ ‘æ•°é‡
        if training_time > 60 and self.current_n_estimators > 100:
            new_n = max(self.current_n_estimators - 50, 100)
            logger.info(f"â±ï¸  è®­ç»ƒæ—¶é—´è¿‡é•¿ï¼Œå‡å°‘æ ‘æ•°é‡: {self.current_n_estimators} â†’ {new_n}")
            self.current_n_estimators = new_n
        
        # æ€§èƒ½å¾ˆå¥½ä¸”è®­ç»ƒå¿«é€Ÿï¼Œå¯ä»¥å¢åŠ æ ‘æ•°é‡
        elif current_accuracy > 0.80 and training_time < 30 and self.current_n_estimators < 400:
            new_n = min(self.current_n_estimators + 50, 400)
            logger.info(f"ğŸŒ³ æ€§èƒ½ä¼˜ç§€ä¸”è®­ç»ƒå¿«é€Ÿï¼Œå¢åŠ æ ‘æ•°é‡: {self.current_n_estimators} â†’ {new_n}")
            self.current_n_estimators = new_n
        
        return self.current_n_estimators
    
    def should_retrain(self, new_samples: int, hours_since_training: float) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡æ–°è®­ç»ƒ
        
        Args:
            new_samples: æ–°å¢æ ·æœ¬æ•°
            hours_since_training: è·ç¦»ä¸Šæ¬¡è®­ç»ƒçš„å°æ—¶æ•°
        
        Returns:
            bool: æ˜¯å¦åº”è¯¥é‡æ–°è®­ç»ƒ
        """
        # åŸºç¡€è§„åˆ™
        if new_samples >= 50:
            logger.info(f"âœ… è§¦å‘é‡è®­ç»ƒ: æ–°å¢ {new_samples} ç¬”äº¤æ˜“ï¼ˆ>= 50ï¼‰")
            return True
        
        if hours_since_training >= 24 and new_samples >= 10:
            logger.info(f"âœ… è§¦å‘é‡è®­ç»ƒ: è·ç¦»ä¸Šæ¬¡ {hours_since_training:.1f}hï¼ˆ>= 24hï¼‰ä¸”æœ‰ {new_samples} ç¬”æ–°æ•°æ®")
            return True
        
        # è‡ªé€‚åº”è§„åˆ™ï¼šæ€§èƒ½ä¸‹é™æ—¶æ›´é¢‘ç¹è®­ç»ƒ
        if len(self.performance_history) >= 2:
            latest_acc = self.performance_history[-1]['accuracy']
            avg_acc = sum(p['accuracy'] for p in self.performance_history) / len(self.performance_history)
            
            if latest_acc < avg_acc - 0.05 and new_samples >= 20:
                logger.warning(f"âš ï¸  è§¦å‘é‡è®­ç»ƒ: æ€§èƒ½ä¸‹é™ ({latest_acc:.2%} < {avg_acc:.2%})ä¸”æœ‰ {new_samples} ç¬”æ–°æ•°æ®")
                return True
        
        return False
    
    def get_adaptive_params(self, current_accuracy: float, training_time: float = 0) -> Dict:
        """
        è·å–è‡ªé€‚åº”è°ƒæ•´åçš„å‚æ•°
        
        Args:
            current_accuracy: å½“å‰å‡†ç¡®ç‡
            training_time: è®­ç»ƒè€—æ—¶ï¼ˆç§’ï¼‰
        
        Returns:
            Dict: è°ƒæ•´åçš„å‚æ•°
        """
        lr = self.adjust_learning_rate(current_accuracy)
        n_est = self.adjust_n_estimators(current_accuracy, training_time)
        
        return {
            'learning_rate': lr,
            'n_estimators': n_est
        }
    
    def get_stats(self) -> Dict:
        """
        è·å–è‡ªé€‚åº”å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        if len(self.performance_history) == 0:
            return {
                'history_size': 0,
                'avg_accuracy': 0.0,
                'current_lr': self.current_lr,
                'current_n_estimators': self.current_n_estimators
            }
        
        accuracies = [p['accuracy'] for p in self.performance_history]
        
        return {
            'history_size': len(self.performance_history),
            'avg_accuracy': sum(accuracies) / len(accuracies),
            'best_accuracy': max(accuracies),
            'worst_accuracy': min(accuracies),
            'current_lr': self.current_lr,
            'current_n_estimators': self.current_n_estimators,
            'consecutive_improvements': self.consecutive_improvements,
            'consecutive_degradations': self.consecutive_degradations
        }
