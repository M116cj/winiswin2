"""
TensorFlow Lite æ¨¡å‹é‡åŒ–å™¨
ä¼˜åŒ–1ï¼šTensorFlow Lite é‡åŒ–ï¼ˆæ¨ç†é€Ÿåº¦æå‡ 3-5 å€ï¼‰
"""
import logging

logger = logging.getLogger(__name__)

try:
    import tensorflow as tf
    TF_AVAILABLE = True
except ImportError:
    TF_AVAILABLE = False
    logger.warning("âš ï¸ TensorFlow ä¸å¯ç”¨ï¼Œé‡åŒ–åŠŸèƒ½å°†ä¸å¯ç”¨")


class ModelQuantizer:
    """TensorFlow Lite æ¨¡å‹é‡åŒ–å™¨"""
    
    @staticmethod
    def quantize_model(model, representative_data_gen):
        """
        é‡åŒ–æ¨¡å‹åˆ° INT8
        
        Args:
            model: Keras æ¨¡å‹
            representative_data_gen: ä»£è¡¨æ€§æ•°æ®ç”Ÿæˆå™¨
        
        Returns:
            é‡åŒ–åçš„ TFLite æ¨¡å‹ï¼ˆå­—èŠ‚ï¼‰
        """
        if not TF_AVAILABLE:
            raise RuntimeError("TensorFlow ä¸å¯ç”¨ï¼Œæ— æ³•é‡åŒ–æ¨¡å‹")
        
        converter = tf.lite.TFLiteConverter.from_keras_model(model)
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.representative_dataset = representative_data_gen
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
        converter.inference_input_type = tf.int8
        converter.inference_output_type = tf.int8
        
        logger.info("ğŸ”„ æ­£åœ¨é‡åŒ–æ¨¡å‹åˆ° INT8...")
        tflite_model = converter.convert()
        logger.info(f"âœ… æ¨¡å‹é‡åŒ–å®Œæˆï¼Œå¤§å°: {len(tflite_model) / 1024:.2f} KB")
        
        return tflite_model
    
    @staticmethod
    def load_quantized_model(tflite_model_path):
        """
        è½½å…¥é‡åŒ–æ¨¡å‹
        
        Args:
            tflite_model_path: TFLite æ¨¡å‹è·¯å¾„
        
        Returns:
            TFLite Interpreter
        """
        if not TF_AVAILABLE:
            raise RuntimeError("TensorFlow ä¸å¯ç”¨ï¼Œæ— æ³•åŠ è½½é‡åŒ–æ¨¡å‹")
        
        interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
        interpreter.allocate_tensors()
        
        logger.info(f"âœ… è½½å…¥é‡åŒ–æ¨¡å‹: {tflite_model_path}")
        
        # è·å–è¾“å…¥è¾“å‡ºè¯¦æƒ…
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        
        logger.info(f"  è¾“å…¥å½¢çŠ¶: {input_details[0]['shape']}")
        logger.info(f"  è¾“å‡ºå½¢çŠ¶: {output_details[0]['shape']}")
        
        return interpreter
    
    @staticmethod
    def predict_with_quantized_model(interpreter, input_data):
        """
        ä½¿ç”¨é‡åŒ–æ¨¡å‹è¿›è¡Œé¢„æµ‹
        
        Args:
            interpreter: TFLite Interpreter
            input_data: è¾“å…¥æ•°æ®ï¼ˆnumpy arrayï¼‰
        
        Returns:
            é¢„æµ‹ç»“æœ
        """
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        
        # è®¾ç½®è¾“å…¥å¼ é‡
        interpreter.set_tensor(input_details[0]['index'], input_data)
        
        # è¿è¡Œæ¨ç†
        interpreter.invoke()
        
        # è·å–è¾“å‡ºå¼ é‡
        output_data = interpreter.get_tensor(output_details[0]['index'])
        
        return output_data
