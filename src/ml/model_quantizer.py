"""
TensorFlow Lite æ¨¡å‹é‡åŒ–å™¨
ä¼˜åŒ–1ï¼šTensorFlow Lite é‡åŒ–ï¼ˆæ¨ç†é€Ÿåº¦æå‡ 3-5 å€ï¼‰
"""
import logging

logger = logging.getLogger(__name__)

try:
    import tensorflow as tf
    TF_AVAILABLE = True
except ImportError:
    TF_AVAILABLE = False
    logger.warning("âš ï¸ TensorFlow ä¸å¯ç”¨ï¼Œé‡åŒ–åŠŸèƒ½å°†ä¸å¯ç”¨")


class QuantizedModelWrapper:
    """é‡åŒ–æ¨¡å‹åŒ…è£…å™¨ï¼Œæä¾›ä¸åŸå§‹æ¨¡å‹ç›¸åŒçš„æ¥å£"""
    
    def __init__(self, interpreter):
        """
        åˆå§‹åŒ–åŒ…è£…å™¨
        
        Args:
            interpreter: TFLite Interpreter
        """
        self.interpreter = interpreter
        self.input_details = interpreter.get_input_details()
        self.output_details = interpreter.get_output_details()
    
    def encode_structure(self, price_data):
        """
        ç¼–ç å¸‚åœºç»“æ„ï¼ˆä¸ MarketStructureAutoencoder æ¥å£å…¼å®¹ï¼‰
        
        Args:
            price_data: ä»·æ ¼æ•°æ®æ•°ç»„
        
        Returns:
            ç¼–ç åçš„ç»“æ„å‘é‡ï¼ˆfloat32ï¼‰
        """
        import numpy as np
        
        # å‡†å¤‡è¾“å…¥æ•°æ®ï¼ˆå–æœ€å50ä¸ªä»·æ ¼ç‚¹ï¼‰
        input_data = np.array([price_data[-50:]], dtype=np.float32)
        
        # è·å–é‡åŒ–å‚æ•°
        input_scale = self.input_details[0].get('quantization', (0.0, 0))[0]
        input_zero_point = self.input_details[0].get('quantization', (0.0, 0))[1]
        
        # é‡åŒ–è¾“å…¥ï¼šfloat32 -> int8
        if input_scale > 0:
            quantized_input = (input_data / input_scale + input_zero_point).astype(np.int8)
        else:
            # å¦‚æœæ²¡æœ‰é‡åŒ–å‚æ•°ï¼Œä½¿ç”¨float32ï¼ˆå¯èƒ½æ˜¯FLOAT16æ¨¡å‹ï¼‰
            quantized_input = input_data
        
        # è®¾ç½®è¾“å…¥å¼ é‡
        self.interpreter.set_tensor(self.input_details[0]['index'], quantized_input)
        
        # è¿è¡Œæ¨ç†
        self.interpreter.invoke()
        
        # è·å–è¾“å‡ºå¼ é‡
        output_data = self.interpreter.get_tensor(self.output_details[0]['index'])
        
        # è·å–è¾“å‡ºé‡åŒ–å‚æ•°
        output_scale = self.output_details[0].get('quantization', (0.0, 0))[0]
        output_zero_point = self.output_details[0].get('quantization', (0.0, 0))[1]
        
        # åé‡åŒ–è¾“å‡ºï¼šint8 -> float32
        if output_scale > 0:
            dequantized_output = (output_data.astype(np.float32) - output_zero_point) * output_scale
        else:
            # å¦‚æœæ²¡æœ‰é‡åŒ–å‚æ•°ï¼Œç›´æ¥ä½¿ç”¨
            dequantized_output = output_data.astype(np.float32)
        
        return dequantized_output[0]  # è¿”å›ç¬¬ä¸€ä¸ªç»“æœ


class ModelQuantizer:
    """TensorFlow Lite æ¨¡å‹é‡åŒ–å™¨"""
    
    @staticmethod
    def quantize_model(model, representative_data_gen):
        """
        é‡åŒ–æ¨¡å‹åˆ° INT8
        
        Args:
            model: Keras æ¨¡å‹
            representative_data_gen: ä»£è¡¨æ€§æ•°æ®ç”Ÿæˆå™¨
        
        Returns:
            é‡åŒ–åçš„ TFLite æ¨¡å‹ï¼ˆå­—èŠ‚ï¼‰
        """
        if not TF_AVAILABLE:
            raise RuntimeError("TensorFlow ä¸å¯ç”¨ï¼Œæ— æ³•é‡åŒ–æ¨¡å‹")
        
        converter = tf.lite.TFLiteConverter.from_keras_model(model)
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.representative_dataset = representative_data_gen
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
        converter.inference_input_type = tf.int8
        converter.inference_output_type = tf.int8
        
        logger.info("ğŸ”„ æ­£åœ¨é‡åŒ–æ¨¡å‹åˆ° INT8...")
        tflite_model = converter.convert()
        logger.info(f"âœ… æ¨¡å‹é‡åŒ–å®Œæˆï¼Œå¤§å°: {len(tflite_model) / 1024:.2f} KB")
        
        return tflite_model
    
    @staticmethod
    def load_quantized_model(tflite_model_path):
        """
        è½½å…¥é‡åŒ–æ¨¡å‹å¹¶è¿”å›åŒ…è£…å™¨
        
        Args:
            tflite_model_path: TFLite æ¨¡å‹è·¯å¾„
        
        Returns:
            QuantizedModelWrapperï¼ˆæä¾›ä¸åŸå§‹æ¨¡å‹ç›¸åŒçš„æ¥å£ï¼‰
        """
        if not TF_AVAILABLE:
            raise RuntimeError("TensorFlow ä¸å¯ç”¨ï¼Œæ— æ³•åŠ è½½é‡åŒ–æ¨¡å‹")
        
        interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
        interpreter.allocate_tensors()
        
        logger.info(f"âœ… è½½å…¥é‡åŒ–æ¨¡å‹: {tflite_model_path}")
        
        # è·å–è¾“å…¥è¾“å‡ºè¯¦æƒ…
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        
        logger.info(f"  è¾“å…¥å½¢çŠ¶: {input_details[0]['shape']}")
        logger.info(f"  è¾“å‡ºå½¢çŠ¶: {output_details[0]['shape']}")
        
        # è¿”å›åŒ…è£…å™¨è€ŒéåŸå§‹ Interpreter
        return QuantizedModelWrapper(interpreter)
    
    @staticmethod
    def predict_with_quantized_model(interpreter, input_data):
        """
        ä½¿ç”¨é‡åŒ–æ¨¡å‹è¿›è¡Œé¢„æµ‹
        
        Args:
            interpreter: TFLite Interpreter
            input_data: è¾“å…¥æ•°æ®ï¼ˆnumpy arrayï¼‰
        
        Returns:
            é¢„æµ‹ç»“æœ
        """
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        
        # è®¾ç½®è¾“å…¥å¼ é‡
        interpreter.set_tensor(input_details[0]['index'], input_data)
        
        # è¿è¡Œæ¨ç†
        interpreter.invoke()
        
        # è·å–è¾“å‡ºå¼ é‡
        output_data = interpreter.get_tensor(output_details[0]['index'])
        
        return output_data
