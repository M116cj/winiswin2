"""
ä¸ç¡®å®šæ€§é‡åŒ–å™¨
èŒè´£ï¼šä½¿ç”¨Bootstrapç”Ÿæˆé¢„æµ‹åŒºé—´ï¼Œè¯„ä¼°é¢„æµ‹ä¸ç¡®å®šæ€§
"""

import numpy as np
import pandas as pd
from typing import Dict, Tuple, List, Optional
import logging

logger = logging.getLogger(__name__)


class UncertaintyQuantifier:
    """ä¸ç¡®å®šæ€§é‡åŒ–å™¨ï¼ˆBootstrapæ–¹æ³•ï¼‰"""
    
    def __init__(self, n_bootstrap: int = 50, confidence_level: float = 0.95):
        """
        åˆå§‹åŒ–é‡åŒ–å™¨
        
        Args:
            n_bootstrap: Bootstrapé‡‡æ ·æ¬¡æ•°
            confidence_level: ç½®ä¿¡æ°´å¹³ï¼ˆå¦‚0.95è¡¨ç¤º95%ç½®ä¿¡åŒºé—´ï¼‰
        """
        self.n_bootstrap = n_bootstrap
        self.confidence_level = confidence_level
        self.bootstrap_models = []
    
    def fit_bootstrap_models(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        base_model,
        sample_ratio: float = 0.8
    ):
        """
        è®­ç»ƒBootstrapé›†æˆæ¨¡å‹
        
        Args:
            X: ç‰¹å¾æ•°æ®
            y: ç›®æ ‡å˜é‡
            base_model: åŸºç¡€æ¨¡å‹ï¼ˆXGBoostï¼‰
            sample_ratio: æ¯æ¬¡é‡‡æ ·æ¯”ä¾‹
        """
        self.bootstrap_models = []
        n_samples = len(X)
        sample_size = int(n_samples * sample_ratio)
        
        logger.info(f"ğŸ”„ å¼€å§‹è®­ç»ƒ{self.n_bootstrap}ä¸ªBootstrapæ¨¡å‹...")
        
        for i in range(self.n_bootstrap):
            # Bootstrapé‡‡æ ·ï¼ˆæœ‰æ”¾å›ï¼‰
            indices = np.random.choice(n_samples, size=sample_size, replace=True)
            X_boot = X.iloc[indices]
            y_boot = y.iloc[indices]
            
            # è®­ç»ƒæ¨¡å‹
            model = base_model.__class__(**base_model.get_params())
            model.fit(X_boot, y_boot, verbose=False)
            
            self.bootstrap_models.append(model)
        
        logger.info(f"âœ… Bootstrapæ¨¡å‹è®­ç»ƒå®Œæˆï¼š{len(self.bootstrap_models)}ä¸ªæ¨¡å‹")
    
    def predict_with_uncertainty(
        self,
        X: pd.DataFrame
    ) -> Dict:
        """
        å¸¦ä¸ç¡®å®šæ€§çš„é¢„æµ‹
        
        Args:
            X: ç‰¹å¾æ•°æ®
        
        Returns:
            Dict: é¢„æµ‹ç»“æœï¼ˆåŒ…å«åŒºé—´ï¼‰
        """
        if not self.bootstrap_models:
            logger.warning("Bootstrapæ¨¡å‹æœªè®­ç»ƒ")
            return {}
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
        predictions = []
        for model in self.bootstrap_models:
            try:
                pred = model.predict_proba(X)[:, 1]  # å‡è®¾äºŒåˆ†ç±»
            except AttributeError:
                pred = model.predict(X)  # å›å½’
            predictions.append(pred)
        
        predictions = np.array(predictions)  # shape: (n_bootstrap, n_samples)
        
        # è®¡ç®—ç»Ÿè®¡é‡
        mean_pred = np.mean(predictions, axis=0)
        std_pred = np.std(predictions, axis=0)
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´
        alpha = 1 - self.confidence_level
        lower_percentile = (alpha / 2) * 100
        upper_percentile = (1 - alpha / 2) * 100
        
        lower_bound = np.percentile(predictions, lower_percentile, axis=0)
        upper_bound = np.percentile(predictions, upper_percentile, axis=0)
        
        # è®¡ç®—ä¸ç¡®å®šæ€§åˆ†æ•°ï¼ˆæ ‡å‡†å·® / å‡å€¼ï¼‰
        uncertainty_score = std_pred / (np.abs(mean_pred) + 1e-6)
        
        return {
            'mean_prediction': mean_pred,
            'std_prediction': std_pred,
            'lower_bound': lower_bound,
            'upper_bound': upper_bound,
            'uncertainty_score': uncertainty_score,
            'confidence_level': self.confidence_level
        }
    
    def filter_high_confidence_predictions(
        self,
        predictions: Dict,
        uncertainty_threshold: float = 0.2
    ) -> np.ndarray:
        """
        è¿‡æ»¤é«˜ä¿¡å¿ƒé¢„æµ‹ï¼ˆä½ä¸ç¡®å®šæ€§ï¼‰
        
        Args:
            predictions: é¢„æµ‹ç»“æœ
            uncertainty_threshold: ä¸ç¡®å®šæ€§é˜ˆå€¼ï¼ˆ<thresholdåˆ™è®¤ä¸ºé«˜ä¿¡å¿ƒï¼‰
        
        Returns:
            np.ndarray: é«˜ä¿¡å¿ƒæ ·æœ¬çš„å¸ƒå°”æ©ç 
        """
        uncertainty_scores = predictions.get('uncertainty_score', np.array([]))
        
        if len(uncertainty_scores) == 0:
            return np.array([])
        
        # ä½ä¸ç¡®å®šæ€§ = é«˜ä¿¡å¿ƒ
        high_confidence_mask = uncertainty_scores < uncertainty_threshold
        
        high_conf_count = high_confidence_mask.sum()
        total_count = len(uncertainty_scores)
        
        logger.info(
            f"ğŸ“Š é«˜ä¿¡å¿ƒè¿‡æ»¤ï¼š{high_conf_count}/{total_count} "
            f"({high_conf_count/total_count:.1%}) æ ·æœ¬é€šè¿‡"
        )
        
        return high_confidence_mask
    
    def get_prediction_interval_width(self, predictions: Dict) -> np.ndarray:
        """
        è·å–é¢„æµ‹åŒºé—´å®½åº¦
        
        åŒºé—´è¶Šçª„ï¼Œé¢„æµ‹è¶Šå¯é 
        
        Args:
            predictions: é¢„æµ‹ç»“æœ
        
        Returns:
            np.ndarray: åŒºé—´å®½åº¦
        """
        lower = predictions.get('lower_bound', np.array([]))
        upper = predictions.get('upper_bound', np.array([]))
        
        if len(lower) == 0 or len(upper) == 0:
            return np.array([])
        
        interval_width = upper - lower
        
        return interval_width
