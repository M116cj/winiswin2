"""
Hybrid ML Processor for SelfLearningTrader v4.6.0 Phase 1A3
æ‰¹é‡MLæ¨ç†çš„å®ç”¨ä¸»ä¹‰å®ç°ï¼šåœ¨æµå¼æ¶æ„ä¸­é€šè¿‡ç¼“å†²å®ç°å°æ‰¹é‡å¤„ç†

Author: SelfLearningTrader Team
Version: 4.6.0
"""

import time
import threading
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from collections import deque
import logging

logger = logging.getLogger(__name__)


class HybridMLProcessor:
    """
    æ··åˆMLå¤„ç†å™¨ï¼šåœ¨ä¸æ”¹å˜æµå¼æ¶æ„çš„å‰æä¸‹å®ç°æ‰¹é‡æ¨ç†ä¼˜åŒ–
    
    æ ¸å¿ƒç­–ç•¥ï¼š
    - ç¼“å†²è¿›æ¥çš„é¢„æµ‹è¯·æ±‚
    - è¾¾åˆ°batch_sizeæˆ–è¶…æ—¶æ—¶è§¦å‘æ‰¹é‡å¤„ç†
    - å°†æ‰¹é‡ç»“æœç¼“å­˜ï¼Œåç»­è¯·æ±‚ç›´æ¥ä»ç¼“å­˜è·å–
    - å…¼å®¹ç°æœ‰çš„å•ä¸ªé¢„æµ‹API
    
    æ€§èƒ½ç›®æ ‡ï¼š15-25% æ¨ç†é€Ÿåº¦æå‡
    """
    
    def __init__(
        self,
        model,
        batch_size: int = 10,
        max_buffer_time: float = 0.1,
        enable_batching: bool = True
    ):
        """
        åˆå§‹åŒ–æ··åˆMLå¤„ç†å™¨
        
        Args:
            model: MLæ¨¡å‹åŒ…è£…å™¨ï¼ˆMLModelWrapperå®ä¾‹ï¼‰
            batch_size: è§¦å‘æ‰¹é‡å¤„ç†çš„æœ€å°è¯·æ±‚æ•°
            max_buffer_time: ç¼“å†²åŒºæœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            enable_batching: æ˜¯å¦å¯ç”¨æ‰¹é‡å¤„ç†ï¼ˆå¯é…ç½®å…³é—­ï¼‰
        """
        self.model = model
        self.batch_size = batch_size
        self.max_buffer_time = max_buffer_time
        self.enable_batching = enable_batching
        
        # ç‰¹å¾ç¼“å†²åŒºï¼š[(symbol, features)]
        self.feature_buffer: List[Tuple[str, Dict]] = []
        
        # é¢„æµ‹ç¼“å­˜ï¼šsymbol -> prediction
        self.prediction_cache: Dict[str, Any] = {}
        
        # ä¸Šæ¬¡æ‰¹é‡å¤„ç†æ—¶é—´
        self.last_batch_time = time.time()
        
        # çº¿ç¨‹é”ï¼ˆthread-safeï¼‰
        self._lock = threading.Lock()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_predictions': 0,
            'batch_predictions': 0,
            'single_predictions': 0,
            'cache_hits': 0,
            'batches_processed': 0
        }
        
        logger.info(
            f"âœ… HybridMLProcessorå·²åˆå§‹åŒ– (v4.6.0 Phase 1A3)\n"
            f"   æ‰¹é‡å¤§å°: {batch_size}\n"
            f"   ç¼“å†²è¶…æ—¶: {max_buffer_time}s\n"
            f"   æ‰¹é‡å¤„ç†: {'å¯ç”¨' if enable_batching else 'ç¦ç”¨'}"
        )
    
    def predict(self, symbol: str, features: Dict) -> Any:
        """
        ä¸»æ¥å£ï¼šå…¼å®¹ç°æœ‰çš„å•ä¸ªé¢„æµ‹APIï¼ˆç®€åŒ–è®¾è®¡ï¼‰
        
        å·¥ä½œæµç¨‹ï¼š
        1. æ£€æŸ¥ç¼“å­˜ï¼ˆä»ä¹‹å‰çš„æ‰¹é‡å¤„ç†ï¼‰
        2. å…ˆåŠ å…¥ç¼“å†²åŒº
        3. å¦‚æœç¼“å†²åŒºæ»¡æˆ–è¶…æ—¶ï¼šæ‰¹é‡å¤„ç†æ‰€æœ‰è¯·æ±‚
        4. å¦‚æœä¸æ»¡ï¼šä¿ç•™åœ¨ç¼“å†²åŒºï¼Œå½“å‰è¯·æ±‚å•ä¸ªé¢„æµ‹
        
        è®¾è®¡ç†å¿µï¼š
        - æ‰¹é‡å¤„ç†ï¼šå½“ç¼“å†²åŒºæ»¡æ—¶å¤„ç†æ‰€æœ‰ç¼“å†²çš„è¯·æ±‚
        - å•ä¸ªé¢„æµ‹ï¼šç¼“å†²åŒºæœªæ»¡æ—¶çš„å½“å‰è¯·æ±‚
        - ç¼“å†²è¯·æ±‚ä¼šåœ¨ä¸‹æ¬¡æ‰¹é‡æ—¶å¤„ç†ï¼ˆå»¶è¿Ÿæ‰¹é‡ï¼‰
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            features: ç‰¹å¾å­—å…¸
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        self.stats['total_predictions'] += 1
        
        # å¦‚æœæ‰¹é‡å¤„ç†è¢«ç¦ç”¨ï¼Œç›´æ¥å•ä¸ªé¢„æµ‹
        if not self.enable_batching:
            return self._predict_single(features)
        
        with self._lock:
            # æ­¥éª¤1ï¼šæ£€æŸ¥ç¼“å­˜ï¼ˆä»ä¹‹å‰çš„æ‰¹é‡å¤„ç†ï¼‰
            if symbol in self.prediction_cache:
                self.stats['cache_hits'] += 1
                prediction = self.prediction_cache.pop(symbol)
                logger.debug(f"ğŸ¯ ç¼“å­˜å‘½ä¸­: {symbol}")
                return prediction
            
            # æ­¥éª¤2ï¼šåŠ å…¥ç¼“å†²åŒºï¼ˆä¸ºå°†æ¥çš„æ‰¹é‡å‡†å¤‡ï¼‰
            buffer_entry = (symbol, features)
            self.feature_buffer.append(buffer_entry)
            
            # æ­¥éª¤3ï¼šæ£€æŸ¥æ˜¯å¦åº”è§¦å‘æ‰¹é‡å¤„ç†
            should_process_batch = (
                len(self.feature_buffer) >= self.batch_size or
                (time.time() - self.last_batch_time) >= self.max_buffer_time
            )
            
            # è®°å½•å½“å‰bufferä½ç½®ï¼ˆç”¨äºåç»­ç§»é™¤ï¼‰
            current_buffer_len = len(self.feature_buffer)
        
        # æ­¥éª¤4ï¼šå†³ç­–åˆ†æ”¯
        if should_process_batch:
            # è·¯å¾„Aï¼šè§¦å‘æ‰¹é‡å¤„ç†ï¼ˆå¤„ç†æ‰€æœ‰ç¼“å†²çš„è¯·æ±‚ï¼‰
            self._process_batch()
            
            # ä»ç¼“å­˜è·å–ç»“æœ
            with self._lock:
                if symbol in self.prediction_cache:
                    return self.prediction_cache.pop(symbol)
            
            # é™çº§ï¼ˆæ‰¹é‡å¤±è´¥çš„æƒ…å†µï¼‰
            logger.warning(f"æ‰¹é‡å¤„ç†åæœªæ‰¾åˆ°{symbol}çš„ç¼“å­˜ï¼Œé™çº§ä¸ºå•ä¸ªé¢„æµ‹")
            return self._predict_single(features)
        else:
            # è·¯å¾„Bï¼šç¼“å†²åŒºæœªæ»¡ï¼Œå½“å‰è¯·æ±‚å•ä¸ªé¢„æµ‹
            # ä¿ç•™åœ¨ç¼“å†²åŒºä¾›åç»­æ‰¹é‡å¤„ç†
            return self._predict_single(features)
    
    def _process_batch(self) -> None:
        """
        å¤„ç†ç¼“å†²åŒºä¸­çš„æ‰¹é‡è¯·æ±‚ï¼ˆåŒæ­¥æ–¹æ³•ï¼‰
        """
        with self._lock:
            if not self.feature_buffer:
                return
            
            # å¤åˆ¶ç¼“å†²åŒºæ•°æ®å¹¶æ¸…ç©º
            symbols = [item[0] for item in self.feature_buffer]
            features_batch = [item[1] for item in self.feature_buffer]
            buffer_copy = self.feature_buffer.copy()
            self.feature_buffer.clear()
            self.last_batch_time = time.time()
        
        batch_size = len(symbols)
        logger.debug(f"ğŸš€ æ‰¹é‡å¤„ç†: {batch_size}ä¸ªé¢„æµ‹è¯·æ±‚")
        
        try:
            # ä½¿ç”¨æ¨¡å‹çš„æ‰¹é‡é¢„æµ‹æ–¹æ³•
            if hasattr(self.model, 'predict_batch'):
                predictions = self.model.predict_batch(features_batch)
            else:
                # é™çº§ï¼šé€ä¸ªé¢„æµ‹
                logger.warning("æ¨¡å‹ä¸æ”¯æŒbatch_predictï¼Œé™çº§ä¸ºé€ä¸ªé¢„æµ‹")
                predictions = [self.model.predict(feat) for feat in features_batch]
            
            # å°†ç»“æœç¼“å­˜
            with self._lock:
                for symbol, prediction in zip(symbols, predictions):
                    self.prediction_cache[symbol] = prediction
                
                self.stats['batch_predictions'] += batch_size
                self.stats['batches_processed'] += 1
            
            logger.debug(
                f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ: {batch_size}ä¸ªé¢„æµ‹, "
                f"æ‰¹é‡æ•ˆç‡: {self.get_batch_efficiency():.1f}%"
            )
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥ï¼Œé¢„æµ‹å°†é™çº§ä¸ºå•ä¸ªæ¨¡å¼: {e}")
            # å¤±è´¥æ—¶ä¸ç¼“å­˜ï¼Œè°ƒç”¨è€…ä¼šè‡ªåŠ¨é™çº§åˆ°_predict_single
    
    def _predict_single(self, features: Dict) -> Any:
        """
        å•ä¸ªé¢„æµ‹ï¼ˆé™çº§æ¨¡å¼ï¼‰
        
        Args:
            features: ç‰¹å¾å­—å…¸
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        self.stats['single_predictions'] += 1
        return self.model.predict(features)
    
    def flush(self) -> None:
        """
        å¼ºåˆ¶å¤„ç†ç¼“å†²åŒºä¸­çš„æ‰€æœ‰å¾…å¤„ç†è¯·æ±‚å¹¶æ¸…ç©ºç¼“å­˜
        
        åº”åœ¨æ¯æ¬¡å¸‚åœºæ‰«æå‘¨æœŸç»“æŸæ—¶è°ƒç”¨ï¼Œç”¨äºï¼š
        1. å¤„ç†å‰©ä½™çš„ç¼“å†²è¯·æ±‚ï¼ˆæ‰¹é‡ä¼˜åŒ–ï¼‰
        2. æ¸…ç©ºé¢„æµ‹ç¼“å­˜ï¼ˆé˜²æ­¢ä¸‹ä¸€å‘¨æœŸä½¿ç”¨è¿‡æœŸç‰¹å¾ï¼‰
        """
        with self._lock:
            buffer_size = len(self.feature_buffer)
        
        if buffer_size > 0:
            logger.debug(f"ğŸ”„ flush: å¤„ç†ç¼“å†²åŒºä¸­çš„{buffer_size}ä¸ªå¾…å¤„ç†è¯·æ±‚")
            self._process_batch()
        
        # æ¸…ç©ºç¼“å­˜ï¼ˆé˜²æ­¢ä¸‹ä¸€å‘¨æœŸä½¿ç”¨è¿‡æœŸç‰¹å¾ï¼‰
        with self._lock:
            cache_size = len(self.prediction_cache)
            if cache_size > 0:
                logger.debug(f"ğŸ§¹ flush: æ¸…ç©º{cache_size}ä¸ªç¼“å­˜é¢„æµ‹")
                self.prediction_cache.clear()
    
    def get_batch_efficiency(self) -> float:
        """
        è®¡ç®—æ‰¹é‡å¤„ç†æ•ˆç‡ï¼ˆæ‰¹é‡é¢„æµ‹å æ¯”ï¼‰
        
        Returns:
            æ‰¹é‡é¢„æµ‹å æ€»é¢„æµ‹çš„ç™¾åˆ†æ¯”
        """
        total = self.stats['total_predictions']
        if total == 0:
            return 0.0
        return (self.stats['batch_predictions'] / total) * 100
    
    def get_stats(self) -> Dict:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡å­—å…¸
        """
        stats = self.stats.copy()
        batch_eff = self.get_batch_efficiency()
        cache_rate = (
            (self.stats['cache_hits'] / self.stats['total_predictions'] * 100)
            if self.stats['total_predictions'] > 0 else 0.0
        )
        # æ·»åŠ é¢å¤–å­—æ®µï¼ˆéintå­—æ®µï¼‰
        result = dict(stats)
        result['batch_efficiency'] = batch_eff
        result['cache_hit_rate'] = cache_rate
        return result
    
    def reset_stats(self) -> None:
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'total_predictions': 0,
            'batch_predictions': 0,
            'single_predictions': 0,
            'cache_hits': 0,
            'batches_processed': 0
        }
        logger.info("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")
    
    def log_stats(self) -> None:
        """è®°å½•å½“å‰ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()
        logger.info(
            f"ğŸ“Š HybridMLProcessorç»Ÿè®¡:\n"
            f"   æ€»é¢„æµ‹æ•°: {stats['total_predictions']}\n"
            f"   æ‰¹é‡é¢„æµ‹: {stats['batch_predictions']} ({stats['batch_efficiency']:.1f}%)\n"
            f"   å•ä¸ªé¢„æµ‹: {stats['single_predictions']}\n"
            f"   ç¼“å­˜å‘½ä¸­: {stats['cache_hits']} ({stats['cache_hit_rate']:.1f}%)\n"
            f"   æ‰¹æ¬¡æ•°: {stats['batches_processed']}"
        )
