"""
XGBoost æ¨¡å‹è¨“ç·´å™¨ï¼ˆv3.4.0å„ªåŒ–ç‰ˆï¼‰
è·è²¬ï¼šæ¨¡å‹è¨“ç·´ã€è¶…åƒæ•¸å„ªåŒ–ã€æ¨¡å‹è©•ä¼°ã€å¢é‡å­¸ç¿’ã€æ¨¡å‹é›†æˆ
"""

import numpy as np
import pandas as pd
from typing import Dict, Tuple, Optional
import pickle
import os
import logging
from datetime import datetime
import time

from src.config import Config
from src.ml.data_processor import MLDataProcessor
from src.ml.hyperparameter_tuner import HyperparameterTuner
from src.ml.adaptive_learner import AdaptiveLearner
from src.ml.ensemble_model import EnsembleModel
from src.ml.label_leakage_validator import LabelLeakageValidator
from src.ml.imbalance_handler import ImbalanceHandler
from src.ml.drift_detector import DriftDetector

logger = logging.getLogger(__name__)


class XGBoostTrainer:
    """XGBoost æ¨¡å‹è¨“ç·´å™¨ï¼ˆv3.4.0å¢å¼·ç‰ˆï¼‰"""
    
    def __init__(self, use_tuning: bool = False, use_ensemble: bool = False):
        """
        åˆå§‹åŒ–è¨“ç·´å™¨
        
        Args:
            use_tuning: æ˜¯å¦ä½¿ç”¨è¶…åƒæ•¸èª¿å„ª
            use_ensemble: æ˜¯å¦ä½¿ç”¨æ¨¡å‹é›†æˆ
        """
        self.config = Config
        self.data_processor = MLDataProcessor()
        self.model = None
        self.model_path = "data/models/xgboost_model.pkl"
        self.metrics_path = "data/models/model_metrics.json"
        
        # âœ¨ v3.4.0æ–°å¢ï¼šé«˜ç´šåŠŸèƒ½
        self.use_tuning = use_tuning
        self.use_ensemble = use_ensemble
        self.tuner = HyperparameterTuner() if use_tuning else None
        self.adaptive_learner = AdaptiveLearner()
        self.ensemble = EnsembleModel() if use_ensemble else None
        
        # ğŸš€ v3.9.0æ–°å¢ï¼šå„ªåŒ–åŠŸèƒ½ï¼ˆé»˜èªå…¨éƒ¨å•Ÿç”¨ï¼‰
        self.leakage_validator = LabelLeakageValidator()
        self.imbalance_handler = ImbalanceHandler()
        self.drift_detector = DriftDetector(
            window_size=1000,
            drift_threshold=0.05,
            enable_dynamic_window=True,      # å‹•æ…‹çª—å£èª¿æ•´
            enable_multivariate_drift=True   # å¤šè®Šé‡æ¼‚ç§»æª¢æ¸¬ï¼ˆMMDï¼‰
        )
        
        # ğŸ¯ é»˜èªä½¿ç”¨risk_adjustedç›®æ¨™ï¼ˆé¢¨éšªèª¿æ•´å¾Œæ”¶ç›Šï¼‰
        from src.ml.target_optimizer import TargetOptimizer
        from src.ml.uncertainty_quantifier import UncertaintyQuantifier
        from src.ml.feature_importance_monitor import FeatureImportanceMonitor
        
        self.target_optimizer = TargetOptimizer(target_type='risk_adjusted')
        self.uncertainty_quantifier = UncertaintyQuantifier()  # Quantile Regression
        self.importance_monitor = FeatureImportanceMonitor()
        
        # ğŸ¯ v3.9.2.8.5: è³ªé‡æ¬Šé‡è¨ˆç®—å™¨ï¼ˆçµ¦å®Œç¾äº¤æ˜“æ›´é«˜æ¬Šé‡ï¼‰
        from src.managers.model_scorer import ModelScorer
        self.model_scorer = ModelScorer(history_limit=100)
        
        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)
    
    def train(
        self,
        params: Optional[Dict] = None,
        use_gpu: bool = True,
        incremental: bool = False
    ) -> Tuple[object, Dict]:
        """
        è¨“ç·´ XGBoost æ¨¡å‹ï¼ˆv3.4.0å¢å¼·ç‰ˆï¼‰
        
        Args:
            params: æ¨¡å‹åƒæ•¸
            use_gpu: æ˜¯å¦ä½¿ç”¨ GPU
            incremental: æ˜¯å¦ä½¿ç”¨å¢é‡å­¸ç¿’
        
        Returns:
            Tuple[object, Dict]: (æ¨¡å‹, è©•ä¼°æŒ‡æ¨™)
        """
        try:
            import xgboost as xgb
            from sklearn.metrics import (
                accuracy_score, precision_score, recall_score,
                f1_score, roc_auc_score, confusion_matrix
            )
            
            # âœ¨ v3.4.0ï¼šé–‹å§‹æ€§èƒ½è¿½è¹¤
            start_time = time.time()
            
            # åŠ è¼‰æ•¸æ“š
            df = self.data_processor.load_training_data()
            
            min_samples = self.config.ML_MIN_TRAINING_SAMPLES
            if df.empty or len(df) < min_samples:
                logger.warning(f"è¨“ç·´æ•¸æ“šä¸è¶³: {len(df)} æ¢è¨˜éŒ„ (æœ€å°‘éœ€è¦ {min_samples})")
                return None, {}
            
            # ğŸ” v3.9.0ï¼šæ¨™ç±¤æ³„æ¼é©—è­‰
            leakage_report = self.leakage_validator.validate_training_data(df)
            if leakage_report['has_leakage']:
                logger.warning(f"âš ï¸ æª¢æ¸¬åˆ°æ½›åœ¨æ¨™ç±¤æ³„æ¼ï¼š{leakage_report['leakage_features']}")
            
            # ğŸ“Š v3.9.1ï¼šæ‡‰ç”¨å‹•æ…‹æ»‘å‹•çª—å£ï¼ˆæ³¢å‹•ç‡è‡ªé©æ‡‰ 500-2000ï¼‰
            df = self.drift_detector.apply_sliding_window(df)  # ä¸å‚³window_sizeï¼Œä½¿ç”¨å‹•æ…‹è¨ˆç®—
            
            # ğŸ¯ v3.9.1ï¼šæº–å‚™risk_adjustedç›®æ¨™è®Šé‡ï¼ˆæ›¿ä»£äºŒåˆ†é¡ï¼‰
            y, target_meta = self.target_optimizer.prepare_target(df)
            logger.info(f"ğŸ“Š ç›®æ¨™é¡å‹ï¼š{target_meta.get('target_type', 'unknown')}")
            
            # æº–å‚™ç‰¹å¾µï¼ˆä¸åŒ…å«ç›®æ¨™ï¼‰
            X, _ = self.data_processor.prepare_features(df)
            
            # åªä¿ç•™æ•¸å€¼ç‰¹å¾µï¼ˆèˆ‡ç›®æ¨™å°é½Šï¼‰
            X = X.loc[y.index]
            
            if X.empty or y.empty:
                logger.error("ç‰¹å¾µæº–å‚™å¤±æ•—")
                return None, {}
            
            # ğŸ“Š v3.9.1ï¼šé¡åˆ¥å¹³è¡¡åˆ†æï¼ˆåƒ…äºŒåˆ†é¡æ¨¡å¼ï¼‰
            balance_report = {}
            is_classification = self.target_optimizer.target_type == 'binary'
            
            if is_classification:
                balance_report = self.imbalance_handler.analyze_class_balance(y, X)
            else:
                # å›æ­¸æ¨¡å¼ï¼šè·³éé¡åˆ¥å¹³è¡¡æª¢æŸ¥
                balance_report = {'needs_balancing': False, 'class_distribution': {}}
            
            # ğŸ” v3.9.0ï¼šç‰¹å¾µåˆ†å¸ƒæ¼‚ç§»æª¢æ¸¬
            drift_report = self.drift_detector.detect_feature_drift(
                X,
                X.columns.tolist(),
                update_baseline=True
            )
            
            # åˆ†å‰²æ•¸æ“š
            X_train, X_test, y_train, y_test = self.data_processor.split_data(X, y)
            
            # ğŸ›¡ï¸ v3.9.1ï¼šè¨ˆç®—æ¨£æœ¬æ¬Šé‡ï¼ˆåˆ†é¡ï¼šé¡åˆ¥æ¬Šé‡ï¼Œå›æ­¸ï¼šæ™‚é–“æ¬Šé‡ï¼‰
            sample_weights = None
            
            if is_classification and balance_report.get('needs_balancing', False):
                logger.info("ğŸ“Š åˆ†é¡æ¨¡å¼ï¼šè¨ˆç®—é¡åˆ¥å¹³è¡¡æ¬Šé‡...")
                sample_weights = self.imbalance_handler.calculate_sample_weight(y_train, method='balanced')
            
            # æ‰€æœ‰æ¨¡å¼ï¼šæ‡‰ç”¨æ™‚é–“è¡°æ¸›æ¬Šé‡ï¼ˆæ–°æ•¸æ“šæ¬Šé‡æ›´é«˜ï¼‰
            time_weights = self.drift_detector.calculate_sample_weights(
                pd.DataFrame({'y': y_train}),
                decay_factor=0.95
            )
            
            if sample_weights is not None:
                sample_weights = sample_weights * time_weights
            else:
                sample_weights = time_weights
            
            # ğŸ¯ v3.9.2.8.5ï¼šæ‡‰ç”¨è³ªé‡æ¬Šé‡ï¼ˆçµ¦å®Œç¾äº¤æ˜“æ›´é«˜æ¬Šé‡ï¼‰
            try:
                quality_weights = self._calculate_quality_weights(df, np.array(X_train.index.values))
                sample_weights = sample_weights * quality_weights
                logger.info(
                    f"âœ… å·²æ‡‰ç”¨è³ªé‡æ¬Šé‡ï¼ˆå®Œç¾äº¤æ˜“3.0xï¼Œå„ªç§€äº¤æ˜“2.0xï¼Œè‰¯å¥½äº¤æ˜“1.5xï¼‰"
                )
            except Exception as e:
                logger.warning(f"è³ªé‡æ¬Šé‡è¨ˆç®—å¤±æ•—ï¼Œç¹¼çºŒä½¿ç”¨åŸºç¤æ¬Šé‡: {e}")
            
            logger.info(f"ğŸ“Š æœ€çµ‚æ¨£æœ¬æ¬Šé‡ï¼šmin={sample_weights.min():.3f}, max={sample_weights.max():.3f}, mean={sample_weights.mean():.3f}")
            
            # âœ¨ v3.4.0ï¼šè¶…åƒæ•¸èª¿å„ª
            if params is None:
                if self.use_tuning and self.tuner:
                    logger.info("å•Ÿå‹•è¶…åƒæ•¸è‡ªå‹•èª¿å„ª...")
                    params, _ = self.tuner.quick_tune(X_train, y_train, use_gpu)
                else:
                    # ğŸ¯ v3.9.1ï¼šæ ¹æ“šç›®æ¨™é¡å‹è¨­ç½®é»˜èªåƒæ•¸
                    # ğŸ¯ v3.9.2.9: å‹•æ…‹æª¢æ¸¬CPUæ ¸å¿ƒæ•¸ï¼ˆæå‡å¯ç§»æ¤æ€§ï¼‰
                    import multiprocessing
                    available_cores = multiprocessing.cpu_count()
                    n_jobs = min(available_cores, 32)  # æœ€å¤šä½¿ç”¨32æ ¸å¿ƒ
                    logger.info(f"ğŸ’» æª¢æ¸¬åˆ°{available_cores}å€‹CPUæ ¸å¿ƒï¼Œä½¿ç”¨{n_jobs}å€‹é€²è¡Œè¨“ç·´")
                    
                    base_params = {
                        'max_depth': 6,
                        'learning_rate': 0.1,
                        'n_estimators': 200,
                        'subsample': 0.8,
                        'colsample_bytree': 0.8,
                        'min_child_weight': 1,
                        'gamma': 0.1,
                        'reg_alpha': 0.1,
                        'reg_lambda': 1.0,
                        'random_state': 42,
                        'n_jobs': n_jobs  # å‹•æ…‹è¨­ç½®
                    }
                    
                    # æ ¹æ“šç›®æ¨™é¡å‹èª¿æ•´objectiveå’Œeval_metric
                    params = self.target_optimizer.get_model_params(base_params)
                    logger.info(f"ğŸ“Š æ¨¡å‹åƒæ•¸ï¼šobjective={params['objective']}, eval_metric={params['eval_metric']}")
                    
                    # ğŸ›¡ï¸ v3.9.1ï¼šåˆ†é¡æ¨¡å¼æ‰æ·»åŠ æˆæœ¬æ„ŸçŸ¥åƒæ•¸
                    if is_classification and balance_report.get('needs_balancing', False):
                        scale_pos_weight = self.imbalance_handler.get_scale_pos_weight(y_train)
                        params['scale_pos_weight'] = scale_pos_weight
                        logger.info(f"ğŸ“Š å•Ÿç”¨æˆæœ¬æ„ŸçŸ¥å­¸ç¿’ï¼šscale_pos_weight = {scale_pos_weight:.2f}")
                    
                    # GPU åŠ é€Ÿ
                    if use_gpu:
                        if self._detect_gpu():
                            params['tree_method'] = 'gpu_hist'
                            params['predictor'] = 'gpu_predictor'
                            logger.info("âœ… ä½¿ç”¨ GPU åŠ é€Ÿè¨“ç·´")
                        else:
                            logger.warning("GPU ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU è¨“ç·´")
                            params['tree_method'] = 'hist'
                    else:
                        params['tree_method'] = 'hist'
            
            # âœ¨ v3.9.1ï¼šæ ¹æ“šç›®æ¨™é¡å‹é¸æ“‡æ¨¡å‹
            logger.info("é–‹å§‹è¨“ç·´ XGBoost æ¨¡å‹...")
            logger.info(f"è¨“ç·´é›†å¤§å°: {X_train.shape}, æ¸¬è©¦é›†å¤§å°: {X_test.shape}")
            
            if is_classification:
                model = xgb.XGBClassifier(**params)
            else:
                # å›æ­¸æ¨¡å‹ï¼ˆç”¨æ–¼risk_adjustedå’Œpnl_pctï¼‰
                model = xgb.XGBRegressor(**params)
            
            # å¢é‡å­¸ç¿’ï¼šåŠ è¼‰èˆŠæ¨¡å‹ç¹¼çºŒè¨“ç·´
            xgb_model_file = None
            temp_model_path = 'data/models/temp_xgb_model.json'
            
            if incremental and os.path.exists(self.model_path):
                try:
                    with open(self.model_path, 'rb') as f:
                        old_model = pickle.load(f)
                    
                    # ä¿å­˜èˆŠæ¨¡å‹ç‚ºJSONæ ¼å¼ï¼ˆXGBoostå¢é‡å­¸ç¿’éœ€è¦ï¼‰
                    old_model.save_model(temp_model_path)
                    xgb_model_file = temp_model_path
                    
                    logger.info(f"âœ… å¢é‡å­¸ç¿’å•Ÿç”¨ï¼šåŸºæ–¼èˆŠæ¨¡å‹ç¹¼çºŒè¨“ç·´")
                except Exception as e:
                    logger.warning(f"åŠ è¼‰èˆŠæ¨¡å‹å¤±æ•—ï¼ŒåŸ·è¡Œå®Œæ•´è¨“ç·´: {e}")
                    xgb_model_file = None
            
            # è¨“ç·´ï¼ˆå¢é‡æˆ–å®Œæ•´ï¼‰- ä½¿ç”¨try-finallyç¢ºä¿è‡¨æ™‚æ–‡ä»¶æ¸…ç†
            try:
                if xgb_model_file:
                    # å¢é‡å­¸ç¿’ï¼ˆå¸¶æ¨£æœ¬æ¬Šé‡ï¼‰
                    if sample_weights is not None:
                        model.fit(
                            X_train, y_train,
                            sample_weight=sample_weights,
                            eval_set=[(X_train, y_train), (X_test, y_test)],
                            early_stopping_rounds=20,
                            verbose=False,
                            xgb_model=xgb_model_file  # âœ¨ å¢é‡å­¸ç¿’é—œéµåƒæ•¸
                        )
                    else:
                        model.fit(
                            X_train, y_train,
                            eval_set=[(X_train, y_train), (X_test, y_test)],
                            early_stopping_rounds=20,
                            verbose=False,
                            xgb_model=xgb_model_file
                        )
                else:
                    # å®Œæ•´è¨“ç·´ï¼ˆå¸¶æ¨£æœ¬æ¬Šé‡ï¼‰
                    if sample_weights is not None:
                        model.fit(
                            X_train, y_train,
                            sample_weight=sample_weights,
                            eval_set=[(X_train, y_train), (X_test, y_test)],
                            early_stopping_rounds=20,
                            verbose=False
                        )
                    else:
                        model.fit(
                            X_train, y_train,
                            eval_set=[(X_train, y_train), (X_test, y_test)],
                            early_stopping_rounds=20,
                            verbose=False
                        )
            finally:
                # ğŸ¯ v3.9.2.9: å¼·åŒ–è‡¨æ™‚æ–‡ä»¶æ¸…ç†æ—¥èªŒï¼ˆé¿å…ç´¯ç©è‡¨æ™‚æ–‡ä»¶ï¼‰
                if xgb_model_file and os.path.exists(temp_model_path):
                    try:
                        os.remove(temp_model_path)
                        logger.debug("âœ… è‡¨æ™‚æ¨¡å‹æ–‡ä»¶å·²æ¸…ç†")
                    except Exception as cleanup_error:
                        logger.warning(f"âš ï¸ è‡¨æ™‚æ¨¡å‹æ–‡ä»¶æ¸…ç†å¤±æ•—: {cleanup_error}")
                        logger.warning(f"   è«‹æ‰‹å‹•åˆªé™¤: {temp_model_path}")
            
            # é æ¸¬
            y_pred = model.predict(X_test)
            
            # âœ¨ v3.9.2.2ï¼šç‰¹å¾µé‡è¦æ€§åˆ†æ
            feature_importance_df = self._analyze_feature_importance(model, X_train)
            
            # âœ¨ v3.9.2.2ï¼šç¶œåˆè©•ä¼°ï¼ˆåƒ…åˆ†é¡æ¨¡å¼ï¼‰
            if is_classification:
                y_pred_proba = model.predict_proba(X_test)[:, 1]
                
                # è¨ˆç®—é¡å¤–è©•ä¼°æŒ‡æ¨™
                from sklearn.metrics import average_precision_score
                avg_precision = average_precision_score(y_test, y_pred_proba)
                
                logger.info(f"\nğŸ“Š ç¶œåˆè©•ä¼°ï¼š")
                logger.info(f"Average Precision: {avg_precision:.4f}")
                
                # ğŸ¯ v3.9.2.9: ä¸åŒé–¾å€¼è¡¨ç¾ï¼ˆzero_division=0æ›´å¯é æ¸¬ï¼‰
                logger.info(f"\nğŸ¯ ä¸åŒé–¾å€¼ä¸‹çš„è¡¨ç¾ï¼š")
                for threshold in [0.3, 0.4, 0.5, 0.6, 0.7]:
                    y_pred_thresh = (y_pred_proba >= threshold).astype(int)
                    prec = precision_score(y_test, y_pred_thresh, zero_division=0)
                    rec = recall_score(y_test, y_pred_thresh, zero_division=0)
                    logger.info(f"  é–¾å€¼{threshold:.1f}: Precision={prec:.3f}, Recall={rec:.3f}")
            
            # ğŸ¯ v3.9.1ï¼šæ ¹æ“šç›®æ¨™é¡å‹è©•ä¼°
            metrics = {
                'training_samples': len(df),
                'train_set_size': len(X_train),
                'test_set_size': len(X_test),
                'trained_at': datetime.now().isoformat(),
                'target_type': self.target_optimizer.target_type
            }
            
            if is_classification:
                # åˆ†é¡è©•ä¼°
                y_pred_proba = model.predict_proba(X_test)[:, 1]
                
                # ğŸ¯ v3.9.2.9: zero_division=0æ›´å¯é æ¸¬ï¼ˆé¿å…è­¦å‘Šå™ªéŸ³ï¼‰
                metrics.update({
                    'accuracy': float(accuracy_score(y_test, y_pred)),
                    'precision': float(precision_score(y_test, y_pred, zero_division=0)),
                    'recall': float(recall_score(y_test, y_pred, zero_division=0)),
                    'f1_score': float(f1_score(y_test, y_pred, zero_division=0)),
                    'roc_auc': float(roc_auc_score(y_test, y_pred_proba))
                })
                
                # æ··æ·†çŸ©é™£å ±å‘Š
                confusion_report = self.imbalance_handler.generate_confusion_matrix_report(
                    y_test.values,
                    y_pred,
                    X_test
                )
                metrics['confusion_matrix_detailed'] = confusion_report
                
                cm = confusion_matrix(y_test, y_pred)
                metrics['confusion_matrix'] = cm.tolist()
            else:
                # å›æ­¸è©•ä¼°ï¼ˆrisk_adjusted / pnl_pctï¼‰
                from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
                import numpy as np
                
                mae = mean_absolute_error(y_test, y_pred)
                mse = mean_squared_error(y_test, y_pred)
                rmse = np.sqrt(mse)
                r2 = r2_score(y_test, y_pred)
                
                # æ–¹å‘æº–ç¢ºç‡ï¼ˆé æ¸¬ç¬¦è™Ÿæ˜¯å¦æ­£ç¢ºï¼‰
                direction_accuracy = np.mean(np.sign(y_test) == np.sign(y_pred))
                
                metrics.update({
                    'mae': float(mae),
                    'mse': float(mse),
                    'rmse': float(rmse),
                    'r2_score': float(r2),
                    'direction_accuracy': float(direction_accuracy)
                })
            
            # ğŸ“Š v3.9.0ï¼šæ·»åŠ å„ªåŒ–å ±å‘Š
            metrics['optimization_reports'] = {
                'label_leakage': leakage_report,
                'class_balance': balance_report,
                'feature_drift': drift_report
            }
            
            # ç‰¹å¾µé‡è¦æ€§
            feature_importance = self.data_processor.get_feature_importance(
                model,
                X.columns.tolist()
            )
            metrics['feature_importance'] = feature_importance
            
            logger.info("=" * 60)
            logger.info(f"æ¨¡å‹è¨“ç·´å®Œæˆï¼ˆç›®æ¨™é¡å‹ï¼š{self.target_optimizer.target_type}ï¼‰")
            
            if is_classification:
                logger.info(f"æº–ç¢ºç‡: {metrics['accuracy']:.4f}")
                logger.info(f"ç²¾ç¢ºç‡: {metrics['precision']:.4f}")
                logger.info(f"å¬å›ç‡: {metrics['recall']:.4f}")
                logger.info(f"F1 åˆ†æ•¸: {metrics['f1_score']:.4f}")
                logger.info(f"ROC-AUC: {metrics['roc_auc']:.4f}")
            else:
                logger.info(f"MAE: {metrics['mae']:.4f}")
                logger.info(f"RMSE: {metrics['rmse']:.4f}")
                logger.info(f"RÂ² Score: {metrics['r2_score']:.4f}")
                logger.info(f"æ–¹å‘æº–ç¢ºç‡: {metrics['direction_accuracy']:.4f} ({metrics['direction_accuracy']*100:.1f}%)")
            
            # âœ¨ v3.4.0ï¼šè¨“ç·´æ€§èƒ½è¿½è¹¤
            training_time = time.time() - start_time
            metrics['training_samples'] = len(df)
            metrics['training_time_seconds'] = training_time
            metrics['incremental'] = incremental
            logger.info(f"è¨“ç·´è€—æ™‚: {training_time:.2f}ç§’")
            
            # âœ¨ v3.9.1ï¼šè‡ªé©æ‡‰å­¸ç¿’æ›´æ–°ï¼ˆæ ¹æ“šç›®æ¨™é¡å‹é¸æ“‡æŒ‡æ¨™ï¼‰
            if is_classification:
                # åˆ†é¡æ¨¡å¼ï¼šä½¿ç”¨æº–ç¢ºç‡
                performance_metric = metrics.get('accuracy', 0.0)
            else:
                # å›æ­¸æ¨¡å¼ï¼šä½¿ç”¨æ–¹å‘æº–ç¢ºç‡ï¼ˆ0-1ç¯„åœï¼‰
                performance_metric = metrics.get('direction_accuracy', 0.0)
            
            self.adaptive_learner.update_performance(
                performance_metric,
                datetime.now()
            )
            
            logger.info("=" * 60)
            
            self.model = model
            
            return model, metrics
            
        except ImportError:
            logger.error("XGBoost æˆ– scikit-learn æœªå®‰è£")
            return None, {}
        except Exception as e:
            logger.error(f"è¨“ç·´æ¨¡å‹å¤±æ•—: {e}", exc_info=True)
            return None, {}
    
    def save_model(self, model, metrics: Dict):
        """
        ä¿å­˜æ¨¡å‹å’ŒæŒ‡æ¨™
        
        Args:
            model: è¨“ç·´å¥½çš„æ¨¡å‹
            metrics: è©•ä¼°æŒ‡æ¨™
        """
        try:
            # ä¿å­˜æ¨¡å‹
            with open(self.model_path, 'wb') as f:
                pickle.dump(model, f)
            logger.info(f"æ¨¡å‹å·²ä¿å­˜: {self.model_path}")
            
            # ä¿å­˜æŒ‡æ¨™
            import json
            with open(self.metrics_path, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, ensure_ascii=False, indent=2)
            logger.info(f"æŒ‡æ¨™å·²ä¿å­˜: {self.metrics_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹å¤±æ•—: {e}")
    
    def load_model(self) -> Optional[object]:
        """
        åŠ è¼‰å·²è¨“ç·´çš„æ¨¡å‹
        
        Returns:
            Optional[object]: æ¨¡å‹å°è±¡
        """
        try:
            if not os.path.exists(self.model_path):
                logger.warning(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
                return None
            
            with open(self.model_path, 'rb') as f:
                model = pickle.load(f)
            
            logger.info(f"æ¨¡å‹å·²åŠ è¼‰: {self.model_path}")
            self.model = model
            
            return model
            
        except Exception as e:
            logger.error(f"åŠ è¼‰æ¨¡å‹å¤±æ•—: {e}")
            return None
    
    def auto_train_if_needed(self, min_samples: int = 100) -> bool:
        """
        å¦‚æœæœ‰è¶³å¤ æ•¸æ“šå‰‡è‡ªå‹•è¨“ç·´æ¨¡å‹
        
        Args:
            min_samples: æœ€å°è¨“ç·´æ¨£æœ¬æ•¸
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸè¨“ç·´
        """
        try:
            df = self.data_processor.load_training_data()
            
            if len(df) < min_samples:
                logger.info(f"è¨“ç·´æ•¸æ“šä¸è¶³: {len(df)}/{min_samples} (é…ç½®: ML_MIN_TRAINING_SAMPLES)")
                return False
            
            logger.info(f"æª¢æ¸¬åˆ° {len(df)} æ¢è¨“ç·´æ•¸æ“šï¼Œé–‹å§‹è‡ªå‹•è¨“ç·´...")
            
            model, metrics = self.train()
            
            if model is not None:
                self.save_model(model, metrics)
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"è‡ªå‹•è¨“ç·´å¤±æ•—: {e}")
            return False
    
    def _calculate_quality_weights(self, df: pd.DataFrame, train_indices: np.ndarray) -> np.ndarray:
        """
        ğŸ¯ v3.9.2.8.5: è¨ˆç®—åŸºæ–¼æ¨¡å‹è©•åˆ†çš„è³ªé‡æ¬Šé‡
        çµ¦100åˆ†çš„å®Œç¾äº¤æ˜“ï¼ˆç›ˆåˆ©+é«˜ç½®ä¿¡åº¦ï¼‰æ›´é«˜çš„è¨“ç·´æ¬Šé‡
        
        Args:
            df: å®Œæ•´çš„è¨“ç·´æ•¸æ“šDataFrame
            train_indices: è¨“ç·´é›†ç´¢å¼•
        
        Returns:
            np.ndarray: è³ªé‡æ¬Šé‡æ•¸çµ„ï¼ˆé•·åº¦ç­‰æ–¼è¨“ç·´é›†å¤§å°ï¼‰
        """
        try:
            # åªè™•ç†è¨“ç·´é›†æ¨£æœ¬
            train_df = df.iloc[train_indices].copy()
            
            # åˆå§‹åŒ–æ¬Šé‡ç‚º1.0
            quality_weights = np.ones(len(train_df))
            
            # æª¢æŸ¥å¿…éœ€æ¬„ä½æ˜¯å¦å­˜åœ¨
            required_cols = ['pnl', 'confidence_score']
            if not all(col in train_df.columns for col in required_cols):
                logger.warning("ç¼ºå°‘pnlæˆ–confidence_scoreæ¬„ä½ï¼Œç„¡æ³•è¨ˆç®—è³ªé‡æ¬Šé‡")
                return quality_weights
            
            # è¨ˆç®—æ¯å€‹æ¨£æœ¬çš„æ¨¡å‹è©•åˆ†
            for idx, row in enumerate(train_df.itertuples()):
                try:
                    pnl_pct = row.pnl * 100  # è½‰æ›ç‚ºç™¾åˆ†æ¯”
                    confidence = row.confidence_score
                    
                    # ä½¿ç”¨ModelScorerçš„è©•åˆ†é‚è¼¯ï¼ˆä¸éœ€è¦èƒœç‡ï¼‰
                    # PnLåˆ†æ•¸ï¼ˆ50%æ¬Šé‡ï¼‰
                    if pnl_pct > 0:
                        pnl_score = min(100, (pnl_pct / 10) * 100)  # 10%æ”¶ç›Š = 100åˆ†
                    else:
                        pnl_score = max(0, 50 + pnl_pct * 5)  # -10%è™§æ = 0åˆ†
                    
                    # ç½®ä¿¡åº¦æº–ç¢ºæ€§åˆ†æ•¸ï¼ˆ30%æ¬Šé‡ï¼‰
                    is_profit = pnl_pct > 0
                    is_high_conf = confidence >= 0.7
                    
                    if is_profit and is_high_conf:
                        conf_score = 100  # å®Œç¾é æ¸¬
                    elif is_profit and not is_high_conf:
                        conf_score = 70   # å¹¸é‹
                    elif not is_profit and is_high_conf:
                        conf_score = 20   # èª¤åˆ¤
                    else:
                        conf_score = 50   # ç¬¦åˆé æœŸ
                    
                    # è¨ˆç®—ç¸½è©•åˆ†ï¼ˆä¸è€ƒæ…®èƒœç‡ï¼Œå› ç‚ºè¨“ç·´æ™‚èƒœç‡ä¸ç©©å®šï¼‰
                    score = (pnl_score * 0.5) + (conf_score * 0.5)
                    
                    # ğŸ¯ æ ¹æ“šè©•åˆ†èª¿æ•´æ¬Šé‡
                    if score >= 95:
                        # 95-100åˆ†ï¼ˆå®Œç¾äº¤æ˜“ï¼‰ï¼š3.0å€æ¬Šé‡
                        quality_weights[idx] = 3.0
                    elif score >= 85:
                        # 85-94åˆ†ï¼ˆå„ªç§€äº¤æ˜“ï¼‰ï¼š2.0å€æ¬Šé‡
                        quality_weights[idx] = 2.0
                    elif score >= 70:
                        # 70-84åˆ†ï¼ˆè‰¯å¥½äº¤æ˜“ï¼‰ï¼š1.5å€æ¬Šé‡
                        quality_weights[idx] = 1.5
                    elif score >= 50:
                        # 50-69åˆ†ï¼ˆä¸€èˆ¬äº¤æ˜“ï¼‰ï¼š1.0å€æ¬Šé‡
                        quality_weights[idx] = 1.0
                    else:
                        # <50åˆ†ï¼ˆä½è³ªé‡äº¤æ˜“ï¼‰ï¼š0.5å€æ¬Šé‡
                        quality_weights[idx] = 0.5
                
                except Exception as e:
                    # ğŸ¯ v3.9.2.9: æå‡éŒ¯èª¤æ—¥èªŒç´šåˆ¥ï¼ˆç™¼ç¾æ•¸æ“šè³ªé‡å•é¡Œï¼‰
                    logger.warning(f"âš ï¸ è¨ˆç®—ç¬¬{idx}å€‹æ¨£æœ¬è³ªé‡æ¬Šé‡å¤±æ•—: {e}ï¼Œä½¿ç”¨é»˜èªæ¬Šé‡1.0")
                    quality_weights[idx] = 1.0
            
            # çµ±è¨ˆè³ªé‡æ¬Šé‡åˆ†å¸ƒ
            perfect_count = np.sum(quality_weights >= 3.0)
            excellent_count = np.sum((quality_weights >= 2.0) & (quality_weights < 3.0))
            good_count = np.sum((quality_weights >= 1.5) & (quality_weights < 2.0))
            
            logger.info(
                f"ğŸ“Š è³ªé‡æ¬Šé‡åˆ†å¸ƒï¼šå®Œç¾äº¤æ˜“(3.0x)={perfect_count}, "
                f"å„ªç§€äº¤æ˜“(2.0x)={excellent_count}, "
                f"è‰¯å¥½äº¤æ˜“(1.5x)={good_count}"
            )
            
            return quality_weights
            
        except Exception as e:
            logger.error(f"è¨ˆç®—è³ªé‡æ¬Šé‡å¤±æ•—: {e}", exc_info=True)
            return np.ones(len(train_indices))
    
    def _detect_gpu(self) -> bool:
        """
        æª¢æ¸¬GPUæ˜¯å¦å¯ç”¨ï¼ˆv3.4.0æ–°å¢ï¼‰
        
        Returns:
            bool: GPUæ˜¯å¦å¯ç”¨
        """
        try:
            import subprocess
            subprocess.check_output(['nvidia-smi'])
            return True
        except:
            return False
    
    def train_with_ensemble(self, use_gpu: bool = True) -> Tuple[Optional[object], Dict]:
        """
        ä½¿ç”¨æ¨¡å‹é›†æˆè¨“ç·´ï¼ˆv3.4.0æ–°å¢ï¼‰
        
        Args:
            use_gpu: æ˜¯å¦ä½¿ç”¨GPU
        
        Returns:
            Tuple[Optional[object], Dict]: (é›†æˆæ¨¡å‹, è©•ä¼°æŒ‡æ¨™)
        """
        if not self.use_ensemble or not self.ensemble:
            logger.warning("æ¨¡å‹é›†æˆæœªå•Ÿç”¨")
            return None, {}
        
        try:
            # åŠ è¼‰æ•¸æ“š
            df = self.data_processor.load_training_data()
            X, y = self.data_processor.prepare_features(df)
            X_train, X_test, y_train, y_test = self.data_processor.split_data(X, y)
            
            # è¨“ç·´é›†æˆæ¨¡å‹
            models, metrics = self.ensemble.train_ensemble(
                X_train, y_train,
                X_test, y_test,
                use_gpu=use_gpu
            )
            
            # ä¿å­˜é›†æˆæ¨¡å‹
            if models:
                self.ensemble.save()
            
            return self.ensemble, metrics
            
        except Exception as e:
            logger.error(f"é›†æˆæ¨¡å‹è¨“ç·´å¤±æ•—: {e}", exc_info=True)
            return None, {}

    
    def _analyze_feature_importance(self, model, X: pd.DataFrame) -> pd.DataFrame:
        """åˆ†æç‰¹å¾µé‡è¦æ€§ï¼ˆv3.9.2.2æ–°å¢ï¼‰"""
        try:
            importance = model.feature_importances_
            feature_importance = pd.DataFrame({
                "feature": X.columns,
                "importance": importance
            }).sort_values("importance", ascending=False)
            
            top_3_sum = feature_importance.head(3)["importance"].sum()
            top_5_sum = feature_importance.head(5)["importance"].sum()
            
            logger.info("\n" + "=" * 60)
            logger.info("ğŸ“Š ç‰¹å¾µé‡è¦æ€§åˆ†æï¼ˆv3.9.2.2ï¼‰")
            logger.info("=" * 60)
            logger.info(f"å‰3å€‹ç‰¹å¾µé‡è¦æ€§ï¼š{top_3_sum:.1%}")
            logger.info(f"å‰5å€‹ç‰¹å¾µé‡è¦æ€§ï¼š{top_5_sum:.1%}")
            
            if top_3_sum > 0.7:
                logger.warning(f"âš ï¸  ç‰¹å¾µéåº¦é›†ä¸­ï¼šå‰3å€‹ç‰¹å¾µå {top_3_sum:.1%}")
            else:
                logger.info("âœ… ç‰¹å¾µé‡è¦æ€§åˆ†å¸ƒåˆç†")
            
            logger.info("\nå‰10é‡è¦ç‰¹å¾µï¼š")
            for idx, row in feature_importance.head(10).iterrows():
                logger.info(f"  {row['feature']:30s}: {row['importance']:.4f}")
            logger.info("=" * 60 + "\n")
            
            return feature_importance
        except Exception as e:
            logger.error(f"ç‰¹å¾µé‡è¦æ€§åˆ†æå¤±æ•—: {e}")
            return pd.DataFrame()

