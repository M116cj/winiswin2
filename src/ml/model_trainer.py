"""
XGBoost æ¨¡å‹è¨“ç·´å™¨ï¼ˆv3.4.0å„ªåŒ–ç‰ˆï¼‰
è·è²¬ï¼šæ¨¡å‹è¨“ç·´ã€è¶…åƒæ•¸å„ªåŒ–ã€æ¨¡å‹è©•ä¼°ã€å¢é‡å­¸ç¿’ã€æ¨¡å‹é›†æˆ
"""

import numpy as np
import pandas as pd
from typing import Dict, Tuple, Optional
import pickle
import os
import logging
from datetime import datetime
import time

from src.config import Config
from src.ml.data_processor import MLDataProcessor
from src.ml.hyperparameter_tuner import HyperparameterTuner
from src.ml.adaptive_learner import AdaptiveLearner
from src.ml.ensemble_model import EnsembleModel
from src.ml.label_leakage_validator import LabelLeakageValidator
from src.ml.imbalance_handler import ImbalanceHandler
from src.ml.drift_detector import DriftDetector

logger = logging.getLogger(__name__)


class XGBoostTrainer:
    """XGBoost æ¨¡å‹è¨“ç·´å™¨ï¼ˆv3.4.0å¢å¼·ç‰ˆï¼‰"""
    
    def __init__(self, use_tuning: bool = False, use_ensemble: bool = False):
        """
        åˆå§‹åŒ–è¨“ç·´å™¨
        
        Args:
            use_tuning: æ˜¯å¦ä½¿ç”¨è¶…åƒæ•¸èª¿å„ª
            use_ensemble: æ˜¯å¦ä½¿ç”¨æ¨¡å‹é›†æˆ
        """
        self.config = Config
        self.data_processor = MLDataProcessor()
        self.model = None
        self.model_path = "data/models/xgboost_model.pkl"
        self.metrics_path = "data/models/model_metrics.json"
        
        # âœ¨ v3.4.0æ–°å¢ï¼šé«˜ç´šåŠŸèƒ½
        self.use_tuning = use_tuning
        self.use_ensemble = use_ensemble
        self.tuner = HyperparameterTuner() if use_tuning else None
        self.adaptive_learner = AdaptiveLearner()
        self.ensemble = EnsembleModel() if use_ensemble else None
        
        # ğŸš€ v3.9.0æ–°å¢ï¼šå„ªåŒ–åŠŸèƒ½
        self.leakage_validator = LabelLeakageValidator()
        self.imbalance_handler = ImbalanceHandler()
        self.drift_detector = DriftDetector(window_size=1000, drift_threshold=0.05)
        
        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)
    
    def train(
        self,
        params: Optional[Dict] = None,
        use_gpu: bool = True,
        incremental: bool = False
    ) -> Tuple[object, Dict]:
        """
        è¨“ç·´ XGBoost æ¨¡å‹ï¼ˆv3.4.0å¢å¼·ç‰ˆï¼‰
        
        Args:
            params: æ¨¡å‹åƒæ•¸
            use_gpu: æ˜¯å¦ä½¿ç”¨ GPU
            incremental: æ˜¯å¦ä½¿ç”¨å¢é‡å­¸ç¿’
        
        Returns:
            Tuple[object, Dict]: (æ¨¡å‹, è©•ä¼°æŒ‡æ¨™)
        """
        try:
            import xgboost as xgb
            from sklearn.metrics import (
                accuracy_score, precision_score, recall_score,
                f1_score, roc_auc_score, confusion_matrix
            )
            
            # âœ¨ v3.4.0ï¼šé–‹å§‹æ€§èƒ½è¿½è¹¤
            start_time = time.time()
            
            # åŠ è¼‰æ•¸æ“š
            df = self.data_processor.load_training_data()
            
            min_samples = self.config.ML_MIN_TRAINING_SAMPLES
            if df.empty or len(df) < min_samples:
                logger.warning(f"è¨“ç·´æ•¸æ“šä¸è¶³: {len(df)} æ¢è¨˜éŒ„ (æœ€å°‘éœ€è¦ {min_samples})")
                return None, {}
            
            # ğŸ” v3.9.0ï¼šæ¨™ç±¤æ³„æ¼é©—è­‰
            leakage_report = self.leakage_validator.validate_training_data(df)
            if leakage_report['has_leakage']:
                logger.warning(f"âš ï¸ æª¢æ¸¬åˆ°æ½›åœ¨æ¨™ç±¤æ³„æ¼ï¼š{leakage_report['leakage_features']}")
            
            # ğŸ“Š v3.9.0ï¼šæ‡‰ç”¨æ»‘å‹•çª—å£ï¼ˆé˜²æ­¢èˆŠæ•¸æ“šå½±éŸ¿éå¤§ï¼‰
            df = self.drift_detector.apply_sliding_window(df, window_size=1000)
            
            # æº–å‚™ç‰¹å¾µ
            X, y = self.data_processor.prepare_features(df)
            
            if X.empty or y.empty:
                logger.error("ç‰¹å¾µæº–å‚™å¤±æ•—")
                return None, {}
            
            # ğŸ“Š v3.9.0ï¼šé¡åˆ¥å¹³è¡¡åˆ†æ
            balance_report = self.imbalance_handler.analyze_class_balance(y, X)
            
            # ğŸ” v3.9.0ï¼šç‰¹å¾µåˆ†å¸ƒæ¼‚ç§»æª¢æ¸¬
            drift_report = self.drift_detector.detect_feature_drift(
                X,
                X.columns.tolist(),
                update_baseline=True
            )
            
            # åˆ†å‰²æ•¸æ“š
            X_train, X_test, y_train, y_test = self.data_processor.split_data(X, y)
            
            # ğŸ›¡ï¸ v3.9.0ï¼šè¨ˆç®—æ¨£æœ¬æ¬Šé‡ï¼ˆè™•ç†é¡åˆ¥ä¸å¹³è¡¡ï¼‰
            sample_weights = None
            if balance_report.get('needs_balancing', False):
                logger.info("ğŸ“Š æª¢æ¸¬åˆ°é¡åˆ¥ä¸å¹³è¡¡ï¼Œè¨ˆç®—å‹•æ…‹æ¨£æœ¬æ¬Šé‡...")
                sample_weights = self.imbalance_handler.calculate_sample_weight(y_train, method='balanced')
                
                # åŒæ™‚çµåˆæ™‚é–“è¡°æ¸›æ¬Šé‡
                time_weights = self.drift_detector.calculate_sample_weights(
                    pd.DataFrame({'y': y_train}),
                    decay_factor=0.95
                )
                
                # çµ„åˆæ¬Šé‡
                if sample_weights is not None:
                    sample_weights = sample_weights * time_weights
            
            # âœ¨ v3.4.0ï¼šè¶…åƒæ•¸èª¿å„ª
            if params is None:
                if self.use_tuning and self.tuner:
                    logger.info("å•Ÿå‹•è¶…åƒæ•¸è‡ªå‹•èª¿å„ª...")
                    params, _ = self.tuner.quick_tune(X_train, y_train, use_gpu)
                else:
                    # é»˜èªåƒæ•¸
                    params = {
                        'max_depth': 6,
                        'learning_rate': 0.1,
                        'n_estimators': 200,
                        'objective': 'binary:logistic',
                        'eval_metric': 'auc',
                        'subsample': 0.8,
                        'colsample_bytree': 0.8,
                        'min_child_weight': 1,
                        'gamma': 0.1,
                        'reg_alpha': 0.1,
                        'reg_lambda': 1.0,
                        'random_state': 42,
                        'n_jobs': 32  # ä½¿ç”¨ 32 æ ¸å¿ƒ
                    }
                    
                    # ğŸ›¡ï¸ v3.9.0ï¼šæ·»åŠ æˆæœ¬æ„ŸçŸ¥åƒæ•¸ï¼ˆè™•ç†ä¸å¹³è¡¡ï¼‰
                    if balance_report.get('needs_balancing', False):
                        scale_pos_weight = self.imbalance_handler.get_scale_pos_weight(y_train)
                        params['scale_pos_weight'] = scale_pos_weight
                        logger.info(f"ğŸ“Š å•Ÿç”¨æˆæœ¬æ„ŸçŸ¥å­¸ç¿’ï¼šscale_pos_weight = {scale_pos_weight:.2f}")
                    
                    # GPU åŠ é€Ÿ
                    if use_gpu:
                        if self._detect_gpu():
                            params['tree_method'] = 'gpu_hist'
                            params['predictor'] = 'gpu_predictor'
                            logger.info("âœ… ä½¿ç”¨ GPU åŠ é€Ÿè¨“ç·´")
                        else:
                            logger.warning("GPU ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU è¨“ç·´")
                            params['tree_method'] = 'hist'
                    else:
                        params['tree_method'] = 'hist'
            
            # âœ¨ v3.4.0ï¼šå¢é‡å­¸ç¿’æ”¯æŒ
            logger.info("é–‹å§‹è¨“ç·´ XGBoost æ¨¡å‹...")
            logger.info(f"è¨“ç·´é›†å¤§å°: {X_train.shape}, æ¸¬è©¦é›†å¤§å°: {X_test.shape}")
            
            model = xgb.XGBClassifier(**params)
            
            # å¢é‡å­¸ç¿’ï¼šåŠ è¼‰èˆŠæ¨¡å‹ç¹¼çºŒè¨“ç·´
            xgb_model_file = None
            temp_model_path = 'data/models/temp_xgb_model.json'
            
            if incremental and os.path.exists(self.model_path):
                try:
                    with open(self.model_path, 'rb') as f:
                        old_model = pickle.load(f)
                    
                    # ä¿å­˜èˆŠæ¨¡å‹ç‚ºJSONæ ¼å¼ï¼ˆXGBoostå¢é‡å­¸ç¿’éœ€è¦ï¼‰
                    old_model.save_model(temp_model_path)
                    xgb_model_file = temp_model_path
                    
                    logger.info(f"âœ… å¢é‡å­¸ç¿’å•Ÿç”¨ï¼šåŸºæ–¼èˆŠæ¨¡å‹ç¹¼çºŒè¨“ç·´")
                except Exception as e:
                    logger.warning(f"åŠ è¼‰èˆŠæ¨¡å‹å¤±æ•—ï¼ŒåŸ·è¡Œå®Œæ•´è¨“ç·´: {e}")
                    xgb_model_file = None
            
            # è¨“ç·´ï¼ˆå¢é‡æˆ–å®Œæ•´ï¼‰- ä½¿ç”¨try-finallyç¢ºä¿è‡¨æ™‚æ–‡ä»¶æ¸…ç†
            try:
                if xgb_model_file:
                    # å¢é‡å­¸ç¿’ï¼ˆå¸¶æ¨£æœ¬æ¬Šé‡ï¼‰
                    if sample_weights is not None:
                        model.fit(
                            X_train, y_train,
                            sample_weight=sample_weights,
                            eval_set=[(X_train, y_train), (X_test, y_test)],
                            early_stopping_rounds=20,
                            verbose=False,
                            xgb_model=xgb_model_file  # âœ¨ å¢é‡å­¸ç¿’é—œéµåƒæ•¸
                        )
                    else:
                        model.fit(
                            X_train, y_train,
                            eval_set=[(X_train, y_train), (X_test, y_test)],
                            early_stopping_rounds=20,
                            verbose=False,
                            xgb_model=xgb_model_file
                        )
                else:
                    # å®Œæ•´è¨“ç·´ï¼ˆå¸¶æ¨£æœ¬æ¬Šé‡ï¼‰
                    if sample_weights is not None:
                        model.fit(
                            X_train, y_train,
                            sample_weight=sample_weights,
                            eval_set=[(X_train, y_train), (X_test, y_test)],
                            early_stopping_rounds=20,
                            verbose=False
                        )
                    else:
                        model.fit(
                            X_train, y_train,
                            eval_set=[(X_train, y_train), (X_test, y_test)],
                            early_stopping_rounds=20,
                            verbose=False
                        )
            finally:
                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶ï¼ˆç¢ºä¿å³ä½¿è¨“ç·´å¤±æ•—ä¹Ÿæœƒæ¸…ç†ï¼‰
                if xgb_model_file and os.path.exists(temp_model_path):
                    try:
                        os.remove(temp_model_path)
                        logger.debug("è‡¨æ™‚æ¨¡å‹æ–‡ä»¶å·²æ¸…ç†")
                    except:
                        pass
            
            # é æ¸¬
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)[:, 1]
            
            # è©•ä¼°æŒ‡æ¨™
            metrics = {
                'training_samples': len(df),  # âœ¨ ä¿å­˜ç¸½è¨“ç·´æ¨£æœ¬æ•¸ï¼ˆç”¨æ–¼æŒçºŒè¨“ç·´ï¼‰
                'accuracy': float(accuracy_score(y_test, y_pred)),
                'precision': float(precision_score(y_test, y_pred, zero_division='warn')),
                'recall': float(recall_score(y_test, y_pred, zero_division='warn')),
                'f1_score': float(f1_score(y_test, y_pred, zero_division='warn')),
                'roc_auc': float(roc_auc_score(y_test, y_pred_proba)),
                'train_set_size': len(X_train),
                'test_set_size': len(X_test),
                'trained_at': datetime.now().isoformat()
            }
            
            # ğŸ“Š v3.9.0ï¼šè©³ç´°æ··æ·†çŸ©é™£å ±å‘Šï¼ˆåŒ…å«åˆ†æ–¹å‘è©•ä¼°ï¼‰
            confusion_report = self.imbalance_handler.generate_confusion_matrix_report(
                y_test.values,
                y_pred,
                X_test
            )
            metrics['confusion_matrix_detailed'] = confusion_report
            
            # ä¿ç•™åŸæœ‰æ··æ·†çŸ©é™£æ ¼å¼ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
            cm = confusion_matrix(y_test, y_pred)
            metrics['confusion_matrix'] = cm.tolist()
            
            # ğŸ“Š v3.9.0ï¼šæ·»åŠ å„ªåŒ–å ±å‘Š
            metrics['optimization_reports'] = {
                'label_leakage': leakage_report,
                'class_balance': balance_report,
                'feature_drift': drift_report
            }
            
            # ç‰¹å¾µé‡è¦æ€§
            feature_importance = self.data_processor.get_feature_importance(
                model,
                X.columns.tolist()
            )
            metrics['feature_importance'] = feature_importance
            
            logger.info("=" * 60)
            logger.info("æ¨¡å‹è¨“ç·´å®Œæˆ")
            logger.info(f"æº–ç¢ºç‡: {metrics['accuracy']:.4f}")
            logger.info(f"ç²¾ç¢ºç‡: {metrics['precision']:.4f}")
            logger.info(f"å¬å›ç‡: {metrics['recall']:.4f}")
            logger.info(f"F1 åˆ†æ•¸: {metrics['f1_score']:.4f}")
            logger.info(f"ROC-AUC: {metrics['roc_auc']:.4f}")
            
            # âœ¨ v3.4.0ï¼šè¨“ç·´æ€§èƒ½è¿½è¹¤
            training_time = time.time() - start_time
            metrics['training_time_seconds'] = training_time
            metrics['incremental'] = incremental
            logger.info(f"è¨“ç·´è€—æ™‚: {training_time:.2f}ç§’")
            
            # âœ¨ v3.4.0ï¼šè‡ªé©æ‡‰å­¸ç¿’æ›´æ–°
            self.adaptive_learner.update_performance(
                metrics['accuracy'],
                datetime.now()
            )
            
            logger.info("=" * 60)
            
            self.model = model
            
            return model, metrics
            
        except ImportError:
            logger.error("XGBoost æˆ– scikit-learn æœªå®‰è£")
            return None, {}
        except Exception as e:
            logger.error(f"è¨“ç·´æ¨¡å‹å¤±æ•—: {e}", exc_info=True)
            return None, {}
    
    def save_model(self, model, metrics: Dict):
        """
        ä¿å­˜æ¨¡å‹å’ŒæŒ‡æ¨™
        
        Args:
            model: è¨“ç·´å¥½çš„æ¨¡å‹
            metrics: è©•ä¼°æŒ‡æ¨™
        """
        try:
            # ä¿å­˜æ¨¡å‹
            with open(self.model_path, 'wb') as f:
                pickle.dump(model, f)
            logger.info(f"æ¨¡å‹å·²ä¿å­˜: {self.model_path}")
            
            # ä¿å­˜æŒ‡æ¨™
            import json
            with open(self.metrics_path, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, ensure_ascii=False, indent=2)
            logger.info(f"æŒ‡æ¨™å·²ä¿å­˜: {self.metrics_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹å¤±æ•—: {e}")
    
    def load_model(self) -> Optional[object]:
        """
        åŠ è¼‰å·²è¨“ç·´çš„æ¨¡å‹
        
        Returns:
            Optional[object]: æ¨¡å‹å°è±¡
        """
        try:
            if not os.path.exists(self.model_path):
                logger.warning(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
                return None
            
            with open(self.model_path, 'rb') as f:
                model = pickle.load(f)
            
            logger.info(f"æ¨¡å‹å·²åŠ è¼‰: {self.model_path}")
            self.model = model
            
            return model
            
        except Exception as e:
            logger.error(f"åŠ è¼‰æ¨¡å‹å¤±æ•—: {e}")
            return None
    
    def auto_train_if_needed(self, min_samples: int = 100) -> bool:
        """
        å¦‚æœæœ‰è¶³å¤ æ•¸æ“šå‰‡è‡ªå‹•è¨“ç·´æ¨¡å‹
        
        Args:
            min_samples: æœ€å°è¨“ç·´æ¨£æœ¬æ•¸
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸè¨“ç·´
        """
        try:
            df = self.data_processor.load_training_data()
            
            if len(df) < min_samples:
                logger.info(f"è¨“ç·´æ•¸æ“šä¸è¶³: {len(df)}/{min_samples} (é…ç½®: ML_MIN_TRAINING_SAMPLES)")
                return False
            
            logger.info(f"æª¢æ¸¬åˆ° {len(df)} æ¢è¨“ç·´æ•¸æ“šï¼Œé–‹å§‹è‡ªå‹•è¨“ç·´...")
            
            model, metrics = self.train()
            
            if model is not None:
                self.save_model(model, metrics)
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"è‡ªå‹•è¨“ç·´å¤±æ•—: {e}")
            return False
    
    def _detect_gpu(self) -> bool:
        """
        æª¢æ¸¬GPUæ˜¯å¦å¯ç”¨ï¼ˆv3.4.0æ–°å¢ï¼‰
        
        Returns:
            bool: GPUæ˜¯å¦å¯ç”¨
        """
        try:
            import subprocess
            subprocess.check_output(['nvidia-smi'])
            return True
        except:
            return False
    
    def train_with_ensemble(self, use_gpu: bool = True) -> Tuple[Optional[object], Dict]:
        """
        ä½¿ç”¨æ¨¡å‹é›†æˆè¨“ç·´ï¼ˆv3.4.0æ–°å¢ï¼‰
        
        Args:
            use_gpu: æ˜¯å¦ä½¿ç”¨GPU
        
        Returns:
            Tuple[Optional[object], Dict]: (é›†æˆæ¨¡å‹, è©•ä¼°æŒ‡æ¨™)
        """
        if not self.use_ensemble or not self.ensemble:
            logger.warning("æ¨¡å‹é›†æˆæœªå•Ÿç”¨")
            return None, {}
        
        try:
            # åŠ è¼‰æ•¸æ“š
            df = self.data_processor.load_training_data()
            X, y = self.data_processor.prepare_features(df)
            X_train, X_test, y_train, y_test = self.data_processor.split_data(X, y)
            
            # è¨“ç·´é›†æˆæ¨¡å‹
            models, metrics = self.ensemble.train_ensemble(
                X_train, y_train,
                X_test, y_test,
                use_gpu=use_gpu
            )
            
            # ä¿å­˜é›†æˆæ¨¡å‹
            if models:
                self.ensemble.save()
            
            return self.ensemble, metrics
            
        except Exception as e:
            logger.error(f"é›†æˆæ¨¡å‹è¨“ç·´å¤±æ•—: {e}", exc_info=True)
            return None, {}
