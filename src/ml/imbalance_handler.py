"""
æ ·æœ¬ä¸å¹³è¡¡å¤„ç†å™¨
èŒè´£ï¼šå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ã€ç”Ÿæˆæ··æ·†çŸ©é˜µã€åˆ†æ–¹å‘è¯„ä¼°
"""

import numpy as np
import pandas as pd
from typing import Dict, Tuple, Optional
import logging
from sklearn.metrics import confusion_matrix, classification_report
from collections import Counter

logger = logging.getLogger(__name__)


class ImbalanceHandler:
    """æ ·æœ¬ä¸å¹³è¡¡å¤„ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        self.balance_threshold = 0.3  # æœ€å°ç±»åˆ«å æ¯”é˜ˆå€¼
    
    def analyze_class_balance(self, y: pd.Series, X: Optional[pd.DataFrame] = None) -> Dict:
        """
        åˆ†æžç±»åˆ«å¹³è¡¡æƒ…å†µ
        
        Args:
            y: ç›®æ ‡å˜é‡
            X: ç‰¹å¾æ•°æ®ï¼ˆå¯é€‰ï¼Œç”¨äºŽåˆ†æžæ–¹å‘å¹³è¡¡ï¼‰
        
        Returns:
            Dict: å¹³è¡¡åˆ†æžæŠ¥å‘Š
        """
        # æ€»ä½“ç±»åˆ«åˆ†å¸ƒ
        class_counts = y.value_counts().to_dict()
        total_samples = len(y)
        
        class_distribution = {}
        for cls, count in class_counts.items():
            class_distribution[f'class_{cls}'] = {
                'count': count,
                'percentage': count / total_samples * 100
            }
        
        # è®¡ç®—ä¸å¹³è¡¡æ¯”çŽ‡
        if len(class_counts) == 2:
            minority_count = min(class_counts.values())
            majority_count = max(class_counts.values())
            imbalance_ratio = majority_count / minority_count if minority_count > 0 else float('inf')
        else:
            imbalance_ratio = 1.0
        
        report = {
            'total_samples': total_samples,
            'class_distribution': class_distribution,
            'imbalance_ratio': imbalance_ratio,
            'is_balanced': imbalance_ratio < 2.0,  # 2:1ä»¥å†…è®¤ä¸ºå¹³è¡¡
            'needs_balancing': imbalance_ratio >= 2.0
        }
        
        # å¦‚æžœæä¾›äº†ç‰¹å¾æ•°æ®ï¼Œåˆ†æžæ–¹å‘å¹³è¡¡
        if X is not None and 'direction_encoded' in X.columns:
            direction_balance = self._analyze_direction_balance(X, y)
            report['direction_balance'] = direction_balance
        
        logger.info(
            f"ðŸ“Š ç±»åˆ«å¹³è¡¡åˆ†æžï¼šä¸å¹³è¡¡æ¯”çŽ‡ {imbalance_ratio:.2f}, "
            f"åˆ†å¸ƒ {class_distribution}"
        )
        
        return report
    
    def _analyze_direction_balance(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """
        åˆ†æžåšå¤š/åšç©ºæ–¹å‘çš„å¹³è¡¡æ€§
        
        Args:
            X: ç‰¹å¾æ•°æ®
            y: ç›®æ ‡å˜é‡
        
        Returns:
            Dict: æ–¹å‘å¹³è¡¡æŠ¥å‘Š
        """
        df = pd.DataFrame({'direction': X['direction_encoded'], 'is_winner': y})
        
        # LONG=1, SHORT=-1
        long_stats = df[df['direction'] == 1]['is_winner']
        short_stats = df[df['direction'] == -1]['is_winner']
        
        report = {
            'long': {
                'total': len(long_stats),
                'winners': long_stats.sum() if len(long_stats) > 0 else 0,
                'win_rate': long_stats.mean() if len(long_stats) > 0 else 0
            },
            'short': {
                'total': len(short_stats),
                'winners': short_stats.sum() if len(short_stats) > 0 else 0,
                'win_rate': short_stats.mean() if len(short_stats) > 0 else 0
            }
        }
        
        # æ£€æŸ¥æ–¹å‘åå·®
        total = len(df)
        long_pct = len(long_stats) / total if total > 0 else 0
        short_pct = len(short_stats) / total if total > 0 else 0
        
        report['balance'] = {
            'long_percentage': long_pct * 100,
            'short_percentage': short_pct * 100,
            'is_balanced': abs(long_pct - short_pct) < 0.3  # å·®å¼‚<30%è®¤ä¸ºå¹³è¡¡
        }
        
        logger.info(
            f"ðŸ“Š æ–¹å‘å¹³è¡¡ï¼šLONG {report['long']['total']}ç¬”({long_pct:.1%}), "
            f"SHORT {report['short']['total']}ç¬”({short_pct:.1%})"
        )
        
        return report
    
    def calculate_sample_weight(
        self,
        y: pd.Series,
        method: str = 'balanced'
    ) -> np.ndarray:
        """
        è®¡ç®—æ ·æœ¬æƒé‡ä»¥å¤„ç†ä¸å¹³è¡¡
        
        Args:
            y: ç›®æ ‡å˜é‡
            method: æƒé‡è®¡ç®—æ–¹æ³• ('balanced', 'sqrt', 'log')
        
        Returns:
            np.ndarray: æ ·æœ¬æƒé‡
        """
        class_counts = Counter(y)
        total_samples = len(y)
        
        if method == 'balanced':
            # sklearnçš„balancedæ¨¡å¼
            weights = {}
            for cls, count in class_counts.items():
                weights[cls] = total_samples / (len(class_counts) * count)
        
        elif method == 'sqrt':
            # å¹³æ–¹æ ¹ç¼©æ”¾ï¼ˆæ›´æ¸©å’Œï¼‰
            weights = {}
            for cls, count in class_counts.items():
                weights[cls] = np.sqrt(total_samples / count)
        
        elif method == 'log':
            # å¯¹æ•°ç¼©æ”¾ï¼ˆæœ€æ¸©å’Œï¼‰
            weights = {}
            for cls, count in class_counts.items():
                weights[cls] = np.log1p(total_samples / count)
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æƒé‡æ–¹æ³•ï¼š{method}")
        
        # å½’ä¸€åŒ–æƒé‡
        weight_sum = sum(weights.values())
        weights = {cls: w / weight_sum * len(class_counts) for cls, w in weights.items()}
        
        # ç”Ÿæˆæ ·æœ¬æƒé‡æ•°ç»„
        sample_weights = np.array([weights[label] for label in y])
        
        logger.info(f"âœ… è®¡ç®—æ ·æœ¬æƒé‡ï¼ˆ{method}ï¼‰ï¼šç±»åˆ«æƒé‡ {weights}")
        
        return sample_weights
    
    def get_scale_pos_weight(self, y: pd.Series) -> float:
        """
        è®¡ç®—XGBoostçš„scale_pos_weightå‚æ•°
        
        ç”¨äºŽäºŒåˆ†ç±»ä¸å¹³è¡¡é—®é¢˜
        scale_pos_weight = num_negative / num_positive
        
        Args:
            y: ç›®æ ‡å˜é‡
        
        Returns:
            float: scale_pos_weightå€¼
        """
        class_counts = Counter(y)
        
        if len(class_counts) != 2:
            logger.warning("scale_pos_weightä»…é€‚ç”¨äºŽäºŒåˆ†ç±»é—®é¢˜")
            return 1.0
        
        # å‡è®¾1æ˜¯æ­£ç±»ï¼ˆwinnerï¼‰ï¼Œ0æ˜¯è´Ÿç±»ï¼ˆloserï¼‰
        num_positive = class_counts.get(1, 1)
        num_negative = class_counts.get(0, 1)
        
        scale_weight = num_negative / num_positive
        
        logger.info(
            f"ðŸ“Š XGBoostä¸å¹³è¡¡å‚æ•°ï¼šscale_pos_weight = {scale_weight:.2f} "
            f"(è´Ÿæ ·æœ¬ {num_negative} / æ­£æ ·æœ¬ {num_positive})"
        )
        
        return scale_weight
    
    def generate_confusion_matrix_report(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        X: Optional[pd.DataFrame] = None
    ) -> Dict:
        """
        ç”Ÿæˆè¯¦ç»†çš„æ··æ·†çŸ©é˜µæŠ¥å‘Š
        
        Args:
            y_true: çœŸå®žæ ‡ç­¾
            y_pred: é¢„æµ‹æ ‡ç­¾
            X: ç‰¹å¾æ•°æ®ï¼ˆå¯é€‰ï¼Œç”¨äºŽåˆ†æ–¹å‘è¯„ä¼°ï¼‰
        
        Returns:
            Dict: æ··æ·†çŸ©é˜µæŠ¥å‘Š
        """
        # æ€»ä½“æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)
        
        total = tn + fp + fn + tp
        accuracy = (tp + tn) / total if total > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        report = {
            'confusion_matrix': {
                'true_negative': int(tn),
                'false_positive': int(fp),
                'false_negative': int(fn),
                'true_positive': int(tp)
            },
            'overall_metrics': {
                'accuracy': float(accuracy),
                'precision': float(precision),
                'recall': float(recall),
                'f1_score': float(f1)
            }
        }
        
        # å¦‚æžœæä¾›äº†ç‰¹å¾æ•°æ®ï¼Œåˆ†æ–¹å‘è¯„ä¼°
        if X is not None and 'direction_encoded' in X.columns:
            direction_metrics = self._evaluate_by_direction(y_true, y_pred, X)
            report['direction_metrics'] = direction_metrics
        
        # æ‰“å°æ··æ·†çŸ©é˜µ
        logger.info("\n" + "=" * 50)
        logger.info("ðŸŽ¯ æ··æ·†çŸ©é˜µæŠ¥å‘Š")
        logger.info("=" * 50)
        logger.info(f"              é¢„æµ‹è´Ÿç±»    é¢„æµ‹æ­£ç±»")
        logger.info(f"å®žé™…è´Ÿç±»        {tn:6d}      {fp:6d}")
        logger.info(f"å®žé™…æ­£ç±»        {fn:6d}      {tp:6d}")
        logger.info("=" * 50)
        logger.info(f"å‡†ç¡®çŽ‡ (Accuracy):  {accuracy:.4f}")
        logger.info(f"ç²¾ç¡®çŽ‡ (Precision): {precision:.4f}")
        logger.info(f"å¬å›žçŽ‡ (Recall):    {recall:.4f}")
        logger.info(f"F1åˆ†æ•° (F1-Score):  {f1:.4f}")
        logger.info("=" * 50)
        
        return report
    
    def _evaluate_by_direction(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        X: pd.DataFrame
    ) -> Dict:
        """
        æŒ‰åšå¤š/åšç©ºæ–¹å‘åˆ†åˆ«è¯„ä¼°
        
        Args:
            y_true: çœŸå®žæ ‡ç­¾
            y_pred: é¢„æµ‹æ ‡ç­¾
            X: ç‰¹å¾æ•°æ®
        
        Returns:
            Dict: åˆ†æ–¹å‘è¯„ä¼°æŠ¥å‘Š
        """
        df = pd.DataFrame({
            'direction': X['direction_encoded'],
            'y_true': y_true,
            'y_pred': y_pred
        })
        
        # LONGæ–¹å‘è¯„ä¼°
        long_df = df[df['direction'] == 1]
        long_metrics = self._calculate_metrics(
            long_df['y_true'].values,
            long_df['y_pred'].values
        )
        
        # SHORTæ–¹å‘è¯„ä¼°
        short_df = df[df['direction'] == -1]
        short_metrics = self._calculate_metrics(
            short_df['y_true'].values,
            short_df['y_pred'].values
        )
        
        report = {
            'long': {
                'samples': len(long_df),
                **long_metrics
            },
            'short': {
                'samples': len(short_df),
                **short_metrics
            }
        }
        
        logger.info("\nðŸ“Š åˆ†æ–¹å‘è¯„ä¼°ï¼š")
        logger.info(f"  LONG:  æ ·æœ¬æ•°{report['long']['samples']:4d}, "
                   f"å‡†ç¡®çŽ‡{report['long']['accuracy']:.2%}, "
                   f"F1={report['long']['f1_score']:.4f}")
        logger.info(f"  SHORT: æ ·æœ¬æ•°{report['short']['samples']:4d}, "
                   f"å‡†ç¡®çŽ‡{report['short']['accuracy']:.2%}, "
                   f"F1={report['short']['f1_score']:.4f}")
        
        return report
    
    def _calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        if len(y_true) == 0:
            return {
                'accuracy': 0.0,
                'precision': 0.0,
                'recall': 0.0,
                'f1_score': 0.0
            }
        
        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])
        
        if cm.size == 4:
            tn, fp, fn, tp = cm.ravel()
        else:
            tn, fp, fn, tp = 0, 0, 0, 0
        
        total = tn + fp + fn + tp
        accuracy = (tp + tn) / total if total > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1)
        }
