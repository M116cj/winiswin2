"""
ML é æ¸¬æœå‹™
è·è²¬ï¼šå¯¦æ™‚é æ¸¬ã€ä¿¡å¿ƒåº¦æ ¡æº–ã€é æ¸¬çµæœé›†æˆ
"""

import numpy as np
import pandas as pd
from typing import Dict, Optional
import logging

from src.ml.model_trainer import XGBoostTrainer
from src.ml.data_processor import MLDataProcessor

logger = logging.getLogger(__name__)


class MLPredictor:
    """ML é æ¸¬æœå‹™"""
    
    def __init__(self):
        """åˆå§‹åŒ–é æ¸¬å™¨"""
        self.trainer = XGBoostTrainer()
        self.data_processor = MLDataProcessor()
        self.model = None
        self.is_ready = False
        self.last_training_samples = 0  # ä¸Šæ¬¡è¨“ç·´æ™‚çš„æ¨£æœ¬æ•¸
        self.retrain_threshold = 50  # ç´¯ç©50ç­†æ–°äº¤æ˜“å¾Œé‡è¨“ç·´
    
    def initialize(self) -> bool:
        """
        åˆå§‹åŒ–é æ¸¬å™¨ï¼ˆåŠ è¼‰æ¨¡å‹ï¼‰
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆå§‹åŒ–
        """
        try:
            # å˜—è©¦åŠ è¼‰å·²æœ‰æ¨¡å‹
            self.model = self.trainer.load_model()
            
            if self.model is None:
                logger.info("æœªæ‰¾åˆ°å·²è¨“ç·´æ¨¡å‹ï¼Œå˜—è©¦è‡ªå‹•è¨“ç·´...")
                
                # å¦‚æœæœ‰è¶³å¤ æ•¸æ“šï¼Œè‡ªå‹•è¨“ç·´
                success = self.trainer.auto_train_if_needed(min_samples=100)
                
                if success:
                    self.model = self.trainer.model
            
            if self.model is not None:
                self.is_ready = True
                # è¨˜éŒ„åˆå§‹è¨“ç·´æ™‚çš„æ¨£æœ¬æ•¸ï¼ˆå¾metricsè®€å–æˆ–ç•¶å‰æ•¸æ“šï¼‰
                self.last_training_samples = self._load_last_training_samples()
                logger.info(f"âœ… ML é æ¸¬å™¨å·²å°±ç·’ (è¨“ç·´æ¨£æœ¬: {self.last_training_samples})")
                return True
            else:
                logger.warning("âš ï¸  ML æ¨¡å‹æœªå°±ç·’ï¼Œå°‡ä½¿ç”¨å‚³çµ±ç­–ç•¥")
                return False
                
        except Exception as e:
            logger.error(f"åˆå§‹åŒ– ML é æ¸¬å™¨å¤±æ•—: {e}")
            return False
    
    def predict(self, signal: Dict) -> Optional[Dict]:
        """
        é æ¸¬ä¿¡è™Ÿçš„æˆåŠŸæ¦‚ç‡
        
        Args:
            signal: äº¤æ˜“ä¿¡è™Ÿ
        
        Returns:
            Optional[Dict]: é æ¸¬çµæœ
        """
        if not self.is_ready or self.model is None:
            return None
        
        try:
            # æº–å‚™ç‰¹å¾µ
            features = self._prepare_signal_features(signal)
            
            if features is None:
                return None
            
            # é æ¸¬
            proba = self.model.predict_proba([features])[0]
            prediction = self.model.predict([features])[0]
            
            result = {
                'predicted_class': int(prediction),
                'win_probability': float(proba[1]),
                'loss_probability': float(proba[0]),
                'ml_confidence': float(proba[1]) if prediction == 1 else float(proba[0])
            }
            
            logger.debug(f"ML é æ¸¬: {signal['symbol']} - å‹ç‡ {result['win_probability']:.2%}")
            
            return result
            
        except Exception as e:
            logger.error(f"ML é æ¸¬å¤±æ•—: {e}")
            return None
    
    def _prepare_signal_features(self, signal: Dict) -> Optional[list]:
        """
        å¾ä¿¡è™Ÿæº–å‚™ç‰¹å¾µå‘é‡
        
        Args:
            signal: äº¤æ˜“ä¿¡è™Ÿ
        
        Returns:
            Optional[list]: ç‰¹å¾µå‘é‡
        """
        try:
            indicators = signal.get('indicators', {})
            timeframes = signal.get('timeframes', {})
            
            # ç·¨ç¢¼è¶¨å‹¢
            trend_encoding = {'bullish': 1, 'bearish': -1, 'neutral': 0}
            market_structure_encoding = {'bullish': 1, 'bearish': -1, 'neutral': 0}
            direction_encoding = {'LONG': 1, 'SHORT': -1}
            
            features = [
                signal.get('confidence', 0),
                0,  # leverage - å°‡åœ¨é¢¨éšªç®¡ç†å™¨ä¸­ç¢ºå®š
                0,  # position_value - å°‡åœ¨äº¤æ˜“æœå‹™ä¸­ç¢ºå®š
                0,  # hold_duration_hours - æœªçŸ¥
                abs(signal.get('take_profit', 0) - signal.get('entry_price', 0)) / 
                    abs(signal.get('entry_price', 0) - signal.get('stop_loss', 0)) 
                    if signal.get('entry_price', 0) != signal.get('stop_loss', 0) else 2.0,
                signal.get('order_blocks', 0),
                signal.get('liquidity_zones', 0),
                indicators.get('rsi', 0),
                indicators.get('macd', 0),
                indicators.get('macd_signal', 0),
                indicators.get('macd_histogram', 0),
                indicators.get('atr', 0),
                indicators.get('bb_width_pct', 0),
                indicators.get('volume_sma_ratio', 0),
                indicators.get('price_vs_ema50', 0),
                indicators.get('price_vs_ema200', 0),
                trend_encoding.get(timeframes.get('1h', 'neutral'), 0),
                trend_encoding.get(timeframes.get('15m', 'neutral'), 0),
                trend_encoding.get(timeframes.get('5m', 'neutral'), 0),
                market_structure_encoding.get(signal.get('market_structure', 'neutral'), 0),
                direction_encoding.get(signal.get('direction', 'LONG'), 1)
            ]
            
            return features
            
        except Exception as e:
            logger.error(f"æº–å‚™ç‰¹å¾µå¤±æ•—: {e}")
            return None
    
    def calibrate_confidence(
        self,
        traditional_confidence: float,
        ml_prediction: Optional[Dict]
    ) -> float:
        """
        æ ¡æº–ä¿¡å¿ƒåº¦ï¼ˆçµåˆå‚³çµ±ç­–ç•¥å’Œ ML é æ¸¬ï¼‰
        
        Args:
            traditional_confidence: å‚³çµ±ç­–ç•¥ä¿¡å¿ƒåº¦
            ml_prediction: ML é æ¸¬çµæœ
        
        Returns:
            float: æ ¡æº–å¾Œçš„ä¿¡å¿ƒåº¦
        """
        if ml_prediction is None or not self.is_ready:
            return traditional_confidence
        
        try:
            ml_confidence = ml_prediction.get('ml_confidence', 0.5)
            
            # åŠ æ¬Šå¹³å‡
            # å‚³çµ±ç­–ç•¥æ¬Šé‡: 60%
            # ML é æ¸¬æ¬Šé‡: 40%
            calibrated = traditional_confidence * 0.6 + ml_confidence * 0.4
            
            return min(1.0, max(0.0, calibrated))
    
    def check_and_retrain_if_needed(self) -> bool:
        """
        æª¢æŸ¥æ˜¯å¦éœ€è¦é‡è¨“ç·´ï¼ˆåŸºæ–¼æ–°å¢æ•¸æ“šé‡ï¼‰
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸé‡è¨“ç·´
        """
        try:
            # åŠ è¼‰ç•¶å‰æ•¸æ“š
            df = self.data_processor.load_training_data()
            current_samples = len(df)
            
            # è¨ˆç®—æ–°å¢æ¨£æœ¬æ•¸
            new_samples = current_samples - self.last_training_samples
            
            # âš ï¸ é˜²ç¦¦ï¼šå¦‚æœæª¢æ¸¬åˆ°æ•¸æ“šæ¸›å°‘ï¼ˆä¾‹å¦‚æ•¸æ“šæ¸…ç†ï¼‰ï¼Œé‡ç½®è¨ˆæ•¸å™¨
            if new_samples < 0:
                logger.warning(
                    f"æª¢æ¸¬åˆ°æ•¸æ“šæ¸›å°‘: {current_samples} < {self.last_training_samples}ï¼Œ"
                    f"é‡ç½®è¨ˆæ•¸å™¨"
                )
                self.last_training_samples = current_samples
                return False
            
            if new_samples < self.retrain_threshold:
                logger.debug(
                    f"æ–°å¢æ¨£æœ¬æ•¸ä¸è¶³: {new_samples}/{self.retrain_threshold} "
                    f"(ç¸½æ¨£æœ¬: {current_samples})"
                )
                return False
            
            # è§¸ç™¼é‡è¨“ç·´
            logger.info(
                f"ğŸ”„ æª¢æ¸¬åˆ° {new_samples} ç­†æ–°äº¤æ˜“æ•¸æ“šï¼Œé–‹å§‹é‡è¨“ç·´æ¨¡å‹... "
                f"(ç¸½æ¨£æœ¬: {current_samples})"
            )
            
            model, metrics = self.trainer.train()
            
            if model is not None:
                self.trainer.save_model(model, metrics)
                self.model = model
                self.last_training_samples = current_samples
                
                logger.info(
                    f"âœ… æ¨¡å‹é‡è¨“ç·´å®Œæˆï¼"
                    f"æº–ç¢ºç‡: {metrics.get('accuracy', 0):.2%}, "
                    f"AUC: {metrics.get('roc_auc', 0):.3f}"
                )
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"é‡è¨“ç·´æª¢æŸ¥å¤±æ•—: {e}")
            return False
    
    def _load_last_training_samples(self) -> int:
        """
        å¾metricsæ–‡ä»¶åŠ è¼‰ä¸Šæ¬¡è¨“ç·´çš„æ¨£æœ¬æ•¸
        
        Returns:
            int: ä¸Šæ¬¡è¨“ç·´æ™‚çš„æ¨£æœ¬æ•¸
        """
        try:
            import json
            metrics_path = 'data/models/model_metrics.json'
            
            if os.path.exists(metrics_path):
                with open(metrics_path, 'r', encoding='utf-8') as f:
                    metrics = json.load(f)
                    samples = metrics.get('training_samples', 0)
                    if samples > 0:
                        logger.debug(f"å¾metricsåŠ è¼‰ä¸Šæ¬¡è¨“ç·´æ¨£æœ¬æ•¸: {samples}")
                        return samples
            
            # å¦‚æœæ²’æœ‰metricsï¼Œä½¿ç”¨ç•¶å‰æ•¸æ“šé‡
            df = self.data_processor.load_training_data()
            return len(df)
            
        except Exception as e:
            logger.warning(f"åŠ è¼‰è¨“ç·´æ¨£æœ¬æ•¸å¤±æ•—: {e}ï¼Œä½¿ç”¨ç•¶å‰æ•¸æ“šé‡")
            df = self.data_processor.load_training_data()
            return len(df)
