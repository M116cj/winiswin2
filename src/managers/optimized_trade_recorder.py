"""
OptimizedTradeRecorder - æ‰¹é‡I/Oä¼˜åŒ–ç‰ˆæœ¬
ğŸ”¥ v3.24+ æ–°å¢åŠŸèƒ½ï¼š
- çœŸæ­£çš„å¼‚æ­¥I/Oï¼ˆaiofilesï¼Œé¿å…é˜»å¡ï¼‰
- å†™ç¼“å†²åŒºä¼˜åŒ–ï¼ˆå‡å°‘ç³»ç»Ÿè°ƒç”¨ï¼‰
- æ–‡ä»¶è½®è½¬å’Œå‹ç¼©ï¼ˆè‡ªåŠ¨ç®¡ç†å†å²æ•°æ®ï¼‰
- æ€§èƒ½ç›‘æ§ï¼ˆI/Oç»Ÿè®¡ï¼‰
"""

import json
import os
import gzip
from typing import Dict, List, Optional, TYPE_CHECKING
from datetime import datetime
import logging
import asyncio
from pathlib import Path

if TYPE_CHECKING:
    import aiofiles

try:
    import aiofiles  # type: ignore
    AIOFILES_AVAILABLE = True
except ImportError:
    aiofiles = None  # type: ignore
    AIOFILES_AVAILABLE = False
    logging.warning("âš ï¸ aiofilesæœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŒæ­¥I/Oï¼ˆæ€§èƒ½è¾ƒä½ï¼‰")

from src.config import Config

logger = logging.getLogger(__name__)


class OptimizedTradeRecorder:
    """
    ä¼˜åŒ–çš„äº¤æ˜“è®°å½•å™¨ï¼ˆæ‰¹é‡I/O + å¼‚æ­¥å†™å…¥ + æ–‡ä»¶è½®è½¬ï¼‰
    
    å…³é”®ä¼˜åŒ–ï¼š
    1. çœŸæ­£çš„å¼‚æ­¥I/Oï¼šä½¿ç”¨aiofilesé¿å…é˜»å¡
    2. å†™ç¼“å†²åŒºï¼šç´¯ç§¯åˆ°ä¸€å®šå¤§å°å†flushï¼ˆå‡å°‘ç³»ç»Ÿè°ƒç”¨ï¼‰
    3. æ–‡ä»¶è½®è½¬ï¼šè‡ªåŠ¨è½®è½¬å¤§æ–‡ä»¶å¹¶å‹ç¼©ï¼ˆèŠ‚çœç©ºé—´ï¼‰
    4. æ€§èƒ½ç›‘æ§ï¼šè·Ÿè¸ªI/Oç»Ÿè®¡ï¼ˆwrites, bytes_writtenç­‰ï¼‰
    """
    
    def __init__(
        self,
        trades_file: str = "data/trades.jsonl",
        pending_file: str = "data/ml_pending.json",
        buffer_size: int = 100,  # ç¼“å†²åŒºå¤§å°ï¼ˆæ¡æ•°ï¼‰
        rotation_size_mb: float = 50,  # æ–‡ä»¶è½®è½¬å¤§å°ï¼ˆMBï¼‰
        enable_compression: bool = True  # å¯ç”¨å‹ç¼©
    ):
        """
        åˆå§‹åŒ–ä¼˜åŒ–è®°å½•å™¨
        
        Args:
            trades_file: äº¤æ˜“è®°å½•æ–‡ä»¶è·¯å¾„
            pending_file: å¾…é…å¯¹è®°å½•æ–‡ä»¶è·¯å¾„
            buffer_size: ç¼“å†²åŒºå¤§å°ï¼ˆè§¦å‘flushçš„äº¤æ˜“æ•°ï¼‰
            rotation_size_mb: æ–‡ä»¶è½®è½¬é˜ˆå€¼ï¼ˆMBï¼‰
            enable_compression: æ˜¯å¦å¯ç”¨å†å²æ–‡ä»¶å‹ç¼©
        """
        self.trades_file = trades_file
        self.pending_file = pending_file
        self.buffer_size = buffer_size
        self.rotation_size_bytes = rotation_size_mb * 1024 * 1024
        self.enable_compression = enable_compression
        
        # ğŸ”¥ v3.24+ å†™ç¼“å†²åŒºï¼ˆå†…å­˜ç¼“å­˜ï¼‰
        self._write_buffer: List[str] = []
        self._buffer_lock = asyncio.Lock()
        
        # ğŸ”¥ v3.24.1+ å®šæ—¶flushæœºåˆ¶ï¼ˆå®æ—¶ä¿å­˜ï¼‰
        self._auto_flush_task: Optional[asyncio.Task] = None
        self._auto_flush_interval = 10.0  # æ¯10ç§’è‡ªåŠ¨flush
        
        # ğŸ”¥ v3.24+ I/Oæ€§èƒ½ç»Ÿè®¡
        self._stats = {
            'total_writes': 0,
            'total_bytes_written': 0,
            'total_flushes': 0,
            'total_rotations': 0,
            'total_compressions': 0,
            'last_flush_time': None,
            'avg_flush_duration_ms': 0.0
        }
        self._stats_lock = asyncio.Lock()
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(self.trades_file), exist_ok=True)
        
        logger.info("=" * 80)
        logger.info("ğŸš€ OptimizedTradeRecorder åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ğŸ“ äº¤æ˜“æ–‡ä»¶: {self.trades_file}")
        logger.info(f"   ğŸ“¦ ç¼“å†²åŒºå¤§å°: {buffer_size} æ¡")
        logger.info(f"   ğŸ”„ è½®è½¬é˜ˆå€¼: {rotation_size_mb} MB")
        logger.info(f"   ğŸ“¦ å‹ç¼©: {'å¯ç”¨' if enable_compression else 'ç¦ç”¨'}")
        logger.info(f"   âš¡ å¼‚æ­¥I/O: {'å¯ç”¨ (aiofiles)' if AIOFILES_AVAILABLE else 'ç¦ç”¨ (åŒæ­¥fallback)'}")
        logger.info(f"   â° å®šæ—¶flush: {self._auto_flush_interval}ç§’ï¼ˆè‡ªåŠ¨å¯åŠ¨ï¼‰")
        logger.info("=" * 80)
        
        # ğŸ”¥ v3.24.2 Critical Fix: é»˜è®¤å¯åŠ¨å®šæ—¶flushï¼ˆå®æ—¶ä¿å­˜ä¿è¯ï¼‰
        # æ³¨æ„ï¼šåœ¨__init__ä¸­ä¸èƒ½ç›´æ¥awaitï¼Œéœ€è¦å»¶è¿Ÿåˆ°ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶
        self._auto_flush_enabled = True  # æ ‡è®°éœ€è¦è‡ªåŠ¨å¯åŠ¨
    
    async def write_trade(self, trade_data: Dict):
        """
        å†™å…¥å•æ¡äº¤æ˜“ï¼ˆæ·»åŠ åˆ°ç¼“å†²åŒºï¼‰
        
        Args:
            trade_data: äº¤æ˜“æ•°æ®å­—å…¸
        """
        # ğŸ”¥ v3.24.2 Critical Fix: ç¬¬ä¸€æ¬¡å†™å…¥æ—¶è‡ªåŠ¨å¯åŠ¨å®šæ—¶flush
        await self._ensure_auto_flush_started()
        
        # åºåˆ—åŒ–ä¸ºJSONLæ ¼å¼
        line = json.dumps(trade_data, ensure_ascii=False, default=str) + "\n"
        
        async with self._buffer_lock:
            self._write_buffer.append(line)
            buffer_count = len(self._write_buffer)
        
        # è¾¾åˆ°ç¼“å†²åŒºå¤§å°æ—¶è‡ªåŠ¨flush
        if buffer_count >= self.buffer_size:
            await self.flush()
    
    async def write_trades_batch(self, trades: List[Dict]):
        """
        æ‰¹é‡å†™å…¥äº¤æ˜“ï¼ˆé«˜æ•ˆæ‰¹é‡æ“ä½œï¼‰
        
        Args:
            trades: äº¤æ˜“æ•°æ®åˆ—è¡¨
        """
        if not trades:
            logger.info(f"ğŸ” [DIAG] OptimizedTradeRecorder.write_trades_batch: ç©ºäº¤æ˜“åˆ—è¡¨")
            return
        
        logger.info(f"ğŸ” [DIAG] OptimizedTradeRecorder.write_trades_batch: æ”¶åˆ°{len(trades)}ç­†äº¤æ˜“")
        
        # ğŸ”¥ v3.24.2 Critical Fix: ç¬¬ä¸€æ¬¡å†™å…¥æ—¶è‡ªåŠ¨å¯åŠ¨å®šæ—¶flush
        await self._ensure_auto_flush_started()
        
        # æ‰¹é‡åºåˆ—åŒ–
        lines = [
            json.dumps(trade, ensure_ascii=False, default=str) + "\n"
            for trade in trades
        ]
        
        logger.info(f"ğŸ” [DIAG] OptimizedTradeRecorder: åºåˆ—åŒ–å®Œæˆï¼Œ{len(lines)}è¡Œ")
        
        async with self._buffer_lock:
            self._write_buffer.extend(lines)
            buffer_count = len(self._write_buffer)
        
        logger.info(f"ğŸ” [DIAG] OptimizedTradeRecorder: ç·©è¡å€å¤§å°={buffer_count}, é–¾å€¼={self.buffer_size}")
        
        # æ‰¹é‡å†™å…¥åç«‹å³flush
        if buffer_count >= self.buffer_size:
            logger.info(f"ğŸ” [DIAG] OptimizedTradeRecorder: è§¸ç™¼flush")
            await self.flush()
        else:
            logger.info(f"ğŸ” [DIAG] OptimizedTradeRecorder: æœªè§¸ç™¼flushï¼Œç­‰å¾…æ›´å¤šæ•¸æ“š")
    
    async def flush(self):
        """
        å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒºåˆ°ç£ç›˜ï¼ˆå¼‚æ­¥I/Oï¼‰
        """
        logger.info(f"ğŸ” [DIAG] OptimizedTradeRecorder.flush: é–‹å§‹flush")
        start_time = datetime.now()
        
        async with self._buffer_lock:
            if not self._write_buffer:
                logger.info(f"ğŸ” [DIAG] OptimizedTradeRecorder.flush: ç·©è¡å€ç‚ºç©ºï¼Œè·³é")
                return
            
            # ğŸ”¥ v3.24.1 Critical Fix: ä¿å­˜åŸå§‹linesåˆ—è¡¨ç”¨äºå¤±è´¥æ¢å¤
            lines_snapshot = self._write_buffer.copy()
            data_to_write = "".join(self._write_buffer)
            num_lines = len(self._write_buffer)
            self._write_buffer = []
        
        logger.info(f"ğŸ” [DIAG] OptimizedTradeRecorder.flush: æº–å‚™å¯«å…¥{num_lines}è¡Œåˆ°{self.trades_file}")
        
        # ğŸ”¥ æ£€æŸ¥æ–‡ä»¶è½®è½¬
        await self._maybe_rotate_file()
        
        # ğŸ”¥ å¼‚æ­¥å†™å…¥ç£ç›˜
        try:
            if AIOFILES_AVAILABLE:
                logger.info(f"ğŸ” [DIAG] OptimizedTradeRecorder.flush: ä½¿ç”¨aiofilesç•°æ­¥å¯«å…¥")
                await self._async_append(data_to_write)
            else:
                logger.info(f"ğŸ” [DIAG] OptimizedTradeRecorder.flush: ä½¿ç”¨åŒæ­¥fallbackå¯«å…¥")
                await self._sync_append_fallback(data_to_write)
            
            # æ›´æ–°ç»Ÿè®¡
            bytes_written = len(data_to_write.encode('utf-8'))
            duration_ms = (datetime.now() - start_time).total_seconds() * 1000
            
            async with self._stats_lock:
                self._stats['total_writes'] += num_lines
                self._stats['total_bytes_written'] += bytes_written
                self._stats['total_flushes'] += 1
                self._stats['last_flush_time'] = datetime.now().isoformat()
                
                # æ›´æ–°å¹³å‡flushæ—¶é—´ï¼ˆç§»åŠ¨å¹³å‡ï¼‰
                alpha = 0.3
                self._stats['avg_flush_duration_ms'] = (
                    alpha * duration_ms +
                    (1 - alpha) * self._stats['avg_flush_duration_ms']
                )
            
            logger.info(f"ğŸ’¾ Flushå®Œæˆ: {num_lines}æ¡è®°å½•, {bytes_written}å­—èŠ‚, {duration_ms:.2f}ms")
            logger.info(f"ğŸ” [DIAG] OptimizedTradeRecorder.flush: æˆåŠŸå®Œæˆ")
        
        except Exception as e:
            logger.error(f"âŒ Flushå¤±è´¥: {e}", exc_info=True)
            logger.error(f"ğŸ” [DIAG] OptimizedTradeRecorder.flush: å¯«å…¥å¤±æ•—ï¼Œæ¢å¾©ç·©è¡å€")
            # ğŸ”¥ v3.24.1 Critical Fix: æ¢å¤åŸå§‹linesåˆ—è¡¨ï¼ˆä¿æŒç¼“å†²åŒºä¸å˜æ€§ï¼‰
            async with self._buffer_lock:
                self._write_buffer = lines_snapshot + self._write_buffer
            raise
    
    async def _async_append(self, data: str):
        """ä½¿ç”¨aiofileså¼‚æ­¥è¿½åŠ æ•°æ®"""
        if aiofiles is None:
            raise RuntimeError("aiofiles not available")
        async with aiofiles.open(self.trades_file, 'a', encoding='utf-8') as f:  # type: ignore
            await f.write(data)
    
    async def _sync_append_fallback(self, data: str):
        """åŒæ­¥I/O fallbackï¼ˆæ— aiofilesæ—¶ï¼‰"""
        await asyncio.to_thread(self._sync_append, data)
    
    def _sync_append(self, data: str):
        """çº¯åŒæ­¥è¿½åŠ ï¼ˆåœ¨çº¿ç¨‹ä¸­è¿è¡Œï¼‰"""
        with open(self.trades_file, 'a', encoding='utf-8') as f:
            f.write(data)
    
    async def _maybe_rotate_file(self):
        """
        æ£€æŸ¥å¹¶æ‰§è¡Œæ–‡ä»¶è½®è½¬ï¼ˆå¤§æ–‡ä»¶è‡ªåŠ¨è½®è½¬ï¼‰
        """
        try:
            file_size = os.path.getsize(self.trades_file) if os.path.exists(self.trades_file) else 0
            
            if file_size >= self.rotation_size_bytes:
                await self._rotate_file()
        
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶è½®è½¬æ£€æŸ¥å¤±è´¥: {e}")
    
    async def _rotate_file(self):
        """
        è½®è½¬æ–‡ä»¶å¹¶å¯é€‰å‹ç¼©
        
        é€»è¾‘ï¼š
        1. é‡å‘½åå½“å‰æ–‡ä»¶ä¸º trades_YYYYMMDD_HHMMSS.jsonl
        2. å¦‚æœå¯ç”¨å‹ç¼©ï¼Œå‹ç¼©æ—§æ–‡ä»¶
        3. åˆ›å»ºæ–°çš„ç©ºæ–‡ä»¶
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            rotated_file = self.trades_file.replace(".jsonl", f"_{timestamp}.jsonl")
            
            # é‡å‘½åå½“å‰æ–‡ä»¶
            if os.path.exists(self.trades_file):
                await asyncio.to_thread(os.rename, self.trades_file, rotated_file)
                
                async with self._stats_lock:
                    self._stats['total_rotations'] += 1
                
                logger.info(f"ğŸ”„ æ–‡ä»¶è½®è½¬: {self.trades_file} â†’ {rotated_file}")
                
                # å‹ç¼©æ—§æ–‡ä»¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.enable_compression:
                    asyncio.create_task(self._compress_file(rotated_file))
        
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶è½®è½¬å¤±è´¥: {e}")
    
    async def _compress_file(self, file_path: str):
        """
        å‹ç¼©æ–‡ä»¶ï¼ˆåå°ä»»åŠ¡ï¼‰
        
        Args:
            file_path: è¦å‹ç¼©çš„æ–‡ä»¶è·¯å¾„
        """
        try:
            compressed_path = file_path + ".gz"
            
            # å¼‚æ­¥å‹ç¼©ï¼ˆåœ¨çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰
            await asyncio.to_thread(self._sync_compress, file_path, compressed_path)
            
            # åˆ é™¤åŸæ–‡ä»¶
            await asyncio.to_thread(os.remove, file_path)
            
            async with self._stats_lock:
                self._stats['total_compressions'] += 1
            
            original_size = os.path.getsize(compressed_path)
            logger.info(f"ğŸ“¦ å‹ç¼©å®Œæˆ: {file_path} â†’ {compressed_path} ({original_size / 1024 / 1024:.2f} MB)")
        
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶å‹ç¼©å¤±è´¥: {e}")
    
    def _sync_compress(self, source: str, dest: str):
        """åŒæ­¥å‹ç¼©æ–‡ä»¶"""
        with open(source, 'rb') as f_in:
            with gzip.open(dest, 'wb', compresslevel=6) as f_out:
                f_out.writelines(f_in)
    
    async def save_pending_entries(self, pending_entries: List[Dict]):
        """
        ä¿å­˜å¾…é…å¯¹æ¡ç›®ï¼ˆè¦†ç›–å†™å…¥ï¼‰
        
        Args:
            pending_entries: å¾…é…å¯¹æ¡ç›®åˆ—è¡¨
        """
        try:
            if AIOFILES_AVAILABLE and aiofiles is not None:
                async with aiofiles.open(self.pending_file, 'w', encoding='utf-8') as f:  # type: ignore
                    content = json.dumps(pending_entries, ensure_ascii=False, indent=2, default=str)
                    await f.write(content)
            else:
                await asyncio.to_thread(
                    self._sync_save_pending,
                    pending_entries
                )
            
            logger.debug(f"ğŸ’¾ ä¿å­˜ {len(pending_entries)} æ¡å¾…é…å¯¹è®°å½•")
        
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å¾…é…å¯¹è®°å½•å¤±è´¥: {e}")
    
    def _sync_save_pending(self, pending_entries: List[Dict]):
        """åŒæ­¥ä¿å­˜å¾…é…å¯¹è®°å½•"""
        with open(self.pending_file, 'w', encoding='utf-8') as f:
            json.dump(pending_entries, f, ensure_ascii=False, indent=2, default=str)
    
    async def _ensure_auto_flush_started(self):
        """
        ğŸ”¥ v3.24.2 Critical Fix: ç¡®ä¿å®šæ—¶flushå·²å¯åŠ¨ï¼ˆç¬¬ä¸€æ¬¡å†™å…¥æ—¶è‡ªåŠ¨è°ƒç”¨ï¼‰
        """
        if self._auto_flush_enabled and self._auto_flush_task is None:
            await self.start_auto_flush()
    
    async def start_auto_flush(self):
        """
        ğŸ”¥ v3.24.1+ å¯åŠ¨å®šæ—¶flushä»»åŠ¡ï¼ˆå®æ—¶ä¿å­˜ï¼‰
        """
        if self._auto_flush_task is None:
            self._auto_flush_task = asyncio.create_task(self._auto_flush_loop())
            logger.info(f"ğŸ”„ å®šæ—¶flushå·²å¯åŠ¨ï¼ˆé—´éš”: {self._auto_flush_interval}ç§’ï¼‰")
    
    async def stop_auto_flush(self):
        """
        ğŸ”¥ v3.24.1+ åœæ­¢å®šæ—¶flushä»»åŠ¡
        """
        if self._auto_flush_task:
            self._auto_flush_task.cancel()
            try:
                await self._auto_flush_task
            except asyncio.CancelledError:
                pass
            self._auto_flush_task = None
            logger.info("â¸ï¸  å®šæ—¶flushå·²åœæ­¢")
    
    async def _auto_flush_loop(self):
        """
        ğŸ”¥ v3.24.1+ å®šæ—¶flushå¾ªç¯ï¼ˆåå°ä»»åŠ¡ï¼‰
        """
        try:
            while True:
                await asyncio.sleep(self._auto_flush_interval)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¾…flushæ•°æ®
                async with self._buffer_lock:
                    has_data = len(self._write_buffer) > 0
                
                if has_data:
                    try:
                        await self.flush()
                        logger.debug("ğŸ”„ å®šæ—¶flushå®Œæˆ")
                    except Exception as e:
                        logger.error(f"âŒ å®šæ—¶flushå¤±è´¥: {e}")
        
        except asyncio.CancelledError:
            logger.debug("ğŸ”„ å®šæ—¶flushä»»åŠ¡å·²å–æ¶ˆ")
            raise
    
    async def get_stats(self) -> Dict:
        """
        è·å–I/Oæ€§èƒ½ç»Ÿè®¡
        
        Returns:
            ç»Ÿè®¡æ•°æ®å­—å…¸
        """
        async with self._stats_lock:
            return self._stats.copy()
    
    async def close(self):
        """
        å…³é—­è®°å½•å™¨ï¼ˆæœ€ç»ˆflushï¼‰
        """
        logger.info("â¸ï¸  OptimizedTradeRecorder å…³é—­ä¸­...")
        
        # åœæ­¢å®šæ—¶flush
        await self.stop_auto_flush()
        
        # æœ€ç»ˆflush
        await self.flush()
        
        # æ‰“å°ç»Ÿè®¡
        stats = await self.get_stats()
        logger.info("=" * 80)
        logger.info("ğŸ“Š OptimizedTradeRecorder ç»Ÿè®¡æ•°æ®:")
        logger.info(f"   æ€»å†™å…¥: {stats['total_writes']} æ¡è®°å½•")
        logger.info(f"   æ€»å­—èŠ‚æ•°: {stats['total_bytes_written'] / 1024 / 1024:.2f} MB")
        logger.info(f"   æ€»flushæ¬¡æ•°: {stats['total_flushes']}")
        logger.info(f"   å¹³å‡flushæ—¶é—´: {stats['avg_flush_duration_ms']:.2f} ms")
        logger.info(f"   æ–‡ä»¶è½®è½¬æ¬¡æ•°: {stats['total_rotations']}")
        logger.info(f"   å‹ç¼©æ¬¡æ•°: {stats['total_compressions']}")
        logger.info("=" * 80)
        
        logger.info("âœ… OptimizedTradeRecorder å·²å…³é—­")
