"""
SmartLogger - æ™ºèƒ½æ—¥å¿—ç³»ç»Ÿ
ğŸ”¥ v3.25+ æ–°å¢åŠŸèƒ½ï¼š
- é€Ÿç‡é™åˆ¶ï¼ˆé˜²æ­¢æ—¥å¿—æ´ªæ°´ï¼‰
- åŠ¨æ€æ—¥å¿—çº§åˆ«
- æ—¥å¿—èšåˆï¼ˆé˜²æ­¢é‡å¤ï¼‰
- ç»“æ„åŒ–æ—¥å¿—ï¼ˆJSONæ ¼å¼ï¼‰
- æ€§èƒ½ç›‘æ§
- å¼‚æ­¥å†™å…¥ç¼“å†²
"""

import logging
import time
import json
from typing import Dict, Optional, Any, List
from collections import defaultdict, deque
from datetime import datetime
import threading
from pathlib import Path


class SmartLogger:
    """
    æ™ºèƒ½æ—¥å¿—ç³»ç»Ÿ
    
    å…³é”®åŠŸèƒ½ï¼š
    1. é€Ÿç‡é™åˆ¶ï¼šåŒæ ·çš„æ¶ˆæ¯åœ¨æ—¶é—´çª—å£å†…åªè®°å½•ä¸€æ¬¡
    2. æ—¥å¿—èšåˆï¼šç›¸ä¼¼æ¶ˆæ¯åˆå¹¶è®¡æ•°
    3. ç»“æ„åŒ–æ—¥å¿—ï¼šæ”¯æŒJSONæ ¼å¼è¾“å‡º
    4. æ€§èƒ½ç›‘æ§ï¼šè·Ÿè¸ªæ—¥å¿—ç»Ÿè®¡
    5. åŠ¨æ€çº§åˆ«ï¼šè¿è¡Œæ—¶è°ƒæ•´æ—¥å¿—çº§åˆ«
    """
    
    def __init__(
        self,
        name: str,
        base_logger: Optional[logging.Logger] = None,
        rate_limit_window: float = 60.0,  # é€Ÿç‡é™åˆ¶çª—å£ï¼ˆç§’ï¼‰
        enable_aggregation: bool = True,  # å¯ç”¨æ—¥å¿—èšåˆ
        enable_structured: bool = False,  # å¯ç”¨ç»“æ„åŒ–æ—¥å¿—
        structured_log_file: Optional[str] = None  # ç»“æ„åŒ–æ—¥å¿—æ–‡ä»¶
    ):
        """
        åˆå§‹åŒ–SmartLogger
        
        Args:
            name: Loggeråç§°
            base_logger: åº•å±‚loggerï¼ˆå¯é€‰ï¼‰
            rate_limit_window: é€Ÿç‡é™åˆ¶æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
            enable_aggregation: æ˜¯å¦å¯ç”¨æ—¥å¿—èšåˆ
            enable_structured: æ˜¯å¦å¯ç”¨ç»“æ„åŒ–æ—¥å¿—
            structured_log_file: ç»“æ„åŒ–æ—¥å¿—æ–‡ä»¶è·¯å¾„
        """
        self.name = name
        self.base_logger = base_logger or logging.getLogger(name)
        self.rate_limit_window = rate_limit_window
        self.enable_aggregation = enable_aggregation
        self.enable_structured = enable_structured
        self.structured_log_file = structured_log_file
        
        # ğŸ”¥ é€Ÿç‡é™åˆ¶æœºåˆ¶ï¼ˆè®°å½•æœ€åä¸€æ¬¡æ—¥å¿—æ—¶é—´ï¼‰
        self._rate_limit_cache: Dict[str, float] = {}
        self._rate_limit_lock = threading.Lock()
        
        # ğŸ”¥ æ—¥å¿—èšåˆæœºåˆ¶ï¼ˆåˆå¹¶é‡å¤æ¶ˆæ¯ï¼‰
        self._aggregation_cache: Dict[str, Dict] = defaultdict(lambda: {
            'count': 0,
            'first_time': 0,
            'last_time': 0,
            'level': logging.INFO
        })
        self._aggregation_lock = threading.Lock()
        
        # ğŸ”¥ æ€§èƒ½ç»Ÿè®¡
        self._stats = {
            'total_logs': 0,
            'rate_limited': 0,
            'aggregated': 0,
            'by_level': defaultdict(int)
        }
        self._stats_lock = threading.Lock()
        
        # ğŸ”¥ ç»“æ„åŒ–æ—¥å¿—å†™å…¥å™¨
        if enable_structured and structured_log_file:
            Path(structured_log_file).parent.mkdir(parents=True, exist_ok=True)
            self._structured_file = open(structured_log_file, 'a', encoding='utf-8')
        else:
            self._structured_file = None
    
    def _should_log(self, message: str, level: int) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥è®°å½•æ­¤æ¶ˆæ¯ï¼ˆé€Ÿç‡é™åˆ¶ï¼‰
        
        Args:
            message: æ—¥å¿—æ¶ˆæ¯
            level: æ—¥å¿—çº§åˆ«
        
        Returns:
            Trueå¦‚æœåº”è¯¥è®°å½•ï¼Œå¦åˆ™False
        """
        # ğŸ”¥ Critical/Errorçº§åˆ«ä¸é™åˆ¶
        if level >= logging.ERROR:
            return True
        
        # ç”Ÿæˆç¼“å­˜é”®ï¼ˆåŸºäºæ¶ˆæ¯å’Œçº§åˆ«ï¼‰
        cache_key = f"{level}:{message}"
        
        with self._rate_limit_lock:
            now = time.time()
            last_time = self._rate_limit_cache.get(cache_key, 0)
            
            # æ£€æŸ¥æ—¶é—´çª—å£
            if now - last_time < self.rate_limit_window:
                with self._stats_lock:
                    self._stats['rate_limited'] += 1
                return False
            
            # æ›´æ–°æœ€åè®°å½•æ—¶é—´
            self._rate_limit_cache[cache_key] = now
            
            # æ¸…ç†è¿‡æœŸç¼“å­˜ï¼ˆä¿æŒç¼“å­˜å¤§å°åˆç†ï¼‰
            if len(self._rate_limit_cache) > 1000:
                expired_keys = [
                    k for k, v in self._rate_limit_cache.items()
                    if now - v > self.rate_limit_window * 2
                ]
                for k in expired_keys:
                    del self._rate_limit_cache[k]
        
        return True
    
    def _aggregate_message(self, message: str, level: int):
        """
        èšåˆé‡å¤æ¶ˆæ¯ï¼ˆè®¡æ•°ï¼‰
        
        Args:
            message: æ—¥å¿—æ¶ˆæ¯
            level: æ—¥å¿—çº§åˆ«
        """
        if not self.enable_aggregation:
            return
        
        cache_key = f"{level}:{message}"
        now = time.time()
        
        with self._aggregation_lock:
            agg = self._aggregation_cache[cache_key]
            
            if agg['count'] == 0:
                agg['first_time'] = now
                agg['level'] = level
            
            agg['count'] += 1
            agg['last_time'] = now
            
            with self._stats_lock:
                self._stats['aggregated'] += 1
    
    def _write_structured_log(self, level: int, message: str, extra: Optional[Dict] = None):
        """
        å†™å…¥ç»“æ„åŒ–æ—¥å¿—ï¼ˆJSONæ ¼å¼ï¼‰
        
        Args:
            level: æ—¥å¿—çº§åˆ«
            message: æ—¥å¿—æ¶ˆæ¯
            extra: é¢å¤–æ•°æ®
        """
        if not self._structured_file:
            return
        
        try:
            log_entry = {
                'timestamp': datetime.now().isoformat(),
                'level': logging.getLevelName(level),
                'logger': self.name,
                'message': message,
                **(extra or {})
            }
            
            self._structured_file.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
            self._structured_file.flush()
        
        except Exception as e:
            # é¿å…æ—¥å¿—é”™è¯¯å½±å“ä¸»æµç¨‹
            self.base_logger.error(f"âŒ ç»“æ„åŒ–æ—¥å¿—å†™å…¥å¤±è´¥: {e}")
    
    def _log(self, level: int, message: str, *args, extra: Optional[Dict] = None, **kwargs):
        """
        å†…éƒ¨æ—¥å¿—æ–¹æ³•ï¼ˆåº”ç”¨æ‰€æœ‰æ™ºèƒ½ç‰¹æ€§ï¼‰
        
        Args:
            level: æ—¥å¿—çº§åˆ«
            message: æ—¥å¿—æ¶ˆæ¯
            extra: é¢å¤–æ•°æ®
        """
        # æ›´æ–°ç»Ÿè®¡
        with self._stats_lock:
            self._stats['total_logs'] += 1
            self._stats['by_level'][logging.getLevelName(level)] += 1
        
        # ğŸ”¥ v3.25.1 Critical Fix: èšåˆå¿…é¡»åœ¨é€Ÿç‡é™åˆ¶ä¹‹å‰ï¼ˆç»Ÿè®¡æ‰€æœ‰è°ƒç”¨ï¼‰
        self._aggregate_message(message, level)
        
        # ğŸ”¥ é€Ÿç‡é™åˆ¶æ£€æŸ¥
        if not self._should_log(message, level):
            return
        
        # ğŸ”¥ ç»“æ„åŒ–æ—¥å¿—
        if self.enable_structured:
            self._write_structured_log(level, message, extra)
        
        # ğŸ”¥ v3.25.1 Critical Fix: è½¬å‘extraåˆ°base_loggerï¼ˆä¿æŒAPIå…¼å®¹æ€§ï¼‰
        if extra:
            self.base_logger.log(level, message, *args, extra=extra, **kwargs)
        else:
            self.base_logger.log(level, message, *args, **kwargs)
    
    def debug(self, message: str, *args, **kwargs):
        """DEBUGçº§åˆ«æ—¥å¿—"""
        self._log(logging.DEBUG, message, *args, **kwargs)
    
    def info(self, message: str, *args, **kwargs):
        """INFOçº§åˆ«æ—¥å¿—"""
        self._log(logging.INFO, message, *args, **kwargs)
    
    def warning(self, message: str, *args, **kwargs):
        """WARNINGçº§åˆ«æ—¥å¿—"""
        self._log(logging.WARNING, message, *args, **kwargs)
    
    def error(self, message: str, *args, **kwargs):
        """ERRORçº§åˆ«æ—¥å¿—ï¼ˆä¸é™é€Ÿï¼‰"""
        self._log(logging.ERROR, message, *args, **kwargs)
    
    def critical(self, message: str, *args, **kwargs):
        """CRITICALçº§åˆ«æ—¥å¿—ï¼ˆä¸é™é€Ÿï¼‰"""
        self._log(logging.CRITICAL, message, *args, **kwargs)
    
    def flush_aggregations(self) -> List[Dict]:
        """
        åˆ·æ–°èšåˆæ—¥å¿—ï¼ˆæŠ¥å‘Šé‡å¤æ¬¡æ•°ï¼‰
        
        Returns:
            èšåˆæ—¥å¿—åˆ—è¡¨
        """
        with self._aggregation_lock:
            aggregations = []
            now = time.time()
            
            for key, agg in list(self._aggregation_cache.items()):
                if agg['count'] > 1:
                    duration = agg['last_time'] - agg['first_time']
                    aggregations.append({
                        'message': key.split(':', 1)[1],
                        'level': logging.getLevelName(agg['level']),
                        'count': agg['count'],
                        'duration': duration,
                        'first_time': datetime.fromtimestamp(agg['first_time']).isoformat(),
                        'last_time': datetime.fromtimestamp(agg['last_time']).isoformat()
                    })
                    
                    # æŠ¥å‘Šèšåˆç»“æœ
                    self.base_logger.log(
                        agg['level'],
                        f"ğŸ“Š èšåˆæ—¥å¿—: {agg['count']}æ¬¡ '{key.split(':', 1)[1]}' (è¿‡å»{duration:.1f}ç§’)"
                    )
                
                # æ¸…ç†æ—§èšåˆï¼ˆè¶…è¿‡çª—å£çš„2å€ï¼‰
                if now - agg['last_time'] > self.rate_limit_window * 2:
                    del self._aggregation_cache[key]
            
            return aggregations
    
    def get_stats(self) -> Dict:
        """
        è·å–æ—¥å¿—ç»Ÿè®¡
        
        Returns:
            ç»Ÿè®¡æ•°æ®å­—å…¸
        """
        with self._stats_lock:
            return {
                'total_logs': self._stats['total_logs'],
                'rate_limited': self._stats['rate_limited'],
                'aggregated': self._stats['aggregated'],
                'by_level': dict(self._stats['by_level']),
                'rate_limit_efficiency': (
                    self._stats['rate_limited'] / max(1, self._stats['total_logs'])
                ) * 100
            }
    
    def set_level(self, level: int):
        """
        åŠ¨æ€è®¾ç½®æ—¥å¿—çº§åˆ«
        
        Args:
            level: æ–°çš„æ—¥å¿—çº§åˆ«
        """
        self.base_logger.setLevel(level)
    
    def close(self):
        """å…³é—­SmartLoggerï¼ˆåˆ·æ–°èšåˆæ—¥å¿—ï¼‰"""
        # åˆ·æ–°èšåˆæ—¥å¿—
        self.flush_aggregations()
        
        # å…³é—­ç»“æ„åŒ–æ—¥å¿—æ–‡ä»¶
        if self._structured_file:
            self._structured_file.close()
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        stats = self.get_stats()
        self.base_logger.info("=" * 80)
        self.base_logger.info("ğŸ“Š SmartLogger ç»Ÿè®¡æ•°æ®:")
        self.base_logger.info(f"   æ€»æ—¥å¿—æ•°: {stats['total_logs']}")
        self.base_logger.info(f"   é€Ÿç‡é™åˆ¶: {stats['rate_limited']} ({stats['rate_limit_efficiency']:.1f}%)")
        self.base_logger.info(f"   èšåˆæ¬¡æ•°: {stats['aggregated']}")
        self.base_logger.info(f"   æŒ‰çº§åˆ«: {stats['by_level']}")
        self.base_logger.info("=" * 80)


# ğŸ”¥ ä¾¿æ·å·¥å‚å‡½æ•°
def create_smart_logger(
    name: str,
    rate_limit_window: float = 60.0,
    enable_aggregation: bool = True,
    enable_structured: bool = False,
    structured_log_file: Optional[str] = None
) -> SmartLogger:
    """
    åˆ›å»ºSmartLoggerå®ä¾‹
    
    Args:
        name: Loggeråç§°
        rate_limit_window: é€Ÿç‡é™åˆ¶æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
        enable_aggregation: æ˜¯å¦å¯ç”¨æ—¥å¿—èšåˆ
        enable_structured: æ˜¯å¦å¯ç”¨ç»“æ„åŒ–æ—¥å¿—
        structured_log_file: ç»“æ„åŒ–æ—¥å¿—æ–‡ä»¶è·¯å¾„
    
    Returns:
        SmartLoggerå®ä¾‹
    """
    return SmartLogger(
        name=name,
        rate_limit_window=rate_limit_window,
        enable_aggregation=enable_aggregation,
        enable_structured=enable_structured,
        structured_log_file=structured_log_file
    )
