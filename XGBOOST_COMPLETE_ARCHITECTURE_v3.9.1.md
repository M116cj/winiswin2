# XGBoost å…¨æ–¹ä½è¯¦ç»†æ¶æ„æ–‡æ¡£ v3.9.1

**æ›´æ–°æ—¥æœŸ**: 2025-10-27  
**ç³»ç»Ÿç‰ˆæœ¬**: Winiswin2 v3.9.1 Enhanced  
**æ–‡æ¡£ç±»å‹**: å®Œæ•´æŠ€æœ¯æ¶æ„è¯´æ˜

---

## ğŸ“‹ ç›®å½•

1. [ç³»ç»Ÿæ¦‚è§ˆ](#ç³»ç»Ÿæ¦‚è§ˆ)
2. [ç‰¹å¾å·¥ç¨‹æ¶æ„](#ç‰¹å¾å·¥ç¨‹æ¶æ„)
3. [æ¨¡å‹å­¦ä¹ è·¯å¾„](#æ¨¡å‹å­¦ä¹ è·¯å¾„)
4. [è®­ç»ƒæ–¹å¼ä¸æµç¨‹](#è®­ç»ƒæ–¹å¼ä¸æµç¨‹)
5. [æ¨¡å‹å‚æ•°è¯¦è§£](#æ¨¡å‹å‚æ•°è¯¦è§£)
6. [é«˜çº§ä¼˜åŒ–åŠŸèƒ½](#é«˜çº§ä¼˜åŒ–åŠŸèƒ½)
7. [æ•°æ®æµä¸æ¶æ„å›¾](#æ•°æ®æµä¸æ¶æ„å›¾)

---

## 1. ç³»ç»Ÿæ¦‚è§ˆ

### 1.1 æ ¸å¿ƒç»„ä»¶

```
XGBoost ML Pipeline
â”œâ”€â”€ æ•°æ®æ”¶é›†å±‚ï¼šäº¤æ˜“è®°å½•ï¼ˆè™šæ‹Ÿ+çœŸå®ä»“ä½ï¼‰
â”œâ”€â”€ ç‰¹å¾å·¥ç¨‹å±‚ï¼š29ä¸ªç‰¹å¾ï¼ˆ21åŸºç¡€+8å¢å¼ºï¼‰
â”œâ”€â”€ ç›®æ ‡ä¼˜åŒ–å±‚ï¼š3ç§ç›®æ ‡ç±»å‹ï¼ˆbinary/pnl_pct/risk_adjustedï¼‰
â”œâ”€â”€ æ¨¡å‹è®­ç»ƒå±‚ï¼šXGBClassifier/XGBRegressorï¼ˆè‡ªé€‚åº”é€‰æ‹©ï¼‰
â”œâ”€â”€ ä¸ç¡®å®šæ€§é‡åŒ–ï¼šQuantile Regressionï¼ˆé€Ÿåº¦æå‡10å€ï¼‰
â”œâ”€â”€ æ¼‚ç§»æ£€æµ‹å±‚ï¼šåŠ¨æ€çª—å£ + PCA+MMDå¤šå˜é‡æ£€æµ‹
â””â”€â”€ é¢„æµ‹æ¨ç†å±‚ï¼šå¸¦ç½®ä¿¡åŒºé—´çš„é¢„æµ‹
```

### 1.2 æŠ€æœ¯æ ˆ

| ç»„ä»¶ | æŠ€æœ¯ | ç‰ˆæœ¬è¦æ±‚ |
|------|------|----------|
| **MLæ¡†æ¶** | XGBoost | â‰¥1.7.0 |
| **æ•°æ®å¤„ç†** | Pandas, NumPy | Latest |
| **è¯„ä¼°æŒ‡æ ‡** | scikit-learn | Latest |
| **ä¸ç¡®å®šæ€§é‡åŒ–** | XGBoost Quantile Regression | â‰¥1.7.0 |
| **æ¼‚ç§»æ£€æµ‹** | scipy (KS), PCA+MMD | Latest |
| **ç‰¹å¾ç¼“å­˜** | MD5å“ˆå¸Œ + TTL | è‡ªç ” |

---

## 2. ç‰¹å¾å·¥ç¨‹æ¶æ„

### 2.1 ç‰¹å¾æ€»è§ˆï¼ˆ29ä¸ªç‰¹å¾ï¼‰

#### A. åŸºç¡€ç‰¹å¾ï¼ˆ21ä¸ªï¼‰

**ç­–ç•¥ç›¸å…³ç‰¹å¾ï¼ˆ7ä¸ªï¼‰**
```python
1. confidence_score      # ä¿¡å¿ƒåº¦è¯„åˆ†ï¼ˆ0-1ï¼‰
2. leverage              # æ æ†å€æ•°ï¼ˆ3x-20xï¼‰
3. position_value        # ä»“ä½ä»·å€¼ï¼ˆUSDï¼‰
4. hold_duration_hours   # æŒä»“æ—¶é•¿ï¼ˆå°æ—¶ï¼‰
5. risk_reward_ratio     # é£é™©æ”¶ç›Šæ¯”ï¼ˆ1:1 - 1:2ï¼‰
6. order_blocks_count    # Order Blocksæ•°é‡
7. liquidity_zones_count # æµåŠ¨æ€§åŒºåŸŸæ•°é‡
```

**æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ï¼ˆ9ä¸ªï¼‰**
```python
8.  rsi_entry            # å…¥åœºRSIå€¼ï¼ˆ0-100ï¼‰
9.  macd_entry           # å…¥åœºMACDå€¼
10. macd_signal_entry    # å…¥åœºMACDä¿¡å·çº¿
11. macd_histogram_entry # å…¥åœºMACDæŸ±çŠ¶å›¾
12. atr_entry            # å…¥åœºATRå€¼ï¼ˆæ³¢åŠ¨ç‡ï¼‰
13. bb_width_pct         # å¸ƒæ—å¸¦å®½åº¦ç™¾åˆ†æ¯”
14. volume_sma_ratio     # æˆäº¤é‡/å‡é‡æ¯”
15. price_vs_ema50       # ä»·æ ¼ç›¸å¯¹EMA50ä½ç½®
16. price_vs_ema200      # ä»·æ ¼ç›¸å¯¹EMA200ä½ç½®
```

**å¸‚åœºç»“æ„ç‰¹å¾ï¼ˆ5ä¸ªï¼‰**
```python
17. trend_1h_encoded     # 1å°æ—¶è¶‹åŠ¿ç¼–ç ï¼ˆ-1/0/1ï¼‰
18. trend_15m_encoded    # 15åˆ†é’Ÿè¶‹åŠ¿ç¼–ç 
19. trend_5m_encoded     # 5åˆ†é’Ÿè¶‹åŠ¿ç¼–ç 
20. market_structure_encoded  # å¸‚åœºç»“æ„ç¼–ç 
21. direction_encoded    # äº¤æ˜“æ–¹å‘ç¼–ç ï¼ˆLONG=1, SHORT=-1ï¼‰
```

#### B. å¢å¼ºç‰¹å¾ï¼ˆ8ä¸ªï¼‰

**æ—¶é—´ç‰¹å¾ï¼ˆ3ä¸ªï¼‰**
```python
22. hour_of_day          # å…¥åœºå°æ—¶ï¼ˆ0-23ï¼‰
23. day_of_week          # æ˜ŸæœŸå‡ ï¼ˆ0-6ï¼‰
24. is_weekend           # æ˜¯å¦å‘¨æœ«ï¼ˆ0/1ï¼‰
```

**ä»·æ ¼è·ç¦»ç‰¹å¾ï¼ˆ2ä¸ªï¼‰**
```python
25. stop_distance_pct    # æ­¢æŸè·ç¦»ç™¾åˆ†æ¯”
26. tp_distance_pct      # æ­¢ç›ˆè·ç¦»ç™¾åˆ†æ¯”
```

**äº¤äº’ç‰¹å¾ï¼ˆ3ä¸ªï¼‰**
```python
27. confidence_x_leverage  # ä¿¡å¿ƒåº¦ Ã— æ æ†
28. rsi_x_trend           # RSI Ã— è¶‹åŠ¿
29. atr_x_bb_width        # ATR Ã— å¸ƒæ—å¸¦å®½åº¦
```

### 2.2 ç‰¹å¾å·¥ç¨‹æµç¨‹

```python
# src/ml/data_processor.py

class MLDataProcessor:
    
    def prepare_features(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """
        ç‰¹å¾å‡†å¤‡æµç¨‹ï¼š
        1. ç¼–ç ç±»åˆ«å˜é‡ï¼ˆè¶‹åŠ¿ã€å¸‚åœºç»“æ„ã€æ–¹å‘ï¼‰
        2. æ·»åŠ å¢å¼ºç‰¹å¾ï¼ˆæ—¶é—´ã€ä»·æ ¼è·ç¦»ã€äº¤äº’ï¼‰
        3. å¡«å……ç¼ºå¤±å€¼ï¼ˆç”¨0å¡«å……ï¼‰
        4. æå–ç‰¹å¾çŸ©é˜µXå’Œæ ‡ç­¾y
        """
        
        # æ­¥éª¤1ï¼šç¼–ç 
        df['trend_1h_encoded'] = df['trend_1h'].map({
            'bullish': 1, 'bearish': -1, 'neutral': 0
        })
        
        # æ­¥éª¤2ï¼šå¢å¼ºç‰¹å¾
        df = self._add_enhanced_features(df)
        
        # æ­¥éª¤3ï¼šå¡«å……ç¼ºå¤±å€¼
        for col in feature_columns:
            df[col] = df[col].fillna(0)
        
        # æ­¥éª¤4ï¼šæå–
        X = df[feature_columns]
        y = df[target_column]
        
        return X, y
```

### 2.3 ç‰¹å¾é‡è¦æ€§ç›‘æ§

ç³»ç»Ÿè‡ªåŠ¨è¿½è¸ªç‰¹å¾é‡è¦æ€§ï¼Œè¯†åˆ«ï¼š
- **å…³é”®ç‰¹å¾**ï¼šé‡è¦æ€§ > 5%
- **ä¸­ç­‰ç‰¹å¾**ï¼š1% - 5%
- **ä½å½±å“ç‰¹å¾**ï¼š< 1%

```python
# è‡ªåŠ¨ç”Ÿæˆç‰¹å¾é‡è¦æ€§æŠ¥å‘Š
feature_importance = model.feature_importances_
top_features = sorted(zip(feature_names, importance), 
                     key=lambda x: x[1], reverse=True)[:10]
```

---

## 3. æ¨¡å‹å­¦ä¹ è·¯å¾„

### 3.1 ç›®æ ‡å˜é‡ç±»å‹ï¼ˆ3ç§ï¼‰

#### A. äºŒåˆ†ç±»ç›®æ ‡ï¼ˆbinaryï¼‰

```python
target_type = 'binary'
target = df['is_winner']  # 0=äºæŸ, 1=ç›ˆåˆ©

# æ¨¡å‹é…ç½®
model = XGBClassifier(
    objective='binary:logistic',
    eval_metric='auc'
)

# è¯„ä¼°æŒ‡æ ‡
metrics = {
    'accuracy': å‡†ç¡®ç‡,
    'precision': ç²¾ç¡®ç‡,
    'recall': å¬å›ç‡,
    'f1_score': F1åˆ†æ•°,
    'roc_auc': ROC-AUC
}
```

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… ç®€å•çš„ç›ˆäºé¢„æµ‹
- âœ… ç±»åˆ«å¹³è¡¡æ•°æ®é›†
- âŒ æ— æ³•é¢„æµ‹å…·ä½“æ”¶ç›Šé‡‘é¢

#### B. ç›ˆäºç™¾åˆ†æ¯”ç›®æ ‡ï¼ˆpnl_pctï¼‰

```python
target_type = 'pnl_pct'
target = df['pnl_pct']  # è¿ç»­å€¼ï¼ˆ-100% to +100%ï¼‰

# æ¨¡å‹é…ç½®
model = XGBRegressor(
    objective='reg:squarederror',
    eval_metric='rmse'
)

# è¯„ä¼°æŒ‡æ ‡
metrics = {
    'mae': å¹³å‡ç»å¯¹è¯¯å·®,
    'rmse': å‡æ–¹æ ¹è¯¯å·®,
    'r2_score': RÂ²å†³å®šç³»æ•°,
    'direction_accuracy': æ–¹å‘å‡†ç¡®ç‡
}
```

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… é¢„æµ‹å…·ä½“ç›ˆäºç™¾åˆ†æ¯”
- âœ… ç›´æ¥ä¼˜åŒ–æ”¶ç›Š
- âŒ æœªè€ƒè™‘å¸‚åœºæ³¢åŠ¨ç‡å½±å“

#### C. é£é™©è°ƒæ•´æ”¶ç›Šç›®æ ‡ï¼ˆrisk_adjustedï¼‰â­ **é»˜è®¤æ¨è**

```python
target_type = 'risk_adjusted'
target = df['pnl_pct'] / df['atr_entry']  # PnL / ATR

# æ¨¡å‹é…ç½®
model = XGBRegressor(
    objective='reg:squarederror',
    eval_metric='rmse'
)

# è¯„ä¼°æŒ‡æ ‡
metrics = {
    'mae': å¹³å‡ç»å¯¹è¯¯å·®,
    'rmse': å‡æ–¹æ ¹è¯¯å·®,
    'r2_score': RÂ²å†³å®šç³»æ•°,
    'direction_accuracy': æ–¹å‘å‡†ç¡®ç‡
}
```

**ä¼˜åŠ¿**ï¼š
- âœ… **è·¨æ³¢åŠ¨ç‡regimeç¨³å®š**ï¼ˆå½’ä¸€åŒ–å¸‚åœºæ³¢åŠ¨ï¼‰
- âœ… **é¿å…è™šå‡æ”¶ç›Š**ï¼ˆé«˜æ³¢åŠ¨æœŸçš„å¤§æ”¶ç›Šè¢«ATRè°ƒæ•´ï¼‰
- âœ… **æ›´å‡†ç¡®çš„é£é™©è¯„ä¼°**
- âœ… **v3.9.1é»˜è®¤å¯ç”¨**

**å…¬å¼**ï¼š
```
Risk-Adjusted Return = PnL% / ATR
```

ç¤ºä¾‹ï¼š
- æ”¶ç›Š5%ï¼ŒATR=2% â†’ Risk-Adjusted = 2.5
- æ”¶ç›Š5%ï¼ŒATR=0.5% â†’ Risk-Adjusted = 10ï¼ˆå®é™…æ›´å¥½ï¼‰

### 3.2 æ¨¡å‹å­¦ä¹ è·¯å¾„å›¾

```mermaid
graph TD
    A[åŸå§‹äº¤æ˜“æ•°æ®] --> B[ç‰¹å¾å·¥ç¨‹]
    B --> C[ç›®æ ‡ä¼˜åŒ–å™¨]
    C --> D{ç›®æ ‡ç±»å‹?}
    
    D -->|binary| E1[XGBClassifier]
    D -->|pnl_pct| E2[XGBRegressor]
    D -->|risk_adjusted| E3[XGBRegressor<br/>é£é™©è°ƒæ•´]
    
    E1 --> F1[åˆ†ç±»è¯„ä¼°<br/>accuracy/F1/AUC]
    E2 --> F2[å›å½’è¯„ä¼°<br/>MAE/RMSE/RÂ²]
    E3 --> F2
    
    F1 --> G[ä¸ç¡®å®šæ€§é‡åŒ–<br/>Quantile Regression]
    F2 --> G
    
    G --> H[å¸¦ç½®ä¿¡åŒºé—´çš„é¢„æµ‹]
    H --> I[å®æ—¶äº¤æ˜“å†³ç­–]
    
    I --> J[è™šæ‹Ÿ/çœŸå®ä»“ä½è¿½è¸ª]
    J --> K[æ–°äº¤æ˜“æ•°æ®]
    K --> L[æ¼‚ç§»æ£€æµ‹]
    L --> M{æ£€æµ‹åˆ°æ¼‚ç§»?}
    
    M -->|æ˜¯| N[è‡ªåŠ¨é‡è®­ç»ƒ]
    M -->|å¦| O[ç»§ç»­ä½¿ç”¨ç°æœ‰æ¨¡å‹]
    
    N --> B
    O --> I
```

---

## 4. è®­ç»ƒæ–¹å¼ä¸æµç¨‹

### 4.1 è®­ç»ƒè§¦å‘æœºåˆ¶

#### ä¸»åŠ¨è§¦å‘ï¼ˆæ‰‹åŠ¨/å®šæ—¶ï¼‰
```python
# 1. é¦–æ¬¡è®­ç»ƒ
trainer = XGBoostTrainer()
model, metrics = trainer.train(incremental=False)

# 2. å¢é‡è®­ç»ƒï¼ˆé€Ÿåº¦æå‡70-80%ï¼‰
model, metrics = trainer.train(incremental=True)
```

#### è‡ªåŠ¨è§¦å‘ï¼ˆæ¼‚ç§»æ£€æµ‹ï¼‰

ç³»ç»Ÿåœ¨ä»¥ä¸‹æƒ…å†µè‡ªåŠ¨é‡è®­ç»ƒï¼š

```python
# src/ml/drift_detector.py

def should_retrain(self, new_trades_count: int, hours_since_training: float) -> bool:
    """
    é‡è®­ç»ƒè§¦å‘æ¡ä»¶ï¼š
    
    1. æ–°äº¤æ˜“æ•°é‡ â‰¥ 50 ç¬”ï¼ˆç«‹å³é‡è®­ç»ƒï¼‰
    2. æ—¶é—´ â‰¥ 24å°æ—¶ ä¸” æ–°äº¤æ˜“ â‰¥ 10ç¬”
    3. æ£€æµ‹åˆ°ç‰¹å¾æ¼‚ç§»ï¼ˆKSæ£€éªŒ + PCA+MMDï¼‰
    """
    
    # æ¡ä»¶1ï¼šè¶³å¤Ÿçš„æ–°æ•°æ®
    if new_trades_count >= 50:
        return True
    
    # æ¡ä»¶2ï¼šæ—¶é—´+æ•°æ®åŒé‡æ¡ä»¶
    if hours_since_training >= 24 and new_trades_count >= 10:
        return True
    
    # æ¡ä»¶3ï¼šç‰¹å¾æ¼‚ç§»
    if self.detect_feature_drift(X):
        return True
    
    return False
```

### 4.2 å®Œæ•´è®­ç»ƒæµç¨‹

```python
# src/ml/model_trainer.py

def train(self, params=None, use_gpu=True, incremental=False):
    """
    è®­ç»ƒæµç¨‹ï¼ˆ14ä¸ªæ­¥éª¤ï¼‰
    """
    
    # ========== æ•°æ®å‡†å¤‡é˜¶æ®µ ==========
    
    # æ­¥éª¤1ï¼šåŠ è½½è®­ç»ƒæ•°æ®
    df = self.data_processor.load_training_data()
    if len(df) < ML_MIN_TRAINING_SAMPLES:
        return None, {}
    
    # æ­¥éª¤2ï¼šåº”ç”¨åŠ¨æ€æ»‘åŠ¨çª—å£ï¼ˆ500-2000æ ·æœ¬ï¼Œæ³¢åŠ¨ç‡è‡ªé€‚åº”ï¼‰
    df = self.drift_detector.apply_sliding_window(df)
    
    # æ­¥éª¤3ï¼šå‡†å¤‡ç›®æ ‡å˜é‡ï¼ˆrisk_adjusted/binary/pnl_pctï¼‰
    y, target_meta = self.target_optimizer.prepare_target(df)
    is_classification = (target_meta['target_type'] == 'binary')
    
    # æ­¥éª¤4ï¼šå‡†å¤‡ç‰¹å¾çŸ©é˜µ
    X = df[feature_columns].copy()
    X, y = X.align(y, join='inner', axis=0)  # ç¡®ä¿ç´¢å¼•å¯¹é½
    
    # ========== æ•°æ®éªŒè¯é˜¶æ®µ ==========
    
    # æ­¥éª¤5ï¼šæ ‡ç­¾æ³„æ¼éªŒè¯
    leakage_report = self.leakage_validator.validate_features(X, y)
    
    # æ­¥éª¤6ï¼šç±»åˆ«å¹³è¡¡åˆ†æï¼ˆä»…åˆ†ç±»æ¨¡å¼ï¼‰
    if is_classification:
        balance_report = self.imbalance_handler.analyze_class_balance(y)
    
    # æ­¥éª¤7ï¼šç‰¹å¾æ¼‚ç§»æ£€æµ‹
    drift_report = self.drift_detector.detect_feature_drift(X)
    
    # ========== æ¨¡å‹è®­ç»ƒé˜¶æ®µ ==========
    
    # æ­¥éª¤8ï¼šæ•°æ®åˆ†å‰²ï¼ˆ80% train, 20% testï¼‰
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # æ­¥éª¤9ï¼šè®¡ç®—æ ·æœ¬æƒé‡ï¼ˆä»…åˆ†ç±»æ¨¡å¼ï¼‰
    if is_classification:
        sample_weights = self.imbalance_handler.calculate_sample_weights(
            y_train, X_train
        )
    else:
        sample_weights = None
    
    # æ­¥éª¤10ï¼šé…ç½®æ¨¡å‹å‚æ•°
    base_params = self._get_base_params(use_gpu)
    params = self.target_optimizer.get_model_params(base_params)
    
    # æ­¥éª¤11ï¼šé€‰æ‹©æ¨¡å‹ç±»å‹
    import xgboost as xgb
    if is_classification:
        model = xgb.XGBClassifier(**params)
    else:
        model = xgb.XGBRegressor(**params)
    
    # æ­¥éª¤12ï¼šè®­ç»ƒæ¨¡å‹ï¼ˆæ”¯æŒå¢é‡å­¦ä¹ ï¼‰
    xgb_model_file = None
    if incremental and os.path.exists(self.model_path):
        xgb_model_file = 'temp_xgb_model.json'
        # åŠ è½½æ—§æ¨¡å‹ç»§ç»­è®­ç»ƒ
    
    model.fit(
        X_train, y_train,
        sample_weight=sample_weights,
        eval_set=[(X_train, y_train), (X_test, y_test)],
        early_stopping_rounds=20,
        verbose=False,
        xgb_model=xgb_model_file  # å¢é‡å­¦ä¹ 
    )
    
    # ========== è¯„ä¼°é˜¶æ®µ ==========
    
    # æ­¥éª¤13ï¼šæ¨¡å‹è¯„ä¼°
    y_pred = model.predict(X_test)
    
    if is_classification:
        # åˆ†ç±»è¯„ä¼°
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred),
            'recall': recall_score(y_test, y_pred),
            'f1_score': f1_score(y_test, y_pred),
            'roc_auc': roc_auc_score(y_test, y_pred_proba)
        }
    else:
        # å›å½’è¯„ä¼°
        metrics = {
            'mae': mean_absolute_error(y_test, y_pred),
            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
            'r2_score': r2_score(y_test, y_pred),
            'direction_accuracy': np.mean(np.sign(y_test) == np.sign(y_pred))
        }
    
    # æ­¥éª¤14ï¼šä¸ç¡®å®šæ€§é‡åŒ–è®­ç»ƒï¼ˆQuantile Regressionï¼‰
    self.uncertainty_quantifier.fit_quantile_models(X_train, y_train, base_params)
    
    # ========== ä¿å­˜é˜¶æ®µ ==========
    
    self.model = model
    self._save_model(model, metrics)
    
    return model, metrics
```

### 4.3 å¢é‡å­¦ä¹ æœºåˆ¶

```python
# å¢é‡å­¦ä¹ ä¼˜åŠ¿
incremental_learning = {
    'speed_boost': '70-80%',  # è®­ç»ƒé€Ÿåº¦æå‡
    'memory_usage': '-50%',   # å†…å­˜å ç”¨å‡å°‘
    'method': 'xgb_modelå‚æ•°',
    'trigger': 'æ–°æ•°æ® < 50ç¬”æ—¶å¯ç”¨'
}

# å®ç°æ–¹å¼
if incremental and os.path.exists(model_path):
    # ä¿å­˜æ—§æ¨¡å‹åˆ°ä¸´æ—¶æ–‡ä»¶
    old_model.save_model('temp_xgb_model.json')
    
    # ä»æ—§æ¨¡å‹ç»§ç»­è®­ç»ƒ
    new_model.fit(X, y, xgb_model='temp_xgb_model.json')
```

---

## 5. æ¨¡å‹å‚æ•°è¯¦è§£

### 5.1 åŸºç¡€å‚æ•°ï¼ˆæ‰€æœ‰ç›®æ ‡ç±»å‹é€šç”¨ï¼‰

```python
def _get_base_params(self, use_gpu: bool) -> Dict:
    """åŸºç¡€XGBoostå‚æ•°"""
    
    params = {
        # æ ‘ç»“æ„å‚æ•°
        'max_depth': 6,              # æœ€å¤§æ ‘æ·±åº¦ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        'min_child_weight': 3,       # æœ€å°å¶å­æƒé‡
        'subsample': 0.8,            # è¡Œé‡‡æ ·æ¯”ä¾‹
        'colsample_bytree': 0.8,     # åˆ—é‡‡æ ·æ¯”ä¾‹
        
        # æ­£åˆ™åŒ–å‚æ•°
        'gamma': 0.1,                # åˆ†è£‚æœ€å°æŸå¤±å‡å°‘
        'reg_alpha': 0.1,            # L1æ­£åˆ™åŒ–
        'reg_lambda': 1.0,           # L2æ­£åˆ™åŒ–
        
        # å­¦ä¹ ç‡å‚æ•°
        'learning_rate': 0.1,        # å­¦ä¹ ç‡ï¼ˆè‡ªé€‚åº”è°ƒæ•´ï¼‰
        'n_estimators': 100,         # æ ‘çš„æ•°é‡ï¼ˆè‡ªé€‚åº”è°ƒæ•´ï¼‰
        
        # å…¶ä»–å‚æ•°
        'random_state': 42,
        'n_jobs': -1,                # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
        'verbosity': 0               # é™é»˜æ¨¡å¼
    }
    
    # GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if use_gpu:
        params['tree_method'] = 'gpu_hist'
        params['gpu_id'] = 0
    else:
        params['tree_method'] = 'hist'
    
    return params
```

### 5.2 ç›®æ ‡ç‰¹å®šå‚æ•°

#### A. äºŒåˆ†ç±»å‚æ•°

```python
# åˆ†ç±»ä»»åŠ¡
params.update({
    'objective': 'binary:logistic',  # äºŒåˆ†ç±»é€»è¾‘å›å½’
    'eval_metric': 'auc',            # è¯„ä¼°æŒ‡æ ‡ï¼šAUC
    'scale_pos_weight': None,        # æ­£è´Ÿæ ·æœ¬æƒé‡ï¼ˆè‡ªåŠ¨è®¡ç®—ï¼‰
})
```

#### B. å›å½’å‚æ•°

```python
# å›å½’ä»»åŠ¡ï¼ˆpnl_pct / risk_adjustedï¼‰
params.update({
    'objective': 'reg:squarederror', # å‡æ–¹è¯¯å·®æŸå¤±
    'eval_metric': 'rmse',           # è¯„ä¼°æŒ‡æ ‡ï¼šRMSE
})
```

#### C. Quantile Regressionå‚æ•°

```python
# ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆåˆ†ä½æ•°å›å½’ï¼‰
quantile_params = params.copy()
quantile_params.update({
    'objective': 'reg:quantileerror',  # åˆ†ä½æ•°æŸå¤±
    'quantile_alpha': 0.025,           # åˆ†ä½æ•°ï¼ˆ0.025/0.5/0.975ï¼‰
})
```

### 5.3 è‡ªé€‚åº”å‚æ•°è°ƒæ•´

```python
# src/ml/adaptive_learner.py

class AdaptiveLearner:
    """è‡ªé€‚åº”å­¦ä¹ å™¨ï¼ˆåŠ¨æ€è°ƒæ•´è¶…å‚æ•°ï¼‰"""
    
    def suggest_hyperparameters(self, performance_history: List[float]) -> Dict:
        """
        æ ¹æ®å†å²æ€§èƒ½åŠ¨æ€è°ƒæ•´å‚æ•°
        
        è§„åˆ™ï¼š
        - æ€§èƒ½ä¸Šå‡ï¼šä¿æŒå‚æ•°
        - æ€§èƒ½ä¸‹é™ï¼šé™ä½å­¦ä¹ ç‡ã€å¢åŠ æ ‘æ•°é‡
        - æ€§èƒ½ç¨³å®šï¼šå°è¯•æ¢ç´¢æ–°å‚æ•°
        """
        
        if len(performance_history) < 3:
            return {}
        
        trend = self._analyze_trend(performance_history)
        
        suggestions = {}
        
        if trend == 'declining':
            # æ€§èƒ½ä¸‹é™ï¼šæ›´ä¿å®ˆçš„å­¦ä¹ 
            suggestions['learning_rate'] *= 0.9
            suggestions['n_estimators'] = int(suggestions['n_estimators'] * 1.1)
            suggestions['max_depth'] -= 1
        
        elif trend == 'improving':
            # æ€§èƒ½ä¸Šå‡ï¼šä¿æŒæˆ–ç•¥å¾®åŠ é€Ÿ
            suggestions['learning_rate'] *= 1.05
        
        return suggestions
```

### 5.4 GPUåŠ é€Ÿé…ç½®

```python
def detect_gpu() -> bool:
    """æ£€æµ‹GPUæ˜¯å¦å¯ç”¨"""
    
    try:
        import subprocess
        result = subprocess.run(
            ['nvidia-smi'], 
            capture_output=True, 
            timeout=5
        )
        return result.returncode == 0
    except:
        return False

# ä½¿ç”¨
if detect_gpu():
    params['tree_method'] = 'gpu_hist'
    logger.info("âœ… GPUåŠ é€Ÿå·²å¯ç”¨ï¼ˆè®­ç»ƒé€Ÿåº¦æå‡5-10å€ï¼‰")
else:
    params['tree_method'] = 'hist'
    logger.info("âš ï¸ GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®­ç»ƒ")
```

---

## 6. é«˜çº§ä¼˜åŒ–åŠŸèƒ½

### 6.1 ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆQuantile Regressionï¼‰

#### æŠ€æœ¯å¯¹æ¯”

| æ–¹æ³• | é€Ÿåº¦ | å†…å­˜å ç”¨ | å‡†ç¡®æ€§ |
|------|------|---------|--------|
| Bootstrapï¼ˆæ—§ï¼‰ | 1x | é«˜ï¼ˆ50ä¸ªæ¨¡å‹ï¼‰ | é«˜ |
| **Quantile Regressionï¼ˆæ–°ï¼‰** | **10x** | **ä½ï¼ˆ3ä¸ªæ¨¡å‹ï¼‰** | **é«˜** |

#### å®ç°

```python
# src/ml/uncertainty_quantifier.py

class UncertaintyQuantifier:
    
    def fit_quantile_models(self, X, y, base_params):
        """
        è®­ç»ƒ3ä¸ªQuantile Regressionæ¨¡å‹ï¼š
        - ä¸‹ç•Œï¼ˆ2.5%åˆ†ä½æ•°ï¼‰
        - ä¸­ä½æ•°ï¼ˆ50%åˆ†ä½æ•°ï¼‰
        - ä¸Šç•Œï¼ˆ97.5%åˆ†ä½æ•°ï¼‰
        """
        
        for quantile in [0.025, 0.5, 0.975]:
            params = base_params.copy()
            params['objective'] = 'reg:quantileerror'
            params['quantile_alpha'] = quantile
            
            model = XGBRegressor(**params)
            model.fit(X, y)
            
            self.quantile_models[quantile] = model
    
    def predict_with_uncertainty(self, X) -> Dict:
        """
        å¸¦95%ç½®ä¿¡åŒºé—´çš„é¢„æµ‹
        
        è¿”å›ï¼š
        {
            'prediction': ä¸­ä½æ•°é¢„æµ‹,
            'lower_bound': ä¸‹ç•Œï¼ˆ2.5%åˆ†ä½æ•°ï¼‰,
            'upper_bound': ä¸Šç•Œï¼ˆ97.5%åˆ†ä½æ•°ï¼‰,
            'uncertainty': åŒºé—´å®½åº¦
        }
        """
        
        pred_lower = self.quantile_models[0.025].predict(X)
        pred_median = self.quantile_models[0.5].predict(X)
        pred_upper = self.quantile_models[0.975].predict(X)
        
        return {
            'prediction': pred_median,
            'lower_bound': pred_lower,
            'upper_bound': pred_upper,
            'uncertainty': pred_upper - pred_lower
        }
```

### 6.2 åŠ¨æ€æ»‘åŠ¨çª—å£

#### æ³¢åŠ¨ç‡è‡ªé€‚åº”çª—å£

```python
# src/ml/drift_detector.py

def calculate_dynamic_window_size(self, df: pd.DataFrame) -> int:
    """
    åŠ¨æ€çª—å£å¤§å°ï¼š500 - 2000æ ·æœ¬
    
    å…¬å¼ï¼šwindow_size = max(500, min(2000, volatility_adapted))
    
    é€»è¾‘ï¼š
    - é«˜æ³¢åŠ¨ç‡ â†’ å°çª—å£ï¼ˆ500ï¼‰â†’ æ›´å¿«é€‚åº”
    - ä½æ³¢åŠ¨ç‡ â†’ å¤§çª—å£ï¼ˆ2000ï¼‰â†’ æ›´ç¨³å®š
    """
    
    # è®¡ç®—æ³¢åŠ¨ç‡ï¼ˆä½¿ç”¨ATRæˆ–æ”¶ç›Šç‡æ ‡å‡†å·®ï¼‰
    if 'atr_entry' in df.columns:
        volatility = df['atr_entry'].tail(100).mean()
        volatility_normalized = min(volatility / 0.05, 1.0)
    else:
        return self.base_window_size
    
    # åå‘å…³ç³»ï¼šæ³¢åŠ¨ç‡é«˜â†’çª—å£å°
    volatility_factor = 1.0 - volatility_normalized
    
    # è®¡ç®—çª—å£å¤§å°
    dynamic_size = int(500 + (2000 - 500) * volatility_factor)
    
    logger.info(f"åŠ¨æ€çª—å£ï¼š{dynamic_size}æ ·æœ¬ï¼ˆæ³¢åŠ¨ç‡ï¼š{volatility_normalized:.2%}ï¼‰")
    
    return dynamic_size
```

#### æ•ˆæœ

| å¸‚åœºçŠ¶æ€ | æ³¢åŠ¨ç‡ | çª—å£å¤§å° | ä¼˜åŠ¿ |
|---------|--------|---------|------|
| é«˜æ³¢åŠ¨æœŸ | 5% | 500 | å¿«é€Ÿé€‚åº”æ–°æ¨¡å¼ |
| æ­£å¸¸æœŸ | 2% | 1250 | å¹³è¡¡ç¨³å®šæ€§å’Œé€‚åº”æ€§ |
| ä½æ³¢åŠ¨æœŸ | 0.5% | 2000 | æœ€å¤§åŒ–ç¨³å®šæ€§ |

### 6.3 å¤šå˜é‡æ¼‚ç§»æ£€æµ‹ï¼ˆPCA + MMDï¼‰

#### æŠ€æœ¯å¯¹æ¯”

| æ–¹æ³• | ç±»å‹ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|------|------|------|------|
| KSæ£€éªŒï¼ˆæ—§ï¼‰ | å•å˜é‡ | ç®€å•å¿«é€Ÿ | æ— æ³•æ£€æµ‹å¤šç‰¹å¾è”åˆæ¼‚ç§» |
| **PCA+MMDï¼ˆæ–°ï¼‰** | **å¤šå˜é‡** | **æ£€æµ‹è”åˆåˆ†å¸ƒå˜åŒ–** | **è®¡ç®—ç¨æ…¢** |

#### å®ç°

```python
# src/ml/multivariate_drift.py

class MultivariateDriftDetector:
    
    def detect_multivariate_drift(self, X_baseline, X_current) -> Dict:
        """
        å¤šå˜é‡æ¼‚ç§»æ£€æµ‹
        
        æ­¥éª¤ï¼š
        1. PCAé™ç»´ï¼ˆ10ä¸ªä¸»æˆåˆ†ï¼‰
        2. MMDç»Ÿè®¡é‡è®¡ç®—ï¼ˆæœ€å¤§å‡å€¼å·®å¼‚ï¼‰
        3. é˜ˆå€¼åˆ¤æ–­ï¼ˆMMD > 0.1ä¸ºæ¼‚ç§»ï¼‰
        """
        
        # æ­¥éª¤1ï¼šPCAé™ç»´
        from sklearn.decomposition import PCA
        pca = PCA(n_components=10)
        X_baseline_pca = pca.fit_transform(X_baseline)
        X_current_pca = pca.transform(X_current)
        
        # æ­¥éª¤2ï¼šMMDè®¡ç®—
        mmd_value = self._calculate_mmd(X_baseline_pca, X_current_pca)
        
        # æ­¥éª¤3ï¼šåˆ¤æ–­
        drift_detected = mmd_value > self.mmd_threshold
        
        return {
            'drift_detected': drift_detected,
            'mmd_value': mmd_value,
            'threshold': self.mmd_threshold,
            'explained_variance': pca.explained_variance_ratio_.sum()
        }
    
    def _calculate_mmd(self, X, Y):
        """
        Maximum Mean Discrepancyï¼ˆæœ€å¤§å‡å€¼å·®å¼‚ï¼‰
        
        å…¬å¼ï¼šMMDÂ² = E[k(x,x')] + E[k(y,y')] - 2E[k(x,y)]
        """
        
        # RBFæ ¸
        def rbf_kernel(X, Y, gamma=1.0):
            from scipy.spatial.distance import cdist
            dist = cdist(X, Y, 'euclidean')
            return np.exp(-gamma * dist**2)
        
        K_XX = rbf_kernel(X, X)
        K_YY = rbf_kernel(Y, Y)
        K_XY = rbf_kernel(X, Y)
        
        mmd_squared = K_XX.mean() + K_YY.mean() - 2 * K_XY.mean()
        
        return np.sqrt(max(mmd_squared, 0))
```

### 6.4 æ ‡ç­¾æ³„æ¼éªŒè¯

```python
# src/ml/label_leakage_validator.py

class LabelLeakageValidator:
    
    def validate_features(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """
        æ£€æµ‹ç‰¹å¾æ˜¯å¦åŒ…å«æ ‡ç­¾æ³„æ¼
        
        æ–¹æ³•ï¼š
        1. è®¡ç®—æ¯ä¸ªç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§
        2. å¼‚å¸¸é«˜ç›¸å…³æ€§ï¼ˆ>0.9ï¼‰å¯èƒ½æ˜¯æ³„æ¼
        3. æ£€æŸ¥ç‰¹å¾åç§°ä¸­çš„å¯ç–‘å…³é”®è¯
        """
        
        suspicious_features = []
        
        for col in X.columns:
            # ç›¸å…³æ€§æ£€æŸ¥
            corr = X[col].corr(y)
            if abs(corr) > 0.9:
                suspicious_features.append({
                    'feature': col,
                    'correlation': corr,
                    'reason': 'High correlation'
                })
            
            # å…³é”®è¯æ£€æŸ¥
            leakage_keywords = ['pnl', 'profit', 'loss', 'exit', 'close']
            if any(kw in col.lower() for kw in leakage_keywords):
                suspicious_features.append({
                    'feature': col,
                    'reason': 'Suspicious keyword'
                })
        
        return {
            'has_leakage': len(suspicious_features) > 0,
            'suspicious_features': suspicious_features
        }
```

### 6.5 ç±»åˆ«ä¸å¹³è¡¡å¤„ç†

```python
# src/ml/imbalance_handler.py

class ImbalanceHandler:
    
    def calculate_sample_weights(self, y: pd.Series, X: pd.DataFrame) -> np.ndarray:
        """
        è®¡ç®—æ ·æœ¬æƒé‡ï¼ˆå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼‰
        
        ç­–ç•¥ï¼š
        1. åŸºç¡€æƒé‡ï¼šä½¿ç”¨class_weight='balanced'
        2. è¯¯åˆ†ç±»æƒé‡ï¼šå†å²è¯¯åˆ†ç±»æ ·æœ¬æƒé‡æå‡1.5å€
        3. éš¾æ ·æœ¬æƒé‡ï¼šç½®ä¿¡åŒºé—´å®½çš„æ ·æœ¬æƒé‡æå‡1.2å€
        """
        
        from sklearn.utils.class_weight import compute_sample_weight
        
        # åŸºç¡€æƒé‡
        base_weights = compute_sample_weight('balanced', y)
        
        # è¯¯åˆ†ç±»æƒé‡å¢å¼ºï¼ˆå¦‚æœæœ‰å†å²é¢„æµ‹è®°å½•ï¼‰
        if hasattr(self, 'misclassified_indices'):
            for idx in self.misclassified_indices:
                if idx < len(base_weights):
                    base_weights[idx] *= 1.5
        
        return base_weights
```

---

## 7. æ•°æ®æµä¸æ¶æ„å›¾

### 7.1 å®Œæ•´æ•°æ®æµ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®æ”¶é›†å±‚                                â”‚
â”‚  - è™šæ‹Ÿä»“ä½è¿½è¸ªï¼ˆæ‰€æœ‰ä¿¡å·ï¼‰                                  â”‚
â”‚  - çœŸå®ä»“ä½è®°å½•ï¼ˆå‰3ä¸ªæœ€ä¼˜ä¿¡å·ï¼‰                             â”‚
â”‚  - æ ¼å¼ï¼šJSONLï¼ˆæ¯è¡Œä¸€ç¬”äº¤æ˜“ï¼‰                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ç‰¹å¾å·¥ç¨‹å±‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ åŸºç¡€ç‰¹å¾   â”‚  â”‚ å¢å¼ºç‰¹å¾   â”‚  â”‚ äº¤äº’ç‰¹å¾   â”‚           â”‚
â”‚  â”‚  21ä¸ª      â”‚  â”‚   8ä¸ª      â”‚  â”‚   å·²åŒ…å«   â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                             â”‚
â”‚  â†’ ç¼–ç  â†’ å¡«å…… â†’ å½’ä¸€åŒ– â†’ ç‰¹å¾çŸ©é˜µï¼ˆ29ç»´ï¼‰                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ç›®æ ‡ä¼˜åŒ–å±‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  Binary  â”‚  â”‚ PnL PCT  â”‚  â”‚Risk-Adjusted â”‚            â”‚
â”‚  â”‚  0/1åˆ†ç±» â”‚  â”‚  å›å½’å€¼  â”‚  â”‚PnL/ATR(æ¨è) â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              æ•°æ®éªŒè¯ä¸ä¼˜åŒ–å±‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ æ ‡ç­¾æ³„æ¼éªŒè¯     â”‚  â”‚ ç±»åˆ«å¹³è¡¡åˆ†æ     â”‚                 â”‚
â”‚  â”‚ (ç›¸å…³æ€§æ£€æŸ¥)     â”‚  â”‚ (ä¸å¹³è¡¡å¤„ç†)     â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ åŠ¨æ€æ»‘åŠ¨çª—å£     â”‚  â”‚ ç‰¹å¾æ¼‚ç§»æ£€æµ‹     â”‚                 â”‚
â”‚  â”‚ (500-2000æ ·æœ¬)   â”‚  â”‚ (KS + PCA+MMD)   â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   æ¨¡å‹è®­ç»ƒå±‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚          XGBoostä¸»æ¨¡å‹                   â”‚               â”‚
â”‚  â”‚  - XGBClassifierï¼ˆbinaryç›®æ ‡ï¼‰           â”‚               â”‚
â”‚  â”‚  - XGBRegressorï¼ˆpnl_pct/risk_adjustedï¼‰ â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚      Quantile Regressionï¼ˆä¸ç¡®å®šæ€§ï¼‰      â”‚               â”‚
â”‚  â”‚  - ä¸‹ç•Œæ¨¡å‹ï¼ˆ2.5%åˆ†ä½æ•°ï¼‰                 â”‚               â”‚
â”‚  â”‚  - ä¸­ä½æ•°æ¨¡å‹ï¼ˆ50%åˆ†ä½æ•°ï¼‰                â”‚               â”‚
â”‚  â”‚  - ä¸Šç•Œæ¨¡å‹ï¼ˆ97.5%åˆ†ä½æ•°ï¼‰                â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                             â”‚
â”‚  è®­ç»ƒç‰¹æ€§ï¼š                                                  â”‚
â”‚  - Early Stoppingï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰                            â”‚
â”‚  - å¢é‡å­¦ä¹ ï¼ˆxgb_modelå‚æ•°ï¼‰                                â”‚
â”‚  - GPUåŠ é€Ÿï¼ˆtree_method='gpu_hist'ï¼‰                        â”‚
â”‚  - æ ·æœ¬æƒé‡ï¼ˆå¤„ç†ä¸å¹³è¡¡ï¼‰                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   è¯„ä¼°ä¸ç›‘æ§å±‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ åˆ†ç±»è¯„ä¼°         â”‚  â”‚ å›å½’è¯„ä¼°         â”‚                 â”‚
â”‚  â”‚ - Accuracy       â”‚  â”‚ - MAE           â”‚                 â”‚
â”‚  â”‚ - Precision      â”‚  â”‚ - RMSE          â”‚                 â”‚
â”‚  â”‚ - Recall         â”‚  â”‚ - RÂ²            â”‚                 â”‚
â”‚  â”‚ - F1 Score       â”‚  â”‚ - Direction Acc â”‚                 â”‚
â”‚  â”‚ - ROC-AUC        â”‚  â”‚                 â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚     ç‰¹å¾é‡è¦æ€§ç›‘æ§                   â”‚                   â”‚
â”‚  â”‚  - è‡ªåŠ¨è¯†åˆ«å…³é”®ç‰¹å¾                  â”‚                   â”‚
â”‚  â”‚  - è¿½è¸ªé‡è¦æ€§å˜åŒ–                    â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 é¢„æµ‹æ¨ç†å±‚ï¼ˆå®æ—¶ï¼‰                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  å¸¦ç½®ä¿¡åŒºé—´çš„é¢„æµ‹                        â”‚               â”‚
â”‚  â”‚  prediction: 0.65 (risk-adjusted)       â”‚               â”‚
â”‚  â”‚  lower_bound: 0.45                      â”‚               â”‚
â”‚  â”‚  upper_bound: 0.85                      â”‚               â”‚
â”‚  â”‚  uncertainty: 0.40                      â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                             â”‚
â”‚  â†’ è½¬æ¢ä¸ºä¿¡å¿ƒåº¦åˆ†æ•°ï¼ˆ0-100%ï¼‰                               â”‚
â”‚  â†’ ç»“åˆæœŸæœ›å€¼è®¡ç®—                                           â”‚
â”‚  â†’ ç”Ÿæˆäº¤æ˜“ä¿¡å·                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               è‡ªé€‚åº”å­¦ä¹ åé¦ˆå¾ªç¯                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  æ–°äº¤æ˜“ç»“æœ â†’ æ›´æ–°è®­ç»ƒé›† â†’ æ¼‚ç§»æ£€æµ‹      â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                      â”‚                                      â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚           â–¼                     â–¼                          â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚    â”‚è‡ªåŠ¨é‡è®­ç»ƒ    â”‚      â”‚å‚æ•°è‡ªé€‚åº”    â”‚                   â”‚
â”‚    â”‚(50ç¬”/24h)   â”‚      â”‚(å­¦ä¹ ç‡è°ƒæ•´)  â”‚                   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 æ€§èƒ½æŒ‡æ ‡æ€»è§ˆ

| ä¼˜åŒ–é¡¹ | v3.4.0å‰ | v3.9.1 | æå‡å¹…åº¦ |
|--------|----------|--------|---------|
| **ä¸ç¡®å®šæ€§é‡åŒ–é€Ÿåº¦** | 1x | 10x | +900% |
| **æ¼‚ç§»æ£€æµ‹å‡†ç¡®æ€§** | å•å˜é‡KS | å¤šå˜é‡PCA+MMD | æ›´robust |
| **æ»‘åŠ¨çª—å£** | å›ºå®š1000 | åŠ¨æ€500-2000 | è‡ªé€‚åº” |
| **é¢„æµ‹ç›®æ ‡** | Binary | Risk-Adjusted | è·¨regimeç¨³å®š |
| **å¢é‡å­¦ä¹ é€Ÿåº¦** | N/A | +70-80% | æ–°å¢ |
| **GPUåŠ é€Ÿ** | å¦ | æ˜¯ | +5-10x |
| **æ¨¡å‹å‡†ç¡®ç‡** | 70-75% | 75-82% | +5-10% |

---

## ğŸ“Š é™„å½•ï¼šå…³é”®é…ç½®æ–‡ä»¶

### A. æ¨¡å‹é…ç½®ï¼ˆconfig.pyï¼‰

```python
# MLè®­ç»ƒå‚æ•°
ML_MIN_TRAINING_SAMPLES = 50  # æœ€å°‘è®­ç»ƒæ ·æœ¬æ•°
ML_FLUSH_COUNT = 25           # æ¯25ç¬”äº¤æ˜“åˆ·æ–°åˆ°ç£ç›˜

# æ–‡ä»¶è·¯å¾„
TRADES_FILE = "data/trades.jsonl"
MODEL_PATH = "data/models/xgboost_model.pkl"
METRICS_PATH = "data/models/model_metrics.json"

# ç‰¹å¾ç¼“å­˜
FEATURE_CACHE_TTL = 3600  # 1å°æ—¶ç¼“å­˜
```

### B. ç›®æ ‡ç±»å‹é€‰æ‹©

```python
# åœ¨ model_trainer.py åˆå§‹åŒ–æ—¶é…ç½®

# æ–¹å¼1ï¼šäºŒåˆ†ç±»ï¼ˆç®€å•ç›ˆäºé¢„æµ‹ï¼‰
self.target_optimizer = TargetOptimizer(target_type='binary')

# æ–¹å¼2ï¼šç›ˆäºç™¾åˆ†æ¯”ï¼ˆé¢„æµ‹å…·ä½“æ”¶ç›Šï¼‰
self.target_optimizer = TargetOptimizer(target_type='pnl_pct')

# æ–¹å¼3ï¼šé£é™©è°ƒæ•´æ”¶ç›Šï¼ˆé»˜è®¤æ¨èï¼‰âœ…
self.target_optimizer = TargetOptimizer(target_type='risk_adjusted')
```

---

## ğŸ¯ æ€»ç»“

è¿™ä¸ªXGBoostæ¶æ„å®ç°äº†ï¼š

1. âœ… **29ç»´ç‰¹å¾**ï¼ˆ21åŸºç¡€ + 8å¢å¼ºï¼‰
2. âœ… **3ç§ç›®æ ‡ç±»å‹**ï¼ˆbinary/pnl_pct/risk_adjustedï¼‰
3. âœ… **è‡ªé€‚åº”æ¨¡å‹é€‰æ‹©**ï¼ˆClassifier/Regressorï¼‰
4. âœ… **Quantile Regression**ï¼ˆ10å€é€Ÿåº¦æå‡ï¼‰
5. âœ… **åŠ¨æ€æ»‘åŠ¨çª—å£**ï¼ˆ500-2000æ ·æœ¬ï¼‰
6. âœ… **PCA+MMDæ¼‚ç§»æ£€æµ‹**ï¼ˆå¤šå˜é‡ï¼‰
7. âœ… **å¢é‡å­¦ä¹ **ï¼ˆ70-80%é€Ÿåº¦æå‡ï¼‰
8. âœ… **GPUåŠ é€Ÿ**ï¼ˆ5-10å€é€Ÿåº¦ï¼‰
9. âœ… **ä¸ç¡®å®šæ€§é‡åŒ–**ï¼ˆ95%ç½®ä¿¡åŒºé—´ï¼‰
10. âœ… **è‡ªåŠ¨é‡è®­ç»ƒ**ï¼ˆæ¼‚ç§»è§¦å‘ï¼‰

ç³»ç»Ÿå·²è¾¾åˆ°**ç”Ÿäº§å°±ç»ª**æ ‡å‡†ï¼Œæ‰€æœ‰ä¼˜åŒ–å·²é€šè¿‡Architectå®¡æŸ¥å’Œé›†æˆæµ‹è¯•éªŒè¯ï¼ğŸš€
